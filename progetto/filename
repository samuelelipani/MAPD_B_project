 1/1: sc.stop()
 3/1:
# set this variable with one of the following values
# -> 'local'
# -> 'docker_container'
# -> 'docker_cluster'
CLUSTER_TYPE ='docker_container'
 3/2:
# set env variable
%env CLUSTER_TYPE $CLUSTER_TYPE
 3/3:
if CLUSTER_TYPE=='local':
    import findspark
    findspark.init('~/mapd_b/spark-3.1.1-bin-hadoop3.2/')
 3/4:
%%script bash --no-raise-error

if [[ "$CLUSTER_TYPE" != "docker_cluster" ]]; then
    echo "Launching master and worker"
    
    # start master 
    $SPARK_HOME/sbin/start-master.sh --host localhost \
        --port 7077 --webui-port 8080
    
    # start worker
    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \
        --cores 4 --memory 2g
fi
 3/5:
from pyspark.sql import SparkSession

if CLUSTER_TYPE in ['local', 'docker_container']:
    
    spark = SparkSession.builder \
        .master("spark://localhost:7077")\
        .appName("Spark streaming application")\
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")\
        .config("spark.sql.execution.arrow.pyspark.fallback.enabled", "false")\
        .getOrCreate()

elif CLUSTER_TYPE == 'docker_cluster':
    
    # use the provided master
    spark = SparkSession.builder \
        .master("spark://spark-master:7077")\
        .appName("Spark streaming application")\
        .config("spark.executor.memory", "512m")\
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")\
        .config("spark.sql.execution.arrow.pyspark.fallback.enabled", "false")\
        .getOrCreate()
 3/6:
sc = spark.sparkContext
sc
 3/7:
# stop streaming context

ssc.stop(stopSparkContext=False)
 5/1: !$SPARK_HOME/sbin/stop-worker.sh
 5/2: !$SPARK_HOME/sbin/stop-master.sh
 7/1:
from time import sleep


def increment(x):
    """
    take a number x and return x+1
    sleep for 1s
    """
    sleep(1)
    return x + 1

def add(x, y):
    """
    sum y to x and return the result
    sleep for 1s
    """
    sleep(1)
    return x + y
 7/2:
%%time

x = increment(1)
y = increment(2)
z = add(x, y)
 7/3: z
 7/4:
%%time

x = increment(1)
y = increment(2)
z = add(x, y)
# each function runs for at least one second since we 
# added sleep(1). In total the time spent is almost equal
# to three seconds since the real time of execution 
# has been covered by this sleep function
 7/5:
%%time

# the delayed function takes several arguments. the first argument is the function that has to be executed in parallel.
# the following arguments are the arguments of the original function.
from dask import delayed

x = delayed(increment)(1)
y = delayed(increment)(2)
z = delayed(add)(x, y)
 7/6:
%%time

# the delayed function takes several arguments. the first argument is the function that has to be executed in parallel.
# the following arguments are the arguments of the original function.
from dask import delayed

x = delayed(increment)(1)
y = delayed(increment)(2)
z = delayed(add)(x, y)
# we know that both increment and add function should run for at least one second. It means that in this cell we are not surely 
# executing any computation; we're only saving x,y and z as delayed objects
 7/7:
%%time

z.compute()
 7/8:
%%time

z.compute() 
# the precessing time decreased from 3 to 2 seconds. Indeed the parallelized 
# object allowed x and y to be both ready to be parallel executed in order to make the sum 
# just after they have been ran.
 7/9: z.visualize(rankdir="LR")
7/10: visualize^
7/11: visualize?
7/12: help(visualize)
7/13: help(.visualize)
7/14: dir(visualize)
7/15:
def increment(x):
    sleep(1)
    return x + 1

data = [1, 2, 3, 4, 5, 6, 7, 8]
7/16:
import time
start = time.time()
# Sequential code

results = []
for x in data:
    y = increment(x)
    results.append(y)
    
result = sum(results)
end = time.time()
print("After computing : {}".format(result))
print("Computation took {}s".format(round(end-start), 2))
7/17:
%%time 

results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
7/18:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : {}".format(result))
print("Computation took {}s".format(round(end-start), 2))
7/19:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : {}" result)
print("Computation took {}s".format(round(end-start), 2))
7/20:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : {}",result)
print("Computation took {}s".format(round(end-start), 2))
7/21:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : %i",result)
print("Computation took {}s".format(round(end-start), 2))
7/22:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : {}",.format(result.compute))
print("Computation took {}s".format(round(end-start), 2))
7/23:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : {}".format(result.compute))
print("Computation took {}s".format(round(end-start), 2))
7/24:
start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result = sum(results)
result.compute()
end = time.time()
print("After computing : {}".format(result.compute()))
print("Computation took {}s".format(round(end-start), 2))
7/25:
import time
start = time.time()

results = []

for x in data:
    y =   delayed(increment)(x)
    results.append(y)
    
total =  delayed(sum)(results) 
result = total.compute()
end = time.time()
print("After computing : {}".format(result))
print("Computation took {}s".format(round(end-start), 2))
7/26: result.visualize()
7/27: total.visualize()
7/28: total.visualize(rankdir="LR")
7/29: total.visualize()
7/30:
# MY SOLUTION 

start = time.time()
results = []
for x in data:
    y = delayed(increment)(x)
    results.append(y)

result1 = sum(results)
result1.compute()
end = time.time()
print("After computing : {}".format(result.compute()))
print("Computation took {}s".format(round(end-start), 2))
7/31: result1.visualize()
7/32: result1.visualize(rankdir = "TB")
7/33: result1.visualize(rankdir = "LR")
7/34:
def double(x):
    sleep(1)
    return 2 * x

def is_even(x):
    return not x % 2

def increment(x):
    sleep(1)
    return x + 1

data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
7/35:
import time
start = time.time()

# Sequential code

results = []
for x in data:
    if is_even(x):
        y = double(x)
    else:
        y = increment(x)
    results.append(y)
    
result = sum(results)
end = time.time()
print("After computing :", result)
print("Computation took {}s".format(round(end-start), 2))
7/36:
import time
start = time.time()
results = []
for x in data:
    if is_even(x):  # even
        y =  delayed(double)(x)
    else:          # odd
        y = delayed(increment)(x)
    results.append(y)
    
total = delayed(sum)(results)
result = total.compute()
end = time.time()
print("After computing :", result)
print("Computation took {}s".format(round(end-start), 2))
7/37: total.visualize()
7/38:
import time

start = time.time()
L = [1, 3, 4, 5, 6, 7, 8, 10]


while len(L) > 1:
    new_L = []
    for i in range(0, len(L), 2): # we move twice in array's elements
        lazy = add(L[i], L[i + 1])  # add neighbors
        new_L.append(lazy)
    L = new_L 
    
print(L[0])
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/39: L
7/40:
start = time.time()

while len(L) > 1:
    new_l = []
    for i in range(0,len(L), 2):
        lazy = delayed(add)(L[i],L[i+1])
        new_L.append(lazy.compute())
    L = new_L
print(L[0])
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/41:
start = time.time()
L1 = [1, 3, 4, 5, 6, 7, 8, 10]

while len(L) > 1:
    new_L1 = []
    for i in range(0,len(L), 2):
        lazy1 = delayed(add)(L[i],L[i+1])
        new_L1.append(lazy1.compute())
    L1 = new_L1
    
print(L1[0])
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/42:
start = time.time()
L1 = [1, 3, 4, 5, 6, 7, 8, 10]

while len(L1) > 1:
    new_L1 = []
    for i in range(0,len(L1), 2):
        lazy1 = delayed(add)(L[i],L[i+1])
        new_L1.append(lazy1.compute())
    L1 = new_L1
    
print(L1[0])
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/43:
start = time.time()
L1 = [1, 3, 4, 5, 6, 7, 8, 10]

while len(L1) > 1:
    new_L1 = []
    for i in range(0,len(L1), 2):
        lazy1 = delayed(add)(L1[i],L1[i+1])
        new_L1.append(lazy1.compute())
    L1 = new_L1
    
print(L1[0])
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/44:
start = time.time()
L1 = [1, 3, 4, 5, 6, 7, 8, 10]

while len(L1) > 1:
    new_L1 = []
    for i in range(0,len(L1), 2):
        lazy1 = delayed(add)(L1[i],L1[i+1])
        new_L1.append(lazy1)
    L1 = new_L1
    
print(L1[0].compute)
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/45:
start = time.time()
L1 = [1, 3, 4, 5, 6, 7, 8, 10]

while len(L1) > 1:
    new_L1 = []
    for i in range(0,len(L1), 2):
        lazy1 = delayed(add)(L1[i],L1[i+1])
        new_L1.append(lazy1)
    L1 = new_L1
    
print(L1[0].compute)
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/46:
start = time.time()
L1 = [1, 3, 4, 5, 6, 7, 8, 10]

while len(L1) > 1:
    new_L1 = []
    for i in range(0,len(L1), 2):
        lazy1 = delayed(add)(L1[i],L1[i+1])
        new_L1.append(lazy1)
    L1 = new_L1
    
print(L1[0].compute())
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/47: Try to parallelize the algorithm:
7/48:
import time

start = time.time()

L = [1, 3, 4, 5, 6, 7, 8, 10]
result = None
while len(L) > 1:
    new_L = []
    for i in range(0, len(L), 2):
        lazy = delayed(add)(L[i], L[i+1])
        new_L.append(lazy)
    L = new_L 

print(L[0].compute()) 
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/49:
import time

start = time.time()

L = [1, 3, 4, 5, 6, 7, 8, 10]
result = None
while len(L) > 1:
    new_L = []
    for i in range(0, len(L), 2):
        lazy = delayed(add)(L[i], L[i + 1])
        new_L.append(lazy)
    L = new_L 
display(L[0].visualize())
7/50:
import numpy as np

def f(x):
    return (np.sin(1/(x*(2-x))))**2
7/51:
import time

start = time.time()

# Monte Carlo integration
N=2000000
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0
        
for i in range(N):
    count.append(get_value())
    
I=2*sum(count)/N
print(I)
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/52:
import time

start = time.time()

# Monte Carlo integration
N=2000000
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0
        
for i in range(N):
    count.append(delayed(get_value()))

real_count = delayed(sum)(count)
I=2*real_count.compute()/N
print(I)
end = time.time()
print("Computation took {}s".format(round(end-start), 2))
7/53:
import time

start = time.time()

# Monte Carlo integration
N=10
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0
        
for i in range(N):
    count.append(delayed(get_value())) 

real_count = delayed(sum)(count)  
display(real_count.visualize()) 

# since all the operations are paused until the last sum can be computed, the 
# whole run takes much more than the sequential way of computing it
13/1:
import numpy as np

def f(x):
    return (np.sin(1/(x*(2-x))))**2
13/2: from dask import delayed
13/3: Try to parallelize the algorithm:
13/4:
# Try to improve the previous code structure in order to let it be parallelized 

start = time.time()

N = 1000 
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0
    
summ = []
for i in range(N):
    count.append(delayed(get_value()))
    summ.append(delayed(sum)(count))
    summ.compute()
print(2*summ/N)
13/5:
# Try to improve the previous code structure in order to let it be parallelized 
import time 
start = time.time()

N = 1000 
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0
    
summ = []
for i in range(N):
    count.append(delayed(get_value()))
    summ.append(delayed(sum)(count))
    summ.compute()
    
end = time.time()
print(2*summ/N)
13/6:
# Try to improve the previous code structure in order to let it be parallelized 
import time 
start = time.time()

N = 1000 
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0
    
summ = []
for i in range(N):
    count.append(delayed(get_value()))
    summ.append(delayed(sum)(count))
    summ[i].compute()
    
end = time.time()
print(2*summ/N)
13/7: summ
13/8: sum(1,2)
13/9: sum(*[1,2])
13/10: sum([1,2])
13/11:
# Try to improve the previous code structure in order to let it be parallelized 
import time 
start = time.time()

N = 1000 
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0

for i in range(N):
    count.append(delayed(get_value()))

summ = []
while len(count) > 1:
    for i in range(0,len(count),2):
        summ.append(delayed(sum([count[i],count[i+1]])))
summ[0].compute
        
    
    
    
end = time.time()

#print(2*summ/N)
13/12:
# Try to improve the previous code structure in order to let it be parallelized 
import time 
start = time.time()

N = 10 
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0

for i in range(N):
    count.append(delayed(get_value()))

summ = []
while len(count) > 1:
    for i in range(0,len(count),2):
        summ.append(delayed(sum([count[i],count[i+1]])))
    count = summ
#result = summ[0].compute
display(summ[0].compute)        

end = time.time()

#print(2*summ/N)
13/13:
# Try to improve the previous code structure in order to let it be parallelized 
import time 
start = time.time()

N = 10 
count=[]
def get_value():
    x=2*np.random.random()
    y=np.random.random()
    if y<f(x): 
        return 1
    else:
        return 0

for i in range(N):
    count.append(delayed(get_value()))

summ = []
while len(count) > 1:
    for i in range(0,len(count),2):
        temp = delayed(sum([count[i],count[i+1]]))
        summ.append(temp)
    count = summ
#result = summ[0].compute
display(count[0].compute)        

end = time.time()

#print(2*summ/N)
10/1:
from dask import delayed
import time

def increment(x):
    print("I'm icrementing "+str(x)+" on this cluster node!")
    time.sleep(5)
    return x + 1

def decrement(x):
    print("I'm decrementing "+str(x)+" on this cluster node!")
    time.sleep(3)
    return x - 1

def add(x, y):
    print("I'm summing "+str(x)+" and "+str(y)+" on this cluster node!")
    time.sleep(7)
    return x + y
10/2:
from dask.distributed import Client

# Leeve blank to setup a local cluster. Put the scheduler IP to distributed cluster.

client = Client()
10/3: client
10/4:
import time 
%%time 

x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
10/5:
import time 
%% time 

x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
10/6:
import time 
%timeit

x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
10/7:
x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
10/8:
x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
10/9:
x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
10/10:
future = client.submit(increment, 1)

client.gather(future)

# future is basically the pointer to the future result. gather waits until the process
# can be executed
10/11:
future = client.submit(increment, 1)

client.gather(future)

# future is basically the pointer to the future result. gather waits until the process
# can be executed
10/12:
future = client.submit(increment, 1)

client.gather(future)

# future is basically the pointer to the future result. gather waits until the process
# can be executed
10/13:
data = [0, 1, 2, 3, 4, 5, 6, 7, 8]

future_results = client.map(increment, data)
client.gather(future_results)
10/14:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

map(sum,data)
10/15:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

print(map(sum,data))
10/16:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

list(map(sum,data))
10/17:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

map(sum(),data)
10/18:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

help(map)
10/19:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

map(increment, data)
10/20:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

list(map(increment, data))
10/21:
# this map function is the same as the one in vanilla python that allows to work 
# with vectors directly instead of using for cycles to iterate over array elements

list(map(lambda x: x+1, data))
10/22:
from dask.distributed import performance_report
with performance_report(filename="dask-report.html"):
    future = client.submit(increment, 1)
    client.gather(future)
10/23: client.scheduler_info()
10/24: client.close()
10/25:
from dask.distributed import Client

# Leeve blank to setup a local cluster. Put the scheduler IP to distributed cluster.

client = Client()
10/26: client
10/27:
x = client.submit(increment, 1)
y = client.submit(decrement, 2)
total = client.submit(add, x, y)

print(total)     # This is still the execution promise
client.gather(total)  # This is the final result
10/28:
x = client.submit(increment, 1)
y = client.submit(decrement, 2)
total = client.submit(add, x, y)

print(total)     # This is still the execution promise
client.gather(total)  # This is the final result
10/29: client = Client('')
10/30:
def double(x):
    time.sleep(4)
    return 2 * x

def is_even(x):
    return not x % 2

data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
10/31:
%%time
# Sequential code

results = []
for x in data:
    if is_even(x):
        y = double(x)
    else:
        y = increment(x)
    results.append(y)
    
total = sum(results)
print(total)
10/32: is_even(data)
10/33: is_even(*data)
10/34:
%%time

results = []
for x in data:
    if is_even(x):
        y = client.submit(double)(x)
    else: y = client.submit(increment)(x)
    results.append(y)

result = client.submit(sum)(results)
total = client.gather(result)
10/35:
%%time

results = []
for x in data:
    if is_even(x):
        y = client.submit(double)(x)
    else: y = client.submit(increment)(x)
    results.append(y)

result = client.submit(sum, results)
total = client.gather(result)
10/36:
%%time

results = []
for x in data:
    if is_even(x):
        y = client.submit(double, x)
    else: y = client.submit(increment, x)
    results.append(y)

result = client.submit(sum, results)
total = client.gather(result)
10/37:
from sklearn.datasets import fetch_20newsgroups
from dask.distributed import Client
import time

categories = [
     'comp.graphics',
     'comp.os.ms-windows.misc',
     'comp.sys.ibm.pc.hardware',
     'comp.sys.mac.hardware',
     'comp.windows.x',
     'misc.forsale',
     'rec.autos',
     'rec.motorcycles',
     'rec.sport.baseball',
     'rec.sport.hockey',
     'sci.crypt',
     'sci.electronics',
     'sci.med',
     'sci.space'
]

dataset = fetch_20newsgroups(subset='train', categories=categories ).data
10/38: print("Texts document present on the dataset: "+str(len(dataset)))
10/39:
def count_word_in_statement(text):
    """
    This function takes a text as input and return the number of the words that it contains
    """
    splitted_words = text.split()
    return len(text)
10/40:
%time

futures = #put your code here
results = #put your code here
for index in range(0, len(results)):
    print("Text #"+str(index+1)+" contains "+str(results[index])+" words")
10/41:
%time

futures = client.submit(count_word_in_statement, dataset)
results = client.gather(futures)
for index in range(0, len(results)):
    print("Text #"+str(index+1)+" contains "+str(results[index])+" words")
10/42: dataset
10/43: dataset[0]
10/44: print("Texts document present on the dataset: "+str(len(dataset)))
10/45:
dataset[0]
# dataset is a list with each element a string of words. In particular one component
# of the list is a document of the ones we selected initially through "categories"

str(dataset[0]).split
10/46:
dataset[0]
# dataset is a list with each element a string of words. In particular one component
# of the list is a document of the ones we selected initially through "categories"

str(dataset[0]).split()
10/47:
dataset[0]
# dataset is a list with each element a string of words. In particular one component
# of the list is a document of the ones we selected initially through "categories"
10/48:
%time

futures = client.map(count_word_in_statement, dataset)
results = client.gather(futures)
for index in range(0, len(results)):
    print("Text #"+str(index+1)+" contains "+str(results[index])+" words")
10/49:
%time

futures = client.map(count_word_in_statement, dataset)
results = client.gather(futures)
#for index in range(0, len(results)):
    #print("Text #"+str(index+1)+" contains "+str(results[index])+" words")
10/50:
import numpy as np

## creation of a matrix of 100 rows and 10 columns with each value between 0 and 100.
np.random.RandomState(42)
arr = np.random.randint(0, 100, size=[100, 10])
10/51: arr
10/52: arr.shape()
10/53: arr.shape
10/54: arr.rows
10/55: arr.row
10/56: arr.shape
10/57: arr.shape[1]
10/58: arr.shape[0]
10/59: len(arr)
10/60: arr[,:]
10/61: arr[,::]
10/62: arr[,]
10/63: arr[:,:]
10/64: arr[0][0]
10/65: arr[0][]
10/66: arr[0]
10/67: arr[]
10/68: arr[0]
10/69: arr[0][:]
10/70: arr[:]
10/71:
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column 

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(arr[:], howmany_within_range) 
resutls = client.gather(futures)
print(results)
10/72:
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column 

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
resutls = client.gather(futures)
print(results)
10/73:
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column 

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
results = client.gather(futures)
print(results)
10/74: arr[:] < 10
10/75: arr[arr < 10]
10/76: [arr < 10]
10/77: map(is.true,[arr < 10])
10/78: map(lambda x: x == True,[arr < 10])
10/79: list(map(lambda x: x == True,[arr < 10]))
10/80: arr
10/81: arr[arr > 10, :]
10/82: arr[(arr > 10), :]
10/83: arr[arr[:] < 10]
10/84: len(arr[arr[:] < 10])
10/85: arr[arr[,:] < 10]
10/86: [(arr[arr[i,:] < 10) for i in range(len(arr)) ]
10/87: [(arr[arr[i,:] < 10]) for i in range(len(arr))]
10/88: [(arr[arr[i,:] < 10]) for i in range(100)]
10/89: [(arr[arr[i] < 10]) for i in range(100)]
10/90: arr[arr < 10]
10/91: arr < 10
10/92:
arr1 = np.array((100,10))
if (arr < 10) == True: 
    arr1 = 1
10/93:
arr1 = np.array((100,10))
if (arr < 10) is True: 
    arr1 = 1
10/94: arr1
10/95:
arr1 = np.array(size = (100,10))
if (arr < 10) is True: 
    arr1 = 1
10/96: help(array)
10/97: help(np.array)
10/98:
arr1 = np.array(dim = (100,10))
if (arr < 10) is True: 
    arr1 = 1
10/99:
arr1 = np.zeros((100,10))
if (arr < 10) is True: 
    arr1 = 1
10/100: arr1
10/101: arr
10/102: arr < 10
10/103:
arr1 = np.zeros((100,10))
if arr < 10: 
    arr1 = 1
10/104: arr1
10/105: sum(arr < 10)
10/106: sum((arr < 10).T)
10/107: arr1 = sum((arr < 10).T)
10/108: arr1 == results
10/109: arr1 is results
10/110: arr1 = sum((arr <= 10).T)
10/111:
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column 

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
results = client.gather(futures)

print(results)
10/112: arr1 is results
10/113: arr1
10/114: arr1 == results
10/115: (arr1 == results).all
10/116: (arr1 == results).all()
10/117:
%%timeit
arr1 = sum((arr <= 10).T)
10/118:
%%timeit -n 10
arr1 = sum((arr <= 10).T)
10/119:
%%timeit -n 10000
arr1 = sum((arr <= 10).T)
10/120:
%%timeit -n 1
arr1 = sum((arr <= 10).T)
10/121:
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column 
%%timeit -n 1
def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
results = client.gather(futures)

print(results)
10/122:
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column 
%%timeit -n 1

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
results = client.gather(futures)

print(results)
10/123:
 
%%timeit -n 1

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
results = client.gather(futures)

print(results)
10/124:
%%timeit -n 1
# let's see that in order to do a parallelizable code we should not use a vectorized
# form. In this case the function "howmany_within_range" could be applied in parallel
# for each element of a column

def howmany_within_range(row):
    """Returns how many numbers lie within 0 and 10 in the given `row`"""
    count = 0
    for n in row:
        if 0 <= n <= 10:
            count = count + 1
    return count

futures = client.map(howmany_within_range, arr[:]) 
results = client.gather(futures)

#print(results)
10/125: results
10/126: np.array(results)
10/127:
%%timeit -n 1 # this -n set the number of loops for the timeit magic function 
arr1 = sum((arr <= 10).T)
10/128: r = np.array([1,2,3],[4,5,6])
10/129: r = np.array(([1,2,3],[4,5,6]))
10/130:
r = np.array(([1,2,3],[4,5,6]))
r
10/131:
r = np.array(([1,2,3],[4,5,6]))
r
sum(r)
10/132:
r = np.array(([1,2,3],[4,5,6]))
print(r)
sum(r)
10/133:
import numpy as np
import random
import math
10/134:

n = 10000

def pi_python_sequential(n):
    points = []
    for i in range(0, n):
        inside_or_out_side = 0
        x2 = random.random()**2
        y2 = random.random()**2
        if math.sqrt(x2 + y2) < 1.0:
            inside_or_out_side = 1
        
        points.append(inside_or_out_side)
    
    return np.multiply(np.divide(float(sum(points)), n), 4)

pi = pi_python_sequential(n)
print("Estimation of Pi: "+str(pi))
10/135:
from dask.distributed import Client
from dask import delayed
10/136:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    l1 = client.map(partial_monte_carlo,l)
    result = client.gather(l1)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(estimation.compute()))
10/137:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    l1 = client.map(partial_monte_carlo(),l)
    result = client.gather(l1)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(estimation.compute()))
10/138:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    for i in range(len(l)):
        l[i] = client.submit(partial_monte_carlo)
    result = client.gather(l)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(estimation.compute()))
10/139:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    for i in range(len(l)):
        l[i] = client.submit(partial_monte_carlo)
    result = client.gather(l)
    return result

estimation = pi_python(n);
#print("Estimation of Pi: "+str(estimation.compute()))
10/140:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    for i in range(len(l)):
        l[i] = client.submit(partial_monte_carlo)
    result = client.submit(sum)(l)
    result = client.gather(l)
    return result

estimation = pi_python(n);
#print("Estimation of Pi: "+str(estimation.compute()))
10/141:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    for i in range(len(l)):
        l[i] = client.submit(partial_monte_carlo)()
    result = client.submit(sum)(l)
    result = client.gather(l)
    return result

estimation = pi_python(n);
#print("Estimation of Pi: "+str(estimation.compute()))
10/142:
%time

n = 1000

def partial_monte_carlo(n):
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    for i in range(len(l)):
        l[i] = client.submit(partial_monte_carlo)(n)
    result = client.submit(sum)(l)
    result = client.gather(l)
    return result

estimation = pi_python(n);
#print("Estimation of Pi: "+str(estimation.compute()))
10/143:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    l = np.zeros(1000)
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    for i in range(len(l)):
        l[i] = client.submit(partial_monte_carlo)()
    result = client.submit(sum)(l)
    result = client.gather(l)
    return result

estimation = pi_python(n);
#print("Estimation of Pi: "+str(estimation.compute()))
10/144: map(partial_monte_carlo, l)
10/145:
l = np.zeros(100)
map(partial_monte_carlo, l)
10/146:
l = np.zeros(100)
np.array(map(partial_monte_carlo, l))
10/147:
l = np.zeros(100)
list(map(partial_monte_carlo(), l))
10/148:
l = np.zeros(100)
l = partial_monte_carlo
10/149:
l = np.zeros(100)
l = partial_monte_carlo()
10/150:
l = np.zeros(100)
l = partial_monte_carlo()
l
10/151:
l = np.zeros(100)
l = [partial_monte_carlo() for i in range(len(l))]
10/152:
l = np.zeros(100)
l = [partial_monte_carlo() for i in range(len(l))]
l
10/153:
l = np.zeros(100)
l = client.submit([partial_monte_carlo() for i in range(len(l))])
10/154: l = client.submit(lambda x: [partial_monte_carlo() for i in range(len(l))])(n)
10/155: l = client.submit(lambda x: [partial_monte_carlo() for i in range(len(l))])
10/156:
l = client.submit(lambda x: [partial_monte_carlo() for i in range(len(l))])
client.gather(l)
10/157:
l = client.submit(lambda x: [partial_monte_carlo() for i in range(len(l))], n)
client.gather(l)
10/158:
l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
client.gather(l)
10/159:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.gather(l)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(estimation.compute()))
10/160:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    result = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(estimation.compute()))
10/161:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    result = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(estimation.gather()))
10/162:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    result = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(client.gather(estimation)))
10/163:
%time 
n = 10000

def pi_python_sequential(n):
    points = []
    for i in range(0, n):
        inside_or_out_side = 0
        x2 = random.random()**2
        y2 = random.random()**2
        if math.sqrt(x2 + y2) < 1.0:
            inside_or_out_side = 1
        
        points.append(inside_or_out_side)
    
    return np.multiply(np.divide(float(sum(points)), n), 4)

pi = pi_python_sequential(n)
print("Estimation of Pi: "+str(pi))
10/164:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum)(l)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(client.gather(estimation)))
10/165:
%time

futures = client.map(count_word_in_statement, dataset)
results = client.gather(futures)
results
#for index in range(0, len(results)):
    #print("Text #"+str(index+1)+" contains "+str(results[index])+" words")
10/166:
%time

futures = client.map(count_word_in_statement, dataset)
results = client.gather(futures)
np.array(results)
#for index in range(0, len(results)):
    #print("Text #"+str(index+1)+" contains "+str(results[index])+" words")
10/167:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = pi_python(n);
print("Estimation of Pi: "+str(client.gather(estimation)))
10/168:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = pi_python(n)/n;
print("Estimation of Pi: "+str(client.gather(estimation)))
10/169:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = client.gather(pi_python(n)/4);
print("Estimation of Pi: "+str(estimation)
10/170:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = client.gather(pi_python(n)/4);
print("Estimation of Pi: "+str(estimation))
10/171:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = client.gather(pi_python(n))/4;
print("Estimation of Pi: "+str(estimation))
10/172:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = client.gather(pi_python(n))/n;
print("Estimation of Pi: "+str(estimation))
10/173:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = (client.gather(pi_python(n))/n)*4;
print("Estimation of Pi: "+str(estimation))
10/174:
%time

n = 1000

def partial_monte_carlo():
    inside_out_side = 0 
    
    x2 = random.random()**2
    y2 = random.random()**2
    if math.sqrt(x2 + y2) < 1.0:
        inside_out_side = 1
    return inside_out_side

def pi_python(n):
    #put your code here
    # after the creation of the zeros array i map the partial_monte_carlo function
    # to the vector elements since it creates only one or zeros independently on the 
    # argument it is give 
    #for i in range(len(l)):
        #l[i] = client.submit(partial_monte_carlo)
    l = client.submit(lambda x: [partial_monte_carlo() for i in range(n)], n)
    result = client.submit(sum,l)
    return result

estimation = (client.gather(pi_python(n))/n)*4;
print("Estimation of Pi: "+str(estimation))
10/175: pi_python(n).visualize()
10/176: display(pi_python(n).visualize(rankdir = "LR"))
10/177: class(py_python(n))
10/178: type(py_python(n))
10/179: type(pi_python(n))
10/180: class(pi_python(n))
14/1: from dask import delayed
14/2:
%%time

# the delayed function takes several arguments. the first argument is the function that has to be executed in parallel.
# the following arguments are the arguments of the original function.
from dask import delayed

x = delayed(increment)(1)
y = delayed(increment)(2)
z = delayed(add)(x, y)
# we know that both increment and add function should run for at least one second. It means that in this cell we are not surely 
# executing any computation; we're only saving x,y and z as delayed objects
14/3:
from time import sleep


def increment(x):
    """
    take a number x and return x+1
    sleep for 1s
    """
    sleep(1)
    return x + 1

def add(x, y):
    """
    sum y to x and return the result
    sleep for 1s
    """
    sleep(1)
    return x + y
14/4:
%%time

x = increment(1)
y = increment(2)
z = add(x, y)
# each function runs for at least one second since we 
# added sleep(1). In total the time spent is almost equal
# to three seconds since the real time of execution 
# has been covered by this sleep function
14/5:
%%time

# the delayed function takes several arguments. the first argument is the function that has to be executed in parallel.
# the following arguments are the arguments of the original function.
from dask import delayed

x = delayed(increment)(1)
y = delayed(increment)(2)
z = delayed(add)(x, y)
# we know that both increment and add function should run for at least one second. It means that in this cell we are not surely 
# executing any computation; we're only saving x,y and z as delayed objects
14/6:
%%time

z.compute() 
# the precessing time decreased from 3 to 2 seconds. Indeed the parallelized 
# object allowed x and y to be both ready to be parallel executed in order to make the sum 
# just after they have been ran.
14/7: type(z)
14/8: type(z)
10/181: client.close()
 9/1:
%%bash

dask-scheduler --help
 9/2:
%%bash

dask-worker --help
 9/3:
%%bash 

ifconfig ##only for those of you that does not use the docker cluster
 9/4:
%%bash 

ifconfig ##only for those of you that does not use the docker cluster
 9/5:
from sklearn.datasets import fetch_20newsgroups
from dask.distributed import Client
import time

categories = [
     'comp.graphics',
     'comp.os.ms-windows.misc',
     'comp.sys.ibm.pc.hardware',
     'comp.sys.mac.hardware',
     'comp.windows.x',
     'misc.forsale',
     'rec.autos',
     'rec.motorcycles',
     'rec.sport.baseball',
     'rec.sport.hockey',
     'sci.crypt',
     'sci.electronics',
     'sci.med',
     'sci.space'
]

dataset = fetch_20newsgroups(subset='train', categories=categories ).data

print("Texts document present on the dataset: "+str(len(dataset)))

def count_word_in_statement(text):
    """
    This function takes a text as input and return the number of the words that it contains
    """
    #time.sleep(0.1)
    splitted_words = text.split()
    return len(splitted_words)
 9/6:
import time
start = time.time()


total_words_in_all_data = 0
for index in range(0, len(dataset)):
    total_words_in_all_data = total_words_in_all_data + count_word_in_statement(dataset[index])

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
 9/7: client = Client('') #change your setting
 9/8: client = Client() #change your setting
 9/9: client.close()
9/10: client = Client() #change your setting
9/11: client.close('8787')
9/12: client = Client() #change your setting
10/182: client.close()
9/13: client.scheduler_info()
9/14: client.close('dask-scheduler:8786')
9/15: client.close('8786')
9/16: client.close('8787')
9/17: client = Client() #change your setting
9/18:
import time
start = time.time()

#futures = client.map(count_word_in_statement, dataset)
futures = [client.submit(count_word_in_statement, data) for data in dataset]

futures = client.submit(sum, futures)
total_words_in_all_data = client.gather(futures)

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/19:
client = Client() #change your setting
# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/20:
import time
start = time.time()

#futures = client.map(count_word_in_statement, dataset)
futures = [client.submit(count_word_in_statement, data) for data in dataset]

futures = client.submit(sum, futures)
total_words_in_all_data = client.gather(futures)

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/21: client.close()
9/22:
import time
start = time.time()

#futures = client.map(count_word_in_statement, dataset)
futures = [client.submit(count_word_in_statement, data) for data in dataset]

futures = client.submit(sum, futures)
total_words_in_all_data = client.gather(futures)

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/23:
client = Client() #change your setting
# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/24:
client = Client("tcp://127.0.0.1:46485") #change your setting
# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/25:
client = Client("://127.0.0.1:46485") #change your setting
# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/26:
client = Client("8787") #change your setting
# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/27:
client = Client(8787) #change your setting
# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/28:
client = Client('127.0.0.1:8786')

# if i opened the client, meaning both the schedulers and the worker it is not necessary to open the client on jupyter! To close the client 
# manually it is enough only to shut the promtp window down
9/29:
import time
start = time.time()

#futures = client.map(count_word_in_statement, dataset)
futures = [client.submit(count_word_in_statement, data) for data in dataset]

futures = client.submit(sum, futures)
total_words_in_all_data = client.gather(futures)

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/30: client.close()
9/31:
def count_word_in_statement(text):
    """
    This function takes a text as input and return the number of the words that it contains
    """
    splitted_words = text.split()
    time.sleep(0.01)
    return len(splitted_words)
9/32:
import time
start = time.time()


total_words_in_all_data = 0
for index in range(0, len(dataset)):
    total_words_in_all_data = total_words_in_all_data + count_word_in_statement(dataset[index])
    end = time.time() - start
    if end >= 50:
        print("More than {}s of computation time...".format(end))

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/33:
import time
start = time.time()


total_words_in_all_data = 0
for index in range(0, len(dataset)):
    total_words_in_all_data = total_words_in_all_data + count_word_in_statement(dataset[index])
    end = time.time() - start
    if end >= 50:
        print("More than {}s of computation time...".format(end))

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/34: client = Client()#'dask-scheduler:8786') #change your setting
9/35:
import time
start = time.time()

futures = [client.submit(count_word_in_statement, data) for data in dataset]
futures = client.submit(sum, futures)
total_words_in_all_data = client.gather(futures)

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/36:
import time
start = time.time()

futures = [client.submit(count_word_in_statement, data) for data in dataset]
futures = client.submit(sum, futures)
total_words_in_all_data = client.gather(futures)

    
end = time.time()
print("Total word in the dataset: {}".format(total_words_in_all_data))
print("Computation took {}s".format(end-start))
9/37: client.close()
18/1:
import numpy as np
client = Client('127.0.0.1:8786') ##change your settings
18/2:
import numpy as np
from dask.distributed import Client
client = Client('127.0.0.1:8786') ##change your settings
18/3: client
18/4:
import numpy as np
from dask.distributed import Client
client = Client('127.0.0.1:8786') ##change your settings
18/5: client
18/6: client.close('127.0.0.1:8786')
18/7: client.close()
17/1:
from dask.distributed import Client

# Leave blank to setup a local cluster. Put the scheduler IP to distributed cluster.

client = Client()
17/2: client
17/3:
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci_sequential(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci.append(1)
    elif num == 2:
        fibonacci = []
        fibonacci.append(1)
        fibonacci.append(1)
    elif num > 2:
        fibonacci = []
        fibonacci.append(1)
        fibonacci.append(1)
        while i < (num - 1):
            fibonacci.append(fibonacci[i] + fibonacci[i-1])
            i += 1
    return fibonacci

result = fibonacci_sequential(5)
print("The first n fibonacci numbers are: " + str(result))
17/4:
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(5)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/5:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci_sequential(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci.append(1)
    elif num == 2:
        fibonacci = []
        fibonacci.append(1)
        fibonacci.append(1)
    elif num > 2:
        fibonacci = []
        fibonacci.append(1)
        fibonacci.append(1)
        while i < (num - 1):
            fibonacci.append(fibonacci[i] + fibonacci[i-1])
            i += 1
    return fibonacci

result = fibonacci_sequential(5)
print("The first n fibonacci numbers are: " + str(result))
17/6:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(5)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/7:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci_sequential(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci.append(1)
    elif num == 2:
        fibonacci = []
        fibonacci.append(1)
        fibonacci.append(1)
    elif num > 2:
        fibonacci = []
        fibonacci.append(1)
        fibonacci.append(1)
        while i < (num - 1):
            fibonacci.append(fibonacci[i] + fibonacci[i-1])
            i += 1
    return fibonacci

result = fibonacci_sequential(10)
print("The first n fibonacci numbers are: " + str(result))
17/8:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(10)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/9:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(2)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/10:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(3)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/11:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(10)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/12:
%time
from dask import delayed

def append(arr = [], val = 0):
    if val != None:
        arr.append(val)
    return arr

def fibonacci(num):
    i = 1
    if num == 0:
        fibonacci = []
    elif num == 1:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
    elif num == 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,2)
    elif num > 2:
        fibonacci = []
        fibonacci = delayed(append)(fibonacci,1)
        fibonacci = delayed(append)(fibonacci,1)
        while i < (num - 1):
            fibonacci = delayed(append)(fibonacci,(fibonacci[i] + fibonacci[i-1]))
            i += 1
    return fibonacci            

future = fibonacci(10)
display(future.visualize(rankdir="LR"))
print(future.compute())
17/13: client.close()
18/8:
import numpy as np
from dask.distributed import Client
client = Client('127.0.0.1:8786') ##change your settings
18/9:
import numpy as np
from dask.distributed import Client
client = Client('127.0.0.1:8786') ##change your settings
18/10: client
18/11: close.client()
18/12: client.close()
18/13: client
18/14:
import numpy as np
from dask.distributed import Client
client = Client('127.0.0.1:8786') ##change your settings
18/15: client
18/16: client.close()
18/17: client = Client()
18/18: client
18/19: client.close()
18/20: client
18/21: client.close()
18/22: client
18/23: client.close()
18/24:
import numpy as np
from dask.distributed import Client
client = Client('127.0.0.1:8786') ##change your settings
18/25: client
18/26: client.close()
18/27: client
22/1:
import os
import dask
from distributed import Client
import pandas as pd
import dask.dataframe as dd

c = Client()

# start to read a series of CSV

from glob import iglob

path = os.path.join('data', 'accounts.*.csv')

all_rec = iglob(path, recursive=True)     
dataframes = (pd.read_csv(f) for f in all_rec)
big_dataframe = pd.concat(dataframes, ignore_index=True)

## if in all your workers the data are in the same position you can read the 
# data directly with dask dataframe 
#### SAMUELE RICORDATI CHE TRAMITE CTRL + DOVE PREMI  SULLA TASTIERA COMMENTI LE RIGHE 
filename = os.path.join('data', 'accounts.*.csv')
df = dd.read_csv(filename) 

# read them into  a distributed dataset
#df = dd.from_pandas(big_dataframe, npartitions=20)

# load and count number of rows
df.head()
22/2: c.close()
22/3: %time len(df)
22/4: c.close()
25/1:
import os
import dask
from distributed import Client
import pandas as pd
import dask.dataframe as dd

c = Client()

# start to read a series of CSV

from glob import iglob

path = os.path.join('data', 'accounts.*.csv')

all_rec = iglob(path, recursive=True)     
dataframes = (pd.read_csv(f) for f in all_rec)
big_dataframe = pd.concat(dataframes, ignore_index=True)
# read them into  a distributed dataset
df = dd.from_pandas(big_dataframe, npartitions=20)

# load and count number of rows
df.head()
25/2:
%time
len(df)
25/3:
## if in all your workers the data are in the same position you can read the 
# data directly with dask dataframe 
#### SAMUELE RICORDATI CHE TRAMITE CTRL + DOVE PREMI  SULLA TASTIERA COMMENTI LE RIGHE 

filename = os.path.join('data', 'accounts.*.csv')
df = dd.read_csv(filename)
25/4: %time len(df)
25/5:
## if in all your workers the data are in the same position you can read the 
# data directly with dask dataframe 
#### SAMUELE RICORDATI CHE TRAMITE CTRL + DOVE PREMI  SULLA TASTIERA COMMENTI LE RIGHE 

filename = os.path.join('data', 'accounts.*.csv')
df = dd.read_csv(filename) 
df.head()
25/6: c.close()
25/7:
from glob import iglob
import pandas as pd

path = os.path.join('data', 'nycflights', '*.csv')

all_rec = iglob(path, recursive=True)     
#dataframes = (pd.read_csv(f, parse_dates={'Date': [0, 1, 2]}) for f in all_rec)
#big_dataframe = pd.concat(dataframes, ignore_index=True)

# read directly with pandas --> the file must be present in all you workers
df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),
                 parse_dates={'Date': [0, 1, 2]})

#df = dd.from_pandas(big_dataframe, npartitions=2)
#df
25/8:
# let's see how it's structured the dataset (partitions, type of each column...)
df 

#try to repartite the df and see how change
df = df.repartition(npartitions=8)
df
25/9:
# take a look to the first elments
df.head()
25/10:
# this take some samples from the end of the dataset, try to run and see how happen
df.tail()
25/11: c.close()
25/12:
c = Client()
df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),
                 parse_dates={'Date': [0, 1, 2]})

df.tail()
25/13:

df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),
                 parse_dates={'Date': [0, 1, 2]},
                 dtype={'TailNum': str,
                        'CRSElapsedTime': float,
                        'Cancelled': bool})

#now it works!
df.tail()
25/14: c.close()
25/15:
c = Client()
c
25/16:
import pandas as pd
import time
filenames = [
    os.path.join('data', 'nycflights', '1993.csv'),
    os.path.join('data', 'nycflights', '1994.csv'),
    os.path.join('data', 'nycflights', '1998.csv'),
    os.path.join('data', 'nycflights', '1999.csv')
]

start_time = time.time()
print(filenames)
maxes = []
for fn in filenames:
    pdf = pd.read_csv(fn)
    time.sleep(0.5) ##dummy for represent complex computation
    maxes.append(pdf.DepDelay.max())
    
final_max = 0
for m in maxes:
    if m >= final_max:
        final_max = m
end_time = time.time()
print("Times in seconds: "+str(end_time-start_time))
print(final_max)
25/17:
start_time = time.time()
final_max = df.DepDelay.max().compute()
end_time = time.time()
print("Times in seconds: "+str(end_time-start_time))
print(final_max)
25/18: df.DepDelay.max().visualize()
25/19:
%time
lengths = []
for fn in filenames:
    pdf = pd.read_csv(fn)
    lengths.append(len(pdf))
length = sum(lengths).compute().visualize()
25/20:
%time
lengths = []
for fn in filenames:
    pdf = pd.read_csv(fn)
    lengths.append(len(pdf))
length = sum(lengths).compute()
25/21:
%time
lengths = []
for fn in filenames:
    pdf = pd.read_csv(fn)
    lengths.append(len(pdf))
length = sum(lengths)
type(length)
25/22:
%time

len(df).compute().visualize
25/23:
%time

df.len().compute().visualize
25/24:
%time

df.shape[0].compute().visualize()
25/25:
%time

df.shape[0].visualize()
25/26:
%time

nrows= df.shape[0].compute()
print("The total number of rows is {}"nrows)
25/27:
%time

nrows= df.shape[0].compute()
print("The total number of rows is {}"\nrows)
25/28:
%time

nrows= df.shape[0].compute()
print("The total number of rows is {}"/nrows)
25/29:
%time

nrows= df.shape[0].compute()
print("The total number of rows is {}",nrows)
25/30:
%time

nrows= df.shape[0].compute()
print("The total number of rows is",nrows)
df.shape[0].visualize()
25/31: df
25/32: df.head()
25/33:
%time
# In order to count the number of non cancelled flights we can see how many rows for 
# each dask partition have the "delayed" column equal to "False". 

df[df.Cancelled == False].compute()
#print("Non cancelled flights: "+)
25/34:
%time
# In order to count the number of non cancelled flights we can see how many rows for 
# each dask partition have the "delayed" column equal to "False". 

fdf = df[df.Cancelled == False]
nc = fdf.shape[0].compute() 
#print("Non cancelled flights: "+)
25/35:
%time
# In order to count the number of non cancelled flights we can see how many rows for 
# each dask partition have the "delayed" column equal to "False". 

fdf = df[df.Cancelled == False]
nc = fdf.shape[0].compute() 
print("Non cancelled flights: "+ nc)
25/36:
%time
# In order to count the number of non cancelled flights we can see how many rows for 
# each dask partition have the "delayed" column equal to "False". 

fdf = df[df.Cancelled == False]
nc = fdf.shape[0].compute() 
print("Non cancelled flights: "+ str(nc))
25/37:
%time
# In order to count the number of non cancelled flights we can see how many rows for 
# each dask partition have the "delayed" column equal to "False". 

fdf = df[df.Cancelled == False]
nc = fdf.shape[0].compute() 
print("Non cancelled flights: "+ str(nc))

fdf.shape[0].visualize()
25/38:
print("Cancelled per airport: "+str(df[df.Cancelled==False].groupby('Origin').Origin.count().compute()))
df[df.Cancelled==False].groupby('Origin').Origin.count().visualize()
25/39:
print("Departure mean delay for each airport: "+str(df.groupby("Origin").DepDelay.mean().compute()))
df.groupby("Origin").DepDelay.mean().visualize()
25/40:
print("Departure mean delay for each airport: "+str(df.groupby("DayOfWeek").DepDelay.mean().compute()))
df.groupby("DayOfWeek").DepDelay.mean().visualize()
25/41:
%time
print("Cancelled per airport: "+str(df[df.Cancelled==False].groupby('Origin').Origin.count().compute()))
df[df.Cancelled==False].groupby('Origin').Origin.count().visualize()
25/42:
%time
print("Departure mean delay for each airport: "+str(df.groupby("Origin").DepDelay.mean().compute()))
df.groupby("Origin").DepDelay.mean().visualize()
25/43:
%time
print("Departure mean delay for each airport: "+str(df.groupby("DayOfWeek").DepDelay.mean().compute()))
df.groupby("DayOfWeek").DepDelay.mean().visualize()
25/44:
non_cancelled = df[~df.Cancelled]
mean_delay = non_cancelled.DepDelay.mean()
std_delay = non_cancelled.DepDelay.std()
25/45:
%%time

mean_delay_res = mean_delay.compute()
std_delay_res = std_delay.compute()
25/46:
%%time
mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)
25/47: dask.visualize(mean_delay, std_delay)
25/48: c.close()
26/1:
from distributed import Client
client = Client()
26/2:

import os
import dask
import dask.dataframe as dd

df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),
                 parse_dates={'Date': [0, 1, 2]},
                 dtype={'TailNum': str,
                        'CRSElapsedTime': float,
                        'Cancelled': bool})



crs_dep_time = df.CRSDepTime.head(100)
crs_dep_time
26/3:
%time
import pandas as pd

# Get the first 10 dates to complement our `crs_dep_time`
date = df.Date.head(100)

# Get hours as an integer, convert to a timedelta
hours = crs_dep_time // 100
hours_timedelta = pd.to_timedelta(hours, unit='h')

# Get minutes as an integer, convert to a timedelta
minutes = crs_dep_time % 100
minutes_timedelta = pd.to_timedelta(minutes, unit='m')

# Apply the timedeltas to offset the dates by the departure time
departure_timestamp = date + hours_timedelta + minutes_timedelta
departure_timestamp
26/4:
%time 
hours = df.CRSDepTime // 100
# hours_timedelta = pd.to_timedelta(hours, unit='h')
hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')

minutes = df.CRSDepTime % 100
# minutes_timedelta = pd.to_timedelta(minutes, unit='m')
minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')

departure_timestamp = df.Date + hours_timedelta + minutes_timedelta
26/5: departure_timestamp
26/6: departure_timestamp.head()
26/7:
def compute_departure_timestamp(df):
    #write your code here 
    hours = df.CRSDepTime // 100
    # hours_timedelta = pd.to_timedelta(hours, unit='h')
    hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')

    minutes = df.CRSDepTime % 100
    # minutes_timedelta = pd.to_timedelta(minutes, unit='m')
    minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')

    departure_timestamp = df.Date + hours_timedelta + minutes_timedelta
    return df
26/8: departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/9:
def compute_departure_timestamp(df):
    #write your code here 
    hours = df.CRSDepTime // 100
    # hours_timedelta = pd.to_timedelta(hours, unit='h')
    hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')

    minutes = df.CRSDepTime % 100
    # minutes_timedelta = pd.to_timedelta(minutes, unit='m')
    minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')

    df.Date + hours_timedelta + minutes_timedelta
    return df
26/10: departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/11:
def compute_departure_timestamp(df):
    #write your code here 
    hours = df.CRSDepTime // 100
    # hours_timedelta = pd.to_timedelta(hours, unit='h')
    hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')

    minutes = df.CRSDepTime % 100
    # minutes_timedelta = pd.to_timedelta(minutes, unit='m')
    minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')

    d = df.Date + hours_timedelta + minutes_timedelta
    return d
26/12: departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/13:
def compute_departure_timestamp(df):
    #write your code here 
    hours = df.CRSDepTime // 100
    # hours_timedelta = pd.to_timedelta(hours, unit='h')
    hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')

    minutes = df.CRSDepTime % 100
    # minutes_timedelta = pd.to_timedelta(minutes, unit='m')
    minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')

    df = df.Date + hours_timedelta + minutes_timedelta
    return d
26/14: departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/15:
def compute_departure_timestamp(df):
    #write your code here 
    hours = df.CRSDepTime // 100
    # hours_timedelta = pd.to_timedelta(hours, unit='h')
    hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')

    minutes = df.CRSDepTime % 100
    # minutes_timedelta = pd.to_timedelta(minutes, unit='m')
    minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')

    departure_timestamp = df.Date + hours_timedelta + minutes_timedelta
    return departure_timestamp
26/16: departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/17:
def compute_departure_timestamp(df):
    #write your code here 
    hours = df.CRSDepTime // 100
    # hours_timedelta = pd.to_timedelta(hours, unit='h')
    hours_timedelta = pd.to_timedelta(hours, unit='h')

    minutes = df.CRSDepTime % 100
    # minutes_timedelta = pd.to_timedelta(minutes, unit='m')
    minutes_timedelta = pd.to_timedelta(minutes, unit='m')

    departure_timestamp = df.Date + hours_timedelta + minutes_timedelta
    return departure_timestamp
26/18: departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/19: departure_timestamp.head()
26/20:
%time 
departure_timestamp = df.map_partitions(compute_departure_timestamp)
26/21: client.close()
26/22: client.close()
28/1:
import os
from dask.distributed import Client
c = Client('dask-scheduler:8786')
filename = os.path.join('data', 'accounts.*.csv')
filename
28/2:
import os
from dask.distributed import Client
c = Client()
filename = os.path.join('data', 'accounts.*.csv')
filename
28/3:
import dask.dataframe as dd
df_csv = dd.read_csv(filename)
df_csv.head()
28/4:

target = os.path.join(os.getcwd()+'/data', 'accounts.h5')
target
28/5:
def install():
    import os
    os.system("pip3 install s3fs tables")  # or pip

c.run(install)
# dask allows to install the package at runtime but only on the worker and not for 
# all clusters
28/6: !pip install s3fs tables
28/7:
%%time 
import os

df_csv.to_hdf(target, os.getcwd()+'/data')
28/8:

# same data as before
df_hdf = dd.read_hdf(target, os.getcwd()+'/data', mode='r')
df_hdf.head()
28/9: %time df_csv.amount.sum().compute()
28/10:
%time df_hdf.amount.sum().compute()
# it spent more time than with csv format
28/11:
target = os.path.join(os.getcwd()+'/data', 'accounts_optimized.h5')
%time df_hdf.categorize(columns=['names']).to_hdf(target, os.getcwd()+'/data')
28/12:
df_hdf = dd.read_hdf(target, os.getcwd()+'/data', mode='r')
df_hdf.head()
28/13:

# But loads more quickly
%time df_hdf.amount.sum().compute()
28/14:
# this file allows to load data from a Amazon S3 Cloud storage, 
# unfortunately the file is larger then 2GB on our machines dont' allows to you to download it!
# Feel free to try it on you own machine at home!
# taxi = dd.read_csv('s3://nyc-tlc/trip data/yellow_tripdata_2015-*.csv')

taxi = dd.read_csv('https://www.dropbox.com/s/17ui51hwpoqeb4p/fhv_tripdata_2015-01.csv?dl=1')
taxi
28/15: taxi.head()
28/16: taxi.tail()
28/17:
taxi = taxi.dropna()
taxi.head()
taxi = taxi.repartition(npartitions=12)
28/18: client.close()
28/19: c.close()
29/1:
from dask.distributed import Client
client = Client()
client
29/2: client.close()
29/3: client.close()
29/4: client.close()
29/5: client.close()
29/6: c.close()
29/7: client.close()
29/8:
from dask.distributed import Client
client = Client()
client
29/9:
import dask
import dask.dataframe as dd
df = dask.datasets.timeseries(start='2019-03-01', end='2019-04-30')
29/10: df
29/11: df.dtypes
29/12:
import pandas as pd
pd.options.display.precision = 2
pd.options.display.max_rows = 10
29/13: df.head(3)
29/14:
df2 = df[df.y > 0]
df3 = df2.groupby('name').x.std()
df3
29/15:
computed_df = df3.compute()
type(computed_df)
29/16: computed_df
29/17: df = df.persist()
29/18:
%matplotlib inline

df[['x', 'y']].resample('1h').mean().head()
29/19: df[['x', 'y']].resample('24h').mean().compute().plot()
29/20: df[['x', 'y']].rolling(window='24h').mean().head()
29/21: df.loc['2019-04-05']
29/22: %time df.loc['2019-04-05'].compute()
29/23:
df = df.set_index('name')
df
29/24: df = df.persist()
29/25: %time df.loc['Alice'].compute()
29/26:
from  sklearn.linear_model import LinearRegression

def train(partition):
    est = LinearRegression()
    est.fit(partition[['x']].values, partition.y.values)
    return est

df.groupby('name').apply(train, meta=object).compute()
29/27: c.close()
29/28: client.close()
50/1:
from dask import delayed
import time

def increment(x):
    print("I'm icrementing "+str(x)+" on this cluster node!")
    time.sleep(5)
    return x + 1

def decrement(x):
    print("I'm decrementing "+str(x)+" on this cluster node!")
    time.sleep(3)
    return x - 1

def add(x, y):
    print("I'm summing "+str(x)+" and "+str(y)+" on this cluster node!")
    time.sleep(7)
    return x + y
50/2:
from dask.distributed import Client

# Leeve blank to setup a local cluster. Put the scheduler IP to distributed cluster.

client = Client()
client.cluster
50/3: client
50/4:
x = delayed(increment)(1)
y = delayed(decrement)(2)
total = delayed(add)(x, y)
total.compute()
50/5: future = client.submit(increment, 1)
50/6: future
50/7:

client.gather(future)
50/8:
data = [0, 1, 2, 3, 4, 5, 6, 7, 8]

future_results = client.map(increment, data)
client.gather(future_results)
50/9:
x = client.submit(increment, 1)
y = client.submit(decrement, 2)
total = client.submit(add, x, y)

print(total)     # This is still the execution promise
client.gather(total)  # This is the final result
50/10: client.close()
52/1: from dask import delayed
52/2:
from time import sleep


def increment(x):
    """
    take a number x and return x+1
    sleep for 1s
    """
    sleep(1)
    return x + 1

def add(x, y):
    """
    sum y to x and return the result
    sleep for 1s
    """
    sleep(1)
    return x + y
52/3:
%%time

x = increment(1)
y = increment(2)
z = add(x, y)
# each function runs for at least one second since we 
# added sleep(1). In total the time spent is almost equal
# to three seconds since the real time of execution 
# has been covered by this sleep function
52/4:
%%time

# the delayed function takes several arguments. the first argument is the function that has to be executed in parallel.
# the following arguments are the arguments of the original function.
from dask import delayed

x = delayed(increment)(1)
y = delayed(increment)(2)
z = delayed(add)(x, y)
# we know that both increment and add function should run for at least one second. It means that in this cell we are not surely 
# executing any computation; we're only saving x,y and z as delayed objects
52/5:
%%time

z.compute() 
# the precessing time decreased from 3 to 2 seconds. Indeed the parallelized 
# object allowed x and y to be both ready to be parallel executed in order to make the sum 
# just after they have been ran.
52/6: z.visualize(rankdir="LR")
56/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db


url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
data_names = requests.get(url, verify = False).text
data_names = data_names.split(sep = '\n')
56/2: data_names
56/3:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re

url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
data_names = requests.get(url, verify = False).text
data_names = data_names.split(sep = '\n')
56/4:
# questa  la funzione che devo rendere tale da poterla applicare per leggere i file e importare il contenuto
# in una dask bag 

fp = "data_000000.dat"
m = np.fromfile(fp, dtype = '<u8') # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

mi = m#.astype(int) # non serve, anzi meglio evitarlo cos resta tutto unsigned

mat = np.zeros((mi.shape[0], 6), dtype = "u4") # orbit raggiunge i 32 bit

mat[:,0] = mi & int('1' * 5, 2) # tdc, 0-4 (5)
mat[:,1] = (mi & int('1' * 12 + '0' * 5, 2)) >> 5 # bx, 5-16 (12)
mat[:,2] = (mi & int('1' * 32 + '0' * 17, 2)) >> 17 # orbit, 17-48 (32)
mat[:,3] = (mi & int('1' * 9 + '0' * 49 , 2)) >> 49 # chan, 49-57 (9)
mat[:,4] = (mi & int('1' * 3 + '0' * 58, 2)) >> 58 # fpga (3)
#mat[:,5] = (mi & int('1' * 3 + '0' * 61, 2)) >> 61 # head (3)
# la riga sopra fallisce a causa di un overflow, sarebbe un bitwise di un numero con un array troppo "stretto" rispetto all'altro numero
# meglio prima traslare e poi fare l'and
mat[:,5] = mi >> 61 # del resto basta questo,  ovvio che se tolgo gli ultimi 61 bits restano solo i primi 3

#print(mat[1])
#print(mat.dtype)
df_little = pd.DataFrame(mat, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
display(df_little)
56/5:
# questa  la funzione che devo rendere tale da poterla applicare per leggere i file e importare il contenuto
# in una dask bag 

fp = "data_000000.dat"
m = np.fromfile(*data_names, dtype = '<u8') # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

mi = m#.astype(int) # non serve, anzi meglio evitarlo cos resta tutto unsigned

mat = np.zeros((mi.shape[0], 6), dtype = "u4") # orbit raggiunge i 32 bit

mat[:,0] = mi & int('1' * 5, 2) # tdc, 0-4 (5)
mat[:,1] = (mi & int('1' * 12 + '0' * 5, 2)) >> 5 # bx, 5-16 (12)
mat[:,2] = (mi & int('1' * 32 + '0' * 17, 2)) >> 17 # orbit, 17-48 (32)
mat[:,3] = (mi & int('1' * 9 + '0' * 49 , 2)) >> 49 # chan, 49-57 (9)
mat[:,4] = (mi & int('1' * 3 + '0' * 58, 2)) >> 58 # fpga (3)
#mat[:,5] = (mi & int('1' * 3 + '0' * 61, 2)) >> 61 # head (3)
# la riga sopra fallisce a causa di un overflow, sarebbe un bitwise di un numero con un array troppo "stretto" rispetto all'altro numero
# meglio prima traslare e poi fare l'and
mat[:,5] = mi >> 61 # del resto basta questo,  ovvio che se tolgo gli ultimi 61 bits restano solo i primi 3

#print(mat[1])
#print(mat.dtype)
df_little = pd.DataFrame(mat, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
display(df_little)
56/6:
# questa  la funzione che devo rendere tale da poterla applicare per leggere i file e importare il contenuto
# in una dask bag 

@jit
for i in data_names:
    m = np.fromfile(i, dtype = '<u8') # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

    mi = m#.astype(int) # non serve, anzi meglio evitarlo cos resta tutto unsigned

    mat = np.zeros((mi.shape[0], 6), dtype = "u4") # orbit raggiunge i 32 bit

    mat[:,0] = mi & int('1' * 5, 2) # tdc, 0-4 (5)
    mat[:,1] = (mi & int('1' * 12 + '0' * 5, 2)) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & int('1' * 32 + '0' * 17, 2)) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & int('1' * 9 + '0' * 49 , 2)) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & int('1' * 3 + '0' * 58, 2)) >> 58 # fpga (3)
    #mat[:,5] = (mi & int('1' * 3 + '0' * 61, 2)) >> 61 # head (3)
    # la riga sopra fallisce a causa di un overflow, sarebbe un bitwise di un numero con un array troppo "stretto" rispetto all'altro numero
    # meglio prima traslare e poi fare l'and
    mat[:,5] = mi >> 61 # del resto basta questo,  ovvio che se tolgo gli ultimi 61 bits restano solo i primi 3

    #print(mat[1])
    #print(mat.dtype)
    df_little = pd.DataFrame(mat, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
    display(df_little)
56/7:
# questa  la funzione che devo rendere tale da poterla applicare per leggere i file e importare il contenuto
# in una dask bag 

for i in data_names:
    m = np.fromfile(i, dtype = '<u8') # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

    mi = m#.astype(int) # non serve, anzi meglio evitarlo cos resta tutto unsigned

    mat = np.zeros((mi.shape[0], 6), dtype = "u4") # orbit raggiunge i 32 bit

    mat[:,0] = mi & int('1' * 5, 2) # tdc, 0-4 (5)
    mat[:,1] = (mi & int('1' * 12 + '0' * 5, 2)) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & int('1' * 32 + '0' * 17, 2)) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & int('1' * 9 + '0' * 49 , 2)) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & int('1' * 3 + '0' * 58, 2)) >> 58 # fpga (3)
    #mat[:,5] = (mi & int('1' * 3 + '0' * 61, 2)) >> 61 # head (3)
    # la riga sopra fallisce a causa di un overflow, sarebbe un bitwise di un numero con un array troppo "stretto" rispetto all'altro numero
    # meglio prima traslare e poi fare l'and
    mat[:,5] = mi >> 61 # del resto basta questo,  ovvio che se tolgo gli ultimi 61 bits restano solo i primi 3

    #print(mat[1])
    #print(mat.dtype)
    df_little = pd.DataFrame(mat, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
    display(df_little)
56/8: client = Client()
56/9: client
56/10:
def load_from_file(data):
    m = np.fromfile(data, dtype = '<u8')
b = db.from_sequence(data_names).map(load_from_file)
56/11: b
56/12: b.take(1)
56/13: b.take(2)
56/14: b.take(3)
56/15:
def load_from_file(data):
    m = np.fromfile(data, dtype = '<u8')
    return m
b = db.from_sequence(data_names).map(load_from_file)
56/16: b.take(3)
56/17:
m = np.fromfile('data_000000.dat', dtype = '<u8') # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

m
56/18: b.take(1)
56/19: b.take(1) == b.take(2)
56/20: (b.take(2)).any(b.take(1))
56/21: (b.take(1) == b.take(2)).all()
56/22: np.allclose(b.take(1), b.take(2))
56/23:
path = os.path.join('data', 'accounts.*.csv')

path
56/24:
path = os.path.join('data', 'accounts.*.csv')

all_rec = iglob(path, recursive=True)     
#dataframes = (pd.read_csv(f) for f in all_rec)
56/25:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
data_names = requests.get(url, verify = False).text
data_names = data_names.split(sep = '\n')
56/26:
path = os.path.join('data', 'accounts.*.csv')

all_rec = iglob(path, recursive=True)     
#dataframes = (pd.read_csv(f) for f in all_rec)
56/27:
path = os.path.join('data', 'accounts.*.csv')

all_rec = iglob(path, recursive=True)     
#dataframes = (pd.read_csv(f) for f in all_rec)
all_rec
56/28:
path = os.path.join('data_00000.*.dat')

all_rec = iglob(path, recursive=True)     
#dataframes = (pd.read_csv(f) for f in all_rec)
all_rec
56/29:
path = os.path.join('data_0000*.dat')

all_rec = iglob(path, recursive=True)     
#dataframes = (pd.read_csv(f) for f in all_rec)
for i in all_rec: print(i)
56/30:
def load_from_file(all_rec):
    path = os.path.join('data_0000*.dat')
    all_rec = iglob(path, recursive=True) 
    m = np.fromfile(all_rec, dtype = '<u8')
    return m
b = db.from_sequence(data_names).map(load_from_file)
56/31: b.take(1)
56/32:
def load_from_file(all_rec):
    path = os.path.join('data_0000*.dat')
    all_rec = iglob(path, recursive=True) 
    m = (np.fromfile(f, dtype = '<u8') for f in all_rec)
    return m
b = db.from_sequence(data_names).map(load_from_file)
56/33: b
56/34: b.take(1)
56/35:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)

def load_from_file(all_rec):
    m = (np.fromfile(f, dtype = '<u8') for f in all_rec)
    return m

b = db.from_sequence(all_rec).map(load_from_file)
56/36: b.take(1)
56/37:
m = np.array(1,2)
m.append(1)
56/38:
m = np.array(1)
m.append(1)
56/39:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence(data_names).map(load_from_file)
56/40: b.take(1)
56/41: len(b.take(1))
56/42: b.take(1).shape()
56/43: b.take(1)
56/44: b.take(1).shape()
56/45: np.shape(b.take(1))
56/46: np.shape(b.take())
56/47: np.shape(b.take(2))
56/48: np.shape(b.take(81))
56/49: np.shape(b.take(82))
56/50: np.shape(b.take(83))
56/51: b
56/52: b.take(1)
56/53: b.take(2)
56/54:
m = np.fromfile('data_000001.dat', dtype = '<u8') 
m
56/55: data_names
56/56:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]
def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence(data_names).map(load_from_file)
56/57: data_names
56/58: b.take(1)
56/59: b.take(2)
56/60: b.take(3)
56/61: b
56/62:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence(data_names)
56/63: b
56/64: b.take(1)
56/65: b.take(2)
56/66: b.take(3)
56/67: b.take(10)
56/68: client.close()
57/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
57/2: client = Client()
57/3: client
57/4:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence(data_names)
57/5: data_names
57/6: b
57/7:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence(data_names).map(load_from_file)
57/8: b.take
57/9: b.take()
57/10: b.take(100000)
57/11: b.take(1)
57/12:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence('data_000001.dat').map(load_from_file)
57/13: b.take(1)
57/14:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence(['data_000001.dat']).map(load_from_file)
57/15: b.take(1)
57/16: data_names
57/17: data_names.pop('data_000000.dat')
57/18: data_names.pop()
57/19: data_names.pop(0)
57/20: data_names.del(0)
57/21: data_names.remove()
57/22: data_names.remove(0)
57/23: data_names.remove('data_000000.dat')
57/24: data_names[data_names != "data_000000.dat"]
57/25: data_names != "data_000000.dat"
57/26: data_names
57/27:
b = db.from_sequence(data_names).map(load_from_file)
b.take(1)
57/28:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence([])
for i in data_names:
    b.append(db.from_sequence(name).map(load_from_file))
57/29:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence([])
for i in data_names:
    b[i] = (db.from_sequence(name).map(load_from_file))
57/30:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

b = db.from_sequence([])
for i in data_names:
    b[i] = (db.from_sequence([i]).map(load_from_file))
57/31:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 
def load_from_file_all(files):
    m = (np.fromfile(file, dtype = '<u8') for file in files)
    return m 
b = db.from_sequence(data_names).map(load_from_file_all)
57/32: b.take(1)
57/33: data_names
57/34: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/35:
m = (np.fromfile(file, dtype = '<u8') for file in data_names)
m
57/36:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 
data = ['data_000000.dat', 'data_000001.dat']
def load_from_file_all(files):
    m = np.array()
    for file in files:
        m = np.concatenate(m, np.fromfile(file. dtype= '<u8'))
#     m = (np.fromfile(file, dtype = '<u8') for file in files)
    return m 

b = db.from_sequence(data).map(load_from_file_all)
57/37:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 
data = ['data_000000.dat', 'data_000001.dat']
def load_from_file_all(files):
    m = np.array()
    for file in files:
        m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = (np.fromfile(file, dtype = '<u8') for file in files)
    return m 

b = db.from_sequence(data).map(load_from_file_all)
57/38: b.take(1)
57/39:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 
data = ['data_000000.dat', 'data_000001.dat']
def load_from_file_all(files):
    m = np.array([])
    for file in files:
        m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = (np.fromfile(file, dtype = '<u8') for file in files)
    return m 

b = db.from_sequence(data).map(load_from_file_all)
57/40: b.take(1)
57/41:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 
data = ['data_000000.dat', 'data_000001.dat']
def load_from_file_all(files):
    m = np.array([])
    for file in files:
        m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = (np.fromfile(file, dtype = '<u8') for file in files)
    return m 

b = db.from_sequence(data_names).map(load_from_file_all)
57/42: b.take(1)
57/43:
for file in data_names:
    m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
57/44:
m = np.array([])
for file in data_names:
    m = np.append(m, np.fromfile(file, dtype= '<u8'))
57/45: m
57/46: len(m)
57/47: m[0]
57/48: data_names
57/49:
m = np.fromfile('data_000000.dat', dtype = '<u8') 
m[0]
57/50: m[0]*10**12
57/51: m[0]*10**-12
57/52: m[0]*10**-18
57/53:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8')
    m = np.append(m, n)
57/54: m[0]*10**-18
57/55:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8')
    m = np.vstack(m, n)
57/56:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8')
    m = np.vstack((m, n))
57/57:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8')
    print(n.shape())
#     m = np.vstack((m, n))
57/58:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8')
    print(n.flatten)
#     m = np.vstack((m, n))
57/59:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8')
    print(n.flatten())
#     m = np.vstack((m, n))
57/60:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8').flatten()
#     print(n.flatten())
    m = np.vstack((m, n))
57/61:
m = np.array()
for file in data_names:
    n = np.fromfile(file, dtype= '<u8').flatten()
#     print(n.flatten())
    m = np.append(m,n)
57/62:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8').flatten()
#     print(n.flatten())
    m = np.append(m,n)
57/63: m[0]
57/64:
m = np.array([])
for file in data_names:
    n = np.fromfile(file, dtype= '<u8').flatten()
#     print(n.flatten())
    m = np.vstack((m,n))
57/65:
m = np.array([]).flatten()
for file in data_names:
    n = np.fromfile(file, dtype= '<u8').flatten()
#     print(n.flatten())
    m = np.vstack((m,n))
57/66:
m = np.array([]).flatten()
for file in data_names:
    n = np.fromfile(file, dtype= '<u8').T
#     print(n.flatten())
    m = np.vstack((m,n))
57/67:
for i in data_names:
    print(i,i+1)
57/68: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/69: m
57/70: print(m)
57/71: dir(m)
57/72: [*m]
57/73: np.array(*m)
57/74: np.array((*m))
57/75: np.array([*m])
57/76: m = [*m]
57/77: m
57/78: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/79: m = [*m]
57/80: m
57/81: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/82: np.array([*m])
57/83: np.array([*m], dtype='<u8')
57/84: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/85: np.array([*m], dtype='<u8')
57/86: n = [*m]
57/87: n
57/88: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/89: n = [*m]
57/90: n
57/91: n = [*m]
57/92: n
57/93: m = (np.fromfile(file, dtype = '<u8') for file in data_names)
57/94: n = [*m]
57/95: n
57/96:
m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
m
57/97:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
m
57/98:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
    m = np.array([])
    for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file_all)
57/99:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
    m = np.array([])
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file_all)
57/100: b.take(1)
57/101:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).submit(load_from_file_all)
57/102:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file_all)
57/103: b.take(1)
57/104: b
57/105:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
57/106: b.take(1)
57/107:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8').flatten
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
57/108: b.take(1)
57/109:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8').flatten()
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
57/110: b.take(1)
57/111:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file):
    m = np.fromfile(file, dtype = '<u8')
    return m

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
57/112: b.take(1)
57/113:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file): np.fromfile(file, dtype = '<u8')

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
57/114: b.take(1)
57/115:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file): 
    return np.fromfile(file, dtype = '<u8')

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
57/116: b.take(1)
57/117: b.take(2)
57/118: len(b.take(2))
57/119: np.shape(b.take(2))
57/120: np.shape(b.take(1))
57/121: np.shape(b.take(3))
57/122:
# questa  la funzione che devo rendere tale da poterla applicare per leggere i file e importare il contenuto
# in una dask bag 


m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)] # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

mi = m#.astype(int) # non serve, anzi meglio evitarlo cos resta tutto unsigned

mat = np.zeros((mi.shape[0], 6), dtype = "u4") # orbit raggiunge i 32 bit

mat[:,0] = mi & int('1' * 5, 2) # tdc, 0-4 (5)
mat[:,1] = (mi & int('1' * 12 + '0' * 5, 2)) >> 5 # bx, 5-16 (12)
mat[:,2] = (mi & int('1' * 32 + '0' * 17, 2)) >> 17 # orbit, 17-48 (32)
mat[:,3] = (mi & int('1' * 9 + '0' * 49 , 2)) >> 49 # chan, 49-57 (9)
mat[:,4] = (mi & int('1' * 3 + '0' * 58, 2)) >> 58 # fpga (3)
#mat[:,5] = (mi & int('1' * 3 + '0' * 61, 2)) >> 61 # head (3)
# la riga sopra fallisce a causa di un overflow, sarebbe un bitwise di un numero con un array troppo "stretto" rispetto all'altro numero
# meglio prima traslare e poi fare l'and
mat[:,5] = mi >> 61 # del resto basta questo,  ovvio che se tolgo gli ultimi 61 bits restano solo i primi 3

#print(mat[1])
#print(mat.dtype)
df_little = pd.DataFrame(mat, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
display(df_little)
57/123:
# questa  la funzione che devo rendere tale da poterla applicare per leggere i file e importare il contenuto
# in una dask bag 


m = np.array([*(np.fromfile(file, dtype = '<u8') for file in data_names)]) # default: '<u8' che sarebbe la little endianess ovvero i bits significativi si trovano a destra della riga estratta dallo stream di dati. Inoltre per determinare la lunghezza di ciascuna riga all'interno di dtype  specificato  il  dtype='u8'. Poich le cpu sono in grado di leggere un file binario per bytes allora si richiede una riga letta alla volta che sia lunga 8 bytes cio 64 bits come avremmo voluto inizialmente. Inoltre per convertire successivamente la riga letta tramite i singoli bits, dunque con operazioni bitwise, nelle informazioni delle hits rivelate bisogna leggere tali stringhe come unsigned int e successivamente riconvertirle in singoli bits, dunque zeri e uni. Questo accade perch vogliamo analizzare ciascuna riga per bit ma per farlo dobbiamo passare necessariamente per i bytes visto che le cpu leggono a multipli di 8 bits. 

mi = m#.astype(int) # non serve, anzi meglio evitarlo cos resta tutto unsigned

mat = np.zeros((mi.shape[0], 6), dtype = "u4") # orbit raggiunge i 32 bit

mat[:,0] = mi & int('1' * 5, 2) # tdc, 0-4 (5)
mat[:,1] = (mi & int('1' * 12 + '0' * 5, 2)) >> 5 # bx, 5-16 (12)
mat[:,2] = (mi & int('1' * 32 + '0' * 17, 2)) >> 17 # orbit, 17-48 (32)
mat[:,3] = (mi & int('1' * 9 + '0' * 49 , 2)) >> 49 # chan, 49-57 (9)
mat[:,4] = (mi & int('1' * 3 + '0' * 58, 2)) >> 58 # fpga (3)
#mat[:,5] = (mi & int('1' * 3 + '0' * 61, 2)) >> 61 # head (3)
# la riga sopra fallisce a causa di un overflow, sarebbe un bitwise di un numero con un array troppo "stretto" rispetto all'altro numero
# meglio prima traslare e poi fare l'and
mat[:,5] = mi >> 61 # del resto basta questo,  ovvio che se tolgo gli ultimi 61 bits restano solo i primi 3

#print(mat[1])
#print(mat.dtype)
df_little = pd.DataFrame(mat, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
display(df_little)
57/124:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
m
57/125:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
m.head(2)
57/126:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
m
57/127: a = db.from_sequence(['data_000000.dat']).map(load_from_file)
57/128:
a = db.from_sequence(['data_000000.dat']).map(load_from_file)
a.take(1)
57/129:
a = db.from_sequence(['data_000000.dat']).map(load_from_file)
a.take(2)
57/130:
a = db.from_sequence(['data_000000.dat']).map(load_from_file)
a.take(3)
57/131:
a = db.from_sequence(['data_000000.dat', 'data_000001']).map(load_from_file)
a.take(1)
57/132:
a = db.from_sequence(['data_000000.dat', 'data_000001']).map(load_from_file)
np.shape(a.take(1))
57/133:
a = db.from_sequence(['data_000000.dat', 'data_000001.dat']).map(load_from_file)
np.shape(a.take(1))
57/134:
a = db.from_sequence(['data_000000.dat', 'data_000001.dat']).map(load_from_file)
np.shape(a.take(2))
57/135:
a = db.from_sequence(['data_000000.dat', 'data_000001.dat']).map(np.fromfile(dype = '<u8'))
np.shape(a.take(1))
57/136:
a = db.from_sequence(['data_000000.dat', 'data_000001.dat']).map(np.fromfile)
np.shape(a.take(1))
57/137:
a = db.from_sequence(['data_000000.dat', 'data_000001.dat']).map(np.fromfile)
np.shape(a.take(2))
57/138:
a = db.from_sequence(['data_000000.dat', 'data_000001.dat']).map(np.fromfile)
np.shape(a.take(3))
58/1: client.close()
58/2:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
58/3: client = Client()
58/4: client.close()
58/5: client.close()
58/6: client
59/1: data_names
59/2:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
59/3: client = Client()
59/4: client
59/5:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file): 
    return np.fromfile(file, dtype = '<u8')

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
59/6: data_names
59/7: b
59/8: np.shape(b.take(1)) == np.shape(b.take(2))
59/9:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
m
59/10: client.close()
60/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
60/2: client = Client()
60/3: client
60/4:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file): 
    return np.fromfile(file, dtype = '<u8')

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 

b = db.from_sequence(data_names).map(load_from_file)
60/5: data_names
60/6: b.take(1)
60/7: b
60/8: b.take(2)
60/9: b.take(1)
60/10: b.take(2)
60/11: b.take(3)
60/12: b.take(4)
60/13: np.shape(b.take(1)) == np.shape(b.take(2))
60/14: data_names
60/15:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file): 
    return np.fromfile(file, dtype = '<u8')

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 
data_names1 = ['data_000001.dat','data_000000.dat']
b = db.from_sequence(data_names1).map(load_from_file)
60/16: b
60/17: b.take(1)
60/18: b.take(2)
60/19: b.take(3)
60/20: b.to_dataframe()
60/21: client.close()
60/22: client = Client('78.13.47.54:8001') #cluster di mattia
60/23:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
60/24: dd.config.set({"distributed.comm.timeouts.tcp": "50s"})
60/25: client = Client('78.13.47.54:8001', timeout='30s') #cluster di mattia
60/26: client = Client('78.13.47.54:8001', timeout='100s') #cluster di mattia
62/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
62/2: client = Client()
62/3: client
62/4:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
# la funzione deve leggere un singolo file 
data_names = [i for i in all_rec]

def load_from_file(file): 
    return np.fromfile(file, dtype = '<u8')

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
    m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
    return m 
data_names1 = ['data_000001.dat','data_000000.dat']

b = db.from_sequence(data_names).map(load_from_file).flatten()
62/5: b.take(2)
62/6: b.take(10000)
62/7: b.take(1)
62/8: b.take(2)
62/9:
m1 = np.fromfile('data_000000.dat', dtype = '<u8')
m1[:2]
62/10:
m1 = np.fromfile('data_000000.dat', dtype = '<u8')
len(m1)
62/11: b.take(1310594)
62/12: # client = Client('78.13.47.54:8001') #cluster di mattia
62/13:
def convert(mi):
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
62/14:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
62/15: b.take(1)
62/16:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
62/17:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
62/18:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
62/19:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
62/20: b.take(1)
62/21: b.take(2)
62/22:
def read_from_file(file): return np.fromfile(file, dtype = '<u8')

bg = db.from_sequence(data_names).map(read_from_file)
62/23: len(m[0])
62/24:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
m
62/25: len(m[0])
62/26: bg.take(1310594)
62/27: bg.take(1310600)
62/28: len(m[0])
62/29: len(m[1])
62/30: bg.take(13107000)
62/31: bg.take(13107000000)
62/32: bg.take(13107000000000000)
62/33: bg.take(131070000000000000000000)
62/34: bg.take(13107000000000000000000)
62/35: bg.take(1310700000000000000000)
62/36: bg.take(131070000000000000000)
62/37: bg.take(13107000000000000000)
62/38: bg.take(1310700000000000000)
62/39: b
62/40:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
b
62/41: b.take(1)
62/42: b.take(2)
62/43: b.take(3)
62/44: b.take(100)
62/45: b.take(10)
62/46: b.take(8)
62/47: b.take(1)
62/48:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
62/49:
%%time
b1 = db.from_sequence(data_names).map(load_from_file).flatten()
b1
62/50:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
b.take(100)
62/51:
%%time
b1 = db.from_sequence(data_names).map(load_from_file).flatten()
b1.take(100)
62/52:
%%time
b1 = db.from_sequence(data_names).map(load_from_file).flatten()
b1.take(10)
62/53:
%%time
b1 = db.from_sequence(data_names).map(load_from_file).flatten()
b1.take(5)
62/54:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
b.take(5)
62/55:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
62/56: b.head()
62/57: b.head(10)
62/58:
%%time
b = db.from_sequence(data_names).map(load_from_file).flatten()
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
62/59: d.head(10)
62/60: d.compute()
62/61:
%%time
d.compute()
62/62:
%%time
d.groupby(['orbit'])
62/63: d.groupby(['orbit']).counts().compute()
62/64: d.groupby(['orbit']).compute()
62/65: d.groupby(['orbit'])
62/66: d.groupby(['orbit']).head()
62/67: d.groupby(['orbit'])
62/68: d.groupby(['orbit']).compute
62/69: d.groupby(['orbit']).compute()
62/70: d.groupby(['orbit'])
62/71: d.groupby(['orbit']).nunique().compute()
62/72: d.groupby(['orbit']).nunique()
62/73: d.groupby(['orbit']).groups.values()
62/74: d.groupby(['orbit']).values()
62/75: type(d.groupby(['orbit']))
62/76: dir(d.groupby(['orbit']))
62/77: d.groupby(['orbit']).count()
62/78: d.groupby(['orbit']).count().compute()
62/79:
%%time
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
62/80: d.groupby(['orbit']).count().compute()
62/81: d[d.groupby(['orbit']).count() < 15]
62/82:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
62/83: fin
62/84: df = pd.Dataframe(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
62/85: df = Dataframe(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
62/86: df = pd.Dataframe(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
62/87: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
62/88: fin
62/89: fin.flatten()
62/90: np.array(fin).flatten()
62/91: fin
62/92: fin[0]
62/93: np.concatenate(fin).ravel
62/94: np.concatenate(fin).ravel()
62/95: np.concatenate(fin)
62/96: fin = np.concatenate(fin)
62/97: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
62/98: df
62/99: df.groupby(['orbit']).count()
62/100: type(df.groupby(['orbit']).count())
62/101: (df.groupby(['orbit']).count())
62/102: (df.groupby(['orbit']).counts())
62/103: (df.groupby(['orbit']).names())
62/104: (df.groupby(['orbit']).name())
62/105: (df.groupby(['orbit']).ncount())
62/106: (df.groupby(['orbit']).nunique())
62/107: (df.groupby(['orbit']).count())
62/108: df[df.groupby(['orbit']).count()['tdc']]
62/109: df.groupby(['orbit']).count()['tdc']
62/110: df.groupby(['orbit']).count()['tdc'] < 15
62/111: df[df.groupby(['orbit']).count()['tdc'] < 15]
62/112: df.groupby(['orbit']).count() < 15
62/113: (df.groupby(['orbit']).count() < 15)['orbit']
62/114: df.groupby(['orbit']).count() < 15
62/115: (df.groupby(['orbit']).count() < 15).index
62/116: df.groupby(['orbit']).count() < 15
62/117: df.groupby(['orbit']).count() < 15 == True
62/118: (df.groupby(['orbit']).count() < 15) == True
62/119: (df.groupby(['orbit']).count() < 15) == False
62/120: (df.groupby(['orbit']).count() > 15)
62/121: (df.groupby(['orbit']).count() > 15).all()
62/122: df.groupby(['orbit'])
62/123: df.groupby(['orbit']).shape[0]
62/124: df.groupby(['orbit']).apply(shape)
62/125: df.groupby(['orbit']).apply(shape[0])
62/126: df.groupby(['orbit']).apply(len)
62/127: df.groupby(['orbit'])[8704]
62/128: df.groupby(['orbit']).loc(8704)
62/129: df.groupby(['orbit']).iloc(8704)
62/130: df.groupby(['orbit'])['orbit']
62/131: df.groupby(['orbit'])['orbit']
62/132: df.groupby(['orbit'])['orbit'].head()
62/133: df.groupby(['orbit'])['tdc'].count()
62/134: type(df.groupby(['orbit'])['tdc'].count())
62/135: df.groupby(['orbit'])['tdc'].count()
62/136: df.groupby(['orbit'])['tdc'].count() < 15
62/137: (df.groupby(['orbit'])['tdc'].count() < 15).index
62/138:
def cut(group): 
    if group.shape[0] < 15 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/1:
%%time 

def cut(group): 
    if group.shape[0] < 15 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/2:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
65/3: client
65/4: client = Client()
65/5: client
65/6: Client().close()
65/7: client.close()
65/8: client = Client()
65/9: client
65/10:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
65/11:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
65/12:
%%time
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
65/13:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
65/14: fin = np.concatenate(fin)
65/15: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
65/16:
%%time 

def cut(group): 
    if group.shape[0] < 15 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/17:
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/18:
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut_parall)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/19: d1
65/20: d1.head()
65/21: d.groupby(['orbit'])
65/22: d.groupby(['orbit']).compute()
65/23: d.groupby(['orbit']).head()
65/24: d.groupby(['orbit']).count()
65/25: d.groupby(['orbit']).count().compute()
65/26:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
65/27: fin = np.concatenate(fin)
65/28: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
65/29:
%%time 

def cut(group): 
    if group.shape[0] < 15 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/30: df1
65/31: len(df1)
65/32: len(df)
65/33: len(df1)/len(df)
65/34: df1
65/35: df1.groupby(['orbit']).count()
65/36:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/37: len(df1)/len(df)
65/38: df1.groupby(['orbit']).count()
65/39:
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut_parall)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/40: d1.head()
65/41:
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/42: d1.head()
65/43: d1.compute()
65/44: d1.map_partitions(len).compute()
65/45: d.map_partitions(len).compute()
65/46: d.map_partitions(len).sum().compute()
65/47: type(d.map_partitions(len))
65/48: d.map_partitions(len).loc(1)
65/49: len(d).compute()
65/50: len(d)
65/51: len(d1)/len(d)
65/52:
lunghezzad1 = len(d1)
lunghezzad = len(d)
65/53: lunghezzad
65/54: lunghezzad1/lunghezzad
65/55:
%%time
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/56:
%%time
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
d1
65/57:
%%time
def cut_parall(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
d1=d.groupby(['orbit'],group_keys=False)\
                 .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
d1.compute()
65/58: d1.persist()
65/59: df1.set_index('orbit', sorted = True)
65/60: df1.set_index('orbit')
65/61: df1
65/62: df1 = df1.set_index('orbit')
65/63: df1.groupby(['orbit'])
65/64: df1.groupby(['orbit']).head()
65/65: df1[df1.head == 2]
65/66: df1[df1['head'] == 2]
65/67: len(df1[df1['head'] == 2])/len(df1)
65/68: df1[df1['head'] == 2 and df1['fpga'] == 1 and df1['chan'] == 128]
65/69: df2 = df1[df1['head'] == 2]
65/70: df2[df1['fpga'] == 1 and df1['chan'] == 128]
65/71: df2[(df1['fpga'] == 1) and (df1['chan'] == 128)]
65/72: df2[(df1['fpga'] == 1)][(df1['chan'] == 128)]
65/73: df2[(df1['fpga'] == 1)]
65/74: df2[df2['fpga'] == 1]
65/75: df2[(df2['fpga'] == 1) and (df2['chan'] == 128)]
65/76:
df3 = df2[(df2['fpga'] == 1)]
df3[df3['chan'] == 128]
65/77:
df3 = df2[(df2['fpga'] == 1)]
len(df3[df3['chan'] == 128])/len(df)
65/78:
df3 = df2[(df2['fpga'] == 1)]
df3[df3['chan'] == 128]
65/79: df4.groupby(['orbit']).count()
65/80:
df3 = df2[(df2['fpga'] == 1)]
df4 = df3[df3['chan'] == 128]
65/81: df4.groupby(['orbit']).count()
65/82:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
65/83: len(df1)/len(df)
65/84: len(df1[df1['head'] == 2])/len(df1)
65/85: df2 = df1[df1['head'] == 2]
65/86:
df3 = df2[(df2['fpga'] == 1)]
df4 = df3[df3['chan'] == 128]
65/87: df4.groupby(['orbit']).count()
65/88: df4
65/89: d1 = client.persist()
65/90: d1 = client.persist(d1)
65/91: d1
65/92: d.groupby(['orbit']).count()
65/93: d.groupby(['orbit']).count().compute()
65/94: d.groupby(['orbit']).counts().compute()
65/95: d.groupby(['orbit'])['orbit'].counts().compute()
65/96: d.groupby(['orbit'])['orbit'].count().compute()
65/97:
%%time
d.groupby(['orbit'])['orbit'].count().compute()
65/98:
%%time
(d.groupby(['orbit'])['orbit'].count() > 3)
65/99:
%%time
(d.groupby(['orbit'])['orbit'].count() > 3).compute()
65/100:
%%time
(d.groupby(['orbit'])['orbit'].count() > 3).index().compute()
65/101:
%%time
(d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
65/102:
%%time
d.isin(d.groupby(['orbit'])['orbit'].count() > 3).index
65/103:
%%time
d.isin((d.groupby(['orbit'])['orbit'].count() > 3).index.compute())
65/104:
%%time
d.isin((d.groupby(['orbit'])['orbit'].count() > 3).index.compute()).compute()
65/105: (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
65/106:
%%time
d[d.isin((d.groupby(['orbit'])['orbit'].count() > 3).index.compute())].compute()
65/107:
%%time
d[d['orbit'].isin((d.groupby(['orbit'])['orbit'].count() > 3).index.compute())].compute()
65/108:
%%time
d[d['orbit'].isin((d.groupby(['orbit'])['orbit'].count() > 3 and d.groupby(['orbit'])['orbit'].count() < 15).index.compute())].compute()
65/109: d.orbit
65/110: d.orbit.compute()
65/111: d.orbit.value_counts()
65/112: d.orbit.value_counts().compute()
65/113:
cnt = d.orbit.value_counts().compute()
cnt[cnt > 3 and cnt < 15]
65/114:
cnt = d.orbit.value_counts().compute()
cnt[cnt > 3 & cnt < 15]
65/115:
cnt = d.orbit.value_counts().compute()
# cnt[cnt > 3 & cnt < 15]
65/116:
cnt = d.orbit.value_counts().index.compute()
# cnt[cnt > 3 & cnt < 15]
65/117:
cnt = d.orbit.value_counts().index.compute()
# cnt[cnt > 3 & cnt < 15]b
cnt
65/118:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
65/119:
%%time
d[d['orbit'].isin(cnt).compute()
65/120:
%%time
d[d['orbit'].isin(cnt)].compute()
65/121: cnt
65/122:
%%time
d[d['orbit'].isin(cnt.index)].compute()
65/123:
%%time
d = d[d['orbit'].isin(cnt.index)].compute()

# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
65/124: client.close()
66/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
66/2: client = Client()
66/3: client
66/4:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
66/5:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
66/6:
%%time
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
66/7:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
66/8:
%%time
d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
66/9:
lunghezzad1 = len(d1)
lunghezzad = len(d)
66/10:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
66/11: b
66/12: b.compute()
66/13: b.take(1)
66/14: b[:,5]
66/15: b[5]
66/16: b.take(1)
66/17: b.filter(lambda x: x[5] == 2)
66/18: b.filter(lambda x: x[5] == 2).compute()
66/19: b.map(lambda x: x[5] == 2).compute()
66/20: b[b.map(lambda x: x[5] == 2).compute()]
66/21: b.map(lambda x: x[5] == 2).compute()
66/22:
def head(x):
    return x[5] == 2
b.filter(head)
66/23:
def head(x):
    return x[5] == 2
b.filter(head).compute()
66/24:
def head(x):
    return x[5] == 2
b.map(head).compute()
66/25:
def head(x):
    if x[5] == 2: return x
b.map(head).compute()
66/26:
def head(x):
    if x[5] == 2: return x

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
b.remove(head_remove).compute()
66/27:
def head(x):
    if x[5] == 2: return x

def head_remove(x):
    return x[5] != 2

b.map(head).drop(None)
# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
66/28:
def head(x):
    if x[5] == 2: return x

def head_remove(x):
    return x[5] != 2

b.map(head).remove(None)
# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
66/29:
def head(x):
    if x[5] == 2: return x

def head_remove(x):
    return x[5] != 2

b.map(head).remove(None).compute()
# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
66/30:
def head(x):
    if x[5] == 2: return x

def head_remove(x):
    return x[5] != 2

b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2
# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
66/31: client.close()
70/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
70/2: client = Client()
70/3: client
70/4:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
70/5:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
70/6:
%%time
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
# prima di convertire a dask dataframe provo a filtrare eliminando quelle righe che hanno head != 2 

# d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
70/7:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b.filter(head_filter).take(5)
70/8:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b.filter(head_filter).compute()
70/9:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b.filter(head_filter).count().compute()
70/10:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
70/11:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b1 = b.filter(head_filter)
70/12: d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
70/13:
# %%time
# def cut_parall(group): 
#     if group.shape[0] < 15 and group.shape[0] > 3 : return group 
#     else : return None 
    
# d1=d.groupby(['orbit'],group_keys=False)\
#                  .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
#                  .dropna()\
#                  .reset_index()\
#                  .drop(['index'],axis=1)
# d1.compute()
70/14:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
70/15:
%%time
d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
70/16:
lunghezzad1 = len(d1)
lunghezzad = len(d)
70/17:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
70/18: # (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
70/19: d1 = client.persist(d1)
70/20: client.close()
72/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
72/2: client = Client()
72/3: client
72/4:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
72/5:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
72/6:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
72/7:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b1 = b.filter(head_filter)
72/8: d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
72/9:
# %%time
# def cut_parall(group): 
#     if group.shape[0] < 15 and group.shape[0] > 3 : return group 
#     else : return None 
    
# d1=d.groupby(['orbit'],group_keys=False)\
#                  .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
#                  .dropna()\
#                  .reset_index()\
#                  .drop(['index'],axis=1)
# d1.compute()
72/10: # (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
72/11:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
72/12:
%%time
d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
72/13:
lunghezzad1 = len(d1)
lunghezzad = len(d)
72/14:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
72/15:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
72/16: d1 = client.persist(d1)
72/17: d1 = d1.persist()
72/18: d1 = d[d['orbit'].isin(cnt.index)]
72/19: d1 = d1.persist()
72/20:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1.repartition(npartitions=12) # tanti quanti sono i threads
72/21:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
72/22: d1
72/23: d1.orbit
72/24: d1.orbit['fpga' == 1]
72/25: d1.groupby(['orbit'])['fpga' == 1]
72/26: d1.groupby(['orbit'])['fpga']
72/27: d1.groupby(['orbit'])['fpga'].head()
72/28: d1.groupby(['orbit'])['fpga'].compute()
72/29: d1.groupby(['orbit'])['fpga'] == 1
72/30:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
72/31: fin = np.concatenate(fin)
72/32: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
72/33:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
72/34: len(df1)/len(df)
72/35:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1
72/36:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.orbit
72/37:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit'])
72/38:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit'])['fpga']
72/39:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit'])['fpga'].show()
72/40:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit'])['fpga'].count()
72/41:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.fpga
72/42:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

(df1.fpga) == 1
72/43:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

((df1.fpga) == 1).index()
72/44:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

((df1.fpga) == 1).index
72/45:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

((df1.fpga) == 1)
72/46:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1[df1['fpga'] == 1]
72/47:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit', 'fpga'])
72/48:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit', 'fpga']).count()
72/49:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit', 'fpga'])['fpga' == 1]
72/50:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit', 'fpga'])[]
72/51:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit', 'fpga'])
72/52:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby(['orbit', 'fpga']).count()
72/53:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1['orbit','fpga']
72/54:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1['orbit']
72/55:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1['orbit', 'fpga']
72/56:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1[('orbit', 'fpga')]
72/57:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.loc(['fpga'])
72/58:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.iloc(['fpga'])
72/59:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.iloc('fpga')
72/60:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.iloc['fpga']
72/61:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.loc['fpga']
72/62:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.loc[orbit]
72/63:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.loc(orbit)
72/64:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.iloc(orbit)
72/65:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.iloc('orbit')
72/66:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.loc('orbit')
72/67:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1
72/68:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1['orbit']
72/69:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1['orbit']['fpga']
72/70:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1.groupby('orbit')
72/71:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1[df1['fpga'] == 1].orbit
72/72:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

df1[df1['fpga'] == 1].orbit.index
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe
72/73:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1 = df1[df1['fpga'] == 1].orbit.values
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe
72/74: fpga1
72/75:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1 = df1[df1['fpga'] == 1].orbit.values
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1
fpga1
72/76: df1[df1['orbit'].isin(fpga1)]
72/77: df1[df1['orbit'].isin(fpga1)].groupby('orbit').count()
72/78: df2 = df1[df1['orbit'].isin(fpga1)]
72/79: df2
72/80:
# adesso il filtro analogo al precedente ma con gli scintillatori 

df2[df2['tdc'] == 128]
72/81: df2['tdc'] == 128
72/82: df2
72/83:
# adesso il filtro analogo al precedente ma con gli scintillatori 

df2[df2['chan'] == 128]
72/84:
# adesso il filtro analogo al precedente ma con gli scintillatori 

df2[df2['chan'] == 128].orbit.values
72/85:
# adesso il filtro analogo al precedente ma con gli scintillatori 

chan2 = df2[df2['chan'] == 128].orbit.values
df3 = df2[df2['orbit'].isin(chan2)]
72/86: df3
72/87: len(df3)/len(df)
72/88:
sprintf('Il preprocessing di fpga, chan e head conduce ad un dataframe il %.3f %% pi piccolo di quello iniziale ',1-len(df3)/len(df))
# rimane dunque lo 0.3 % del dataframe iniziale
72/89:
printf('Il preprocessing di fpga, chan e head conduce ad un dataframe il %.3f %% pi piccolo di quello iniziale ',1-len(df3)/len(df))
# rimane dunque lo 0.3 % del dataframe iniziale
72/90:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il %.3f %% pi piccolo di quello iniziale ',1-len(df3)/len(df))
# rimane dunque lo 0.3 % del dataframe iniziale
72/91:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale ',format(1-len(df3)/len(df)))
# rimane dunque lo 0.3 % del dataframe iniziale
72/92:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format(1-len(df3)/len(df)))
# rimane dunque lo 0.3 % del dataframe iniziale
72/93:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe lo {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale
72/94:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale
72/95:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('il numero unico di eventi  uguale a ', df3.groupby('orbit').nunuque())
72/96: df3.groupby('orbit').nunique()
72/97: df3.groupby('orbit').nunique().count()
72/98: df3.groupby('orbit').unique()
72/99: df3.groupby('orbit').nunique()
72/100: len(df3.groupby('orbit').nunique())
72/101:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('il numero unico di eventi  uguale a ', len(df3.groupby('orbit').nunique())
72/102:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('il numero unico di eventi  uguale a ', len(df3.groupby('orbit').nunique()))
72/103:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
72/104: fpga = d1[d1['fpga'] == 1].orbit.values.compute()
72/105: fpga
72/106: d2 = d1[d1['orbit'].isin(fpga)]
72/107: d2
72/108:
chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan
72/109:
%%time
d3 = d2[d2['orbit'].isin(chan)].compute()
72/110: d3
72/111: df3
72/112: d3 = d2[d2['orbit'].isin(chan)]
72/113: df3.reset_index
72/114: df3.reset_index()
72/115: df3.reset_index().drop('index')
72/116: df3.reset_index().drop('index', axis=1)
72/117: df3 = df3.reset_index().drop('index', axis=1)
72/118: d3
72/119:
# adesso aggiungo le colonne del tempo e delle camere: 
# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tcd']/30))
72/120:
# adesso aggiungo le colonne del tempo e delle camere: 
# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
72/121: df3
72/122:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

dict = {0: 'fpga' == 0 and 'chan' in range(64),
        1: 'fpga' == 0 and 'chan' in range(64,128),
        2: 'fpga' == 0 and 'chan' in range(0,64),
        3: 'fpga' == 0 and 'chan' in range(64,128)}
df3[dict[0]]
72/123:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

dict = {0: 'fpga' == 0 and 'chan' in range(64),
        1: 'fpga' == 0 and 'chan' in range(64,128),
        2: 'fpga' == 0 and 'chan' in range(0,64),
        3: 'fpga' == 0 and 'chan' in range(64,128)}
dict[0]
72/124:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

dict = {0: str('fpga' == 0 and 'chan' in range(64)),
        1: 'fpga' == 0 and 'chan' in range(64,128),
        2: 'fpga' == 0 and 'chan' in range(0,64),
        3: 'fpga' == 0 and 'chan' in range(64,128)}
dict[0]
72/125:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga = df3['fpga'] == 1 
channel = (df3['tdc'] > 64) & (df3['tdc'] <= 128)
72/126: fpga
72/127:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)
72/128: fpga_cond
72/129:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond and ~channel_cond, ~fpga and channel_cond, fpga_cond and ~channel_cond, fpga and channel_cond]
cond
72/130:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
cond
72/131:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
cond[0]
72/132:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
72/133:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(detectors)) : df3.loc[cond[i],'chamb'] = i+1
72/134:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond)) : df3.loc[cond[i],'chamb'] = i+1
72/135: df3
72/136:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond)) : df3.loc[cond[i],'chamb'] = int(i+1)
72/137: df3
72/138:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond)) : df3.loc[cond[i],'chamb'] = np.uint32(i+1)
72/139: df3
72/140:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_par & channel_cond_par]
for i in range(len(cond_par)) : d3.loc[cond_par[i],'chamb'] = np.uint32(i+1)
72/141:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]
for i in range(len(cond_par)) : d3.loc[cond_par[i],'chamb'] = np.uint32(i+1)
72/142:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]
for i in range(len(cond_par)) : d3.loc[cond_par[i],'chamb'] = i+1
72/143:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]
cond_par
# for i in range(len(cond_par)) : d3.loc[cond_par[i],'chamb'] = i+1
72/144:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]
cond_par[0]
# for i in range(len(cond_par)) : d3.loc[cond_par[i],'chamb'] = i+1
72/145:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]
cond_par[1]
# for i in range(len(cond_par)) : d3.loc[cond_par[i],'chamb'] = i+1
72/146:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3.loc[cond_par[i],'chamb'] = i+1
72/147: df3['chamb'][0]
72/148: df3['chamb'][1]
72/149:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3['chamb'][i] = i+1
72/150:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3['chamb'][cond_par[i]] = i+1
72/151:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond_par)) : 
    d3['chamb'][cond_par[i]] = i+1
72/152:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond_par)) : 
    df3['chamb'][cond_par[i]] = i+1
73/1:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
73/2: fin = np.concatenate(fin)
73/3: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
73/4:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
73/5: client
73/6: client = Client()
73/7: client.close()
73/8: client.close()
73/9:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
73/10: fin = np.concatenate(fin)
73/11: client
73/12:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
73/13:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
73/14:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
73/15:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
73/16:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b1 = b.filter(head_filter)
73/17: d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
73/18:
# %%time
# def cut_parall(group): 
#     if group.shape[0] < 15 and group.shape[0] > 3 : return group 
#     else : return None 
    
# d1=d.groupby(['orbit'],group_keys=False)\
#                  .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
#                  .dropna()\
#                  .reset_index()\
#                  .drop(['index'],axis=1)
# d1.compute()
73/19: # (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
73/20:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
73/21:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
73/22: d1 = d[d['orbit'].isin(cnt.index)]
73/23:
lunghezzad1 = len(d1)
lunghezzad = len(d)
73/24:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
73/25:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
73/26:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
73/27:
fpga = d1[d1['fpga'] == 1].orbit.values.compute()
# anche questa volta memorizzo la serie contenente le orbite che devo mantenere nel dataframe. Visto che per ogni evento le hits hanno la 
# stessa orbit allora in questo modo ho selezionato tutte le hit che soddisfano il filtro
73/28: d2 = d1[d1['orbit'].isin(fpga)]
73/29:
chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan memorizzo l'array che mi serve per selezionare gli eventi buoni
73/30:
d3 = d2[d2['orbit'].isin(chan)]
# facendo compute quindi calcolando esplicitamente il d2 e il d3 perch ancora d2  una chiamata non un'esecuzione impieghiamo soltanto un 
# centinaio di millisecondi; il tutto semplicemente memorizzando il dataframe d1 con .persist e aumentando il numero di partizioni a 12. 
# Adesso che per il numero di righe del dataframe  diminuito sarebbe necessario diminuire altrettanto il numero di partizioni
73/31: client = Client()
73/32: client
73/33:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
73/34:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
73/35:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
73/36:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
73/37:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b1 = b.filter(head_filter)
73/38: d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
73/39:
# %%time
# def cut_parall(group): 
#     if group.shape[0] < 15 and group.shape[0] > 3 : return group 
#     else : return None 
    
# d1=d.groupby(['orbit'],group_keys=False)\
#                  .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
#                  .dropna()\
#                  .reset_index()\
#                  .drop(['index'],axis=1)
# d1.compute()
73/40: # (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
73/41:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
73/42:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
73/43: d1 = d[d['orbit'].isin(cnt.index)]
73/44:
lunghezzad1 = len(d1)
lunghezzad = len(d)
73/45:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
73/46:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
73/47:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
73/48:
fpga = d1[d1['fpga'] == 1].orbit.values.compute()
# anche questa volta memorizzo la serie contenente le orbite che devo mantenere nel dataframe. Visto che per ogni evento le hits hanno la 
# stessa orbit allora in questo modo ho selezionato tutte le hit che soddisfano il filtro
73/49: d2 = d1[d1['orbit'].isin(fpga)]
73/50:
chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan memorizzo l'array che mi serve per selezionare gli eventi buoni
73/51:
d3 = d2[d2['orbit'].isin(chan)]
# facendo compute quindi calcolando esplicitamente il d2 e il d3 perch ancora d2  una chiamata non un'esecuzione impieghiamo soltanto un 
# centinaio di millisecondi; il tutto semplicemente memorizzando il dataframe d1 con .persist e aumentando il numero di partizioni a 12. 
# Adesso che per il numero di righe del dataframe  diminuito sarebbe necessario diminuire altrettanto il numero di partizioni
73/52: d3
73/53:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
73/54: fin = np.concatenate(fin)
73/55: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
73/56:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
73/57: len(df1)/len(df)
73/58:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1 = df1[df1['fpga'] == 1].orbit.values
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df2 = df1[df1['orbit'].isin(fpga1)]
73/59:
# adesso il filtro analogo al precedente ma con gli scintillatori 128

chan2 = df2[df2['chan'] == 128].orbit.values
df3 = df2[df2['orbit'].isin(chan2)]
73/60:
df3 = df3.reset_index().drop('index', axis=1)
# per resettare gli indici e droppare quelli che otteniamo introducendo una nuova colonna "index". Serve soltanto per il caso sequenziale 
# poich nel dask dataframe non ci importa come sono ordinate le hits tramite gli indici
73/61:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
73/62:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond_par)) : 
    df3['chamb'][cond_par[i]] = i+1
73/63:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond_par)) : 
    df3.loc[cond_par[i], 'chamb'] = i+1
73/64:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond)) : 
    df3.loc[cond[i], 'chamb'] = i+1
73/65: df3
73/66:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga & channel_cond]
for i in range(len(cond)) : df3.loc[cond[i],'chamb'] = i+1
73/67:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga & channel_cond, fpga_cond & ~channel_cond, fpga_cond & channel_cond]
for i in range(len(cond)) : df3.loc[cond[i],'chamb'] = i+1
73/68:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga_cond & channel_cond, fpga_cond & ~channel_cond, fpga_cond & channel_cond]
for i in range(len(cond)) : df3.loc[cond[i],'chamb'] = i+1
73/69: df3
73/70:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga_cond & channel_cond, fpga_cond & ~channel_cond, fpga_cond & channel_cond]
for i in range(len(cond)) : df3['chamb'][cond[i]] = i+1
73/71: df3
73/72:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3['chamb'][cond_par[i]] = i+1
73/73:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3.loc[cond_par[i], 'chamb'] = i+1
73/74: d3['time'] = 25 * ((d3['orbit'] * 3564) + d3['bx'] + (d3['tdc']/30))
73/75: d3
73/76:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3['chamb'][cond_par[i]] = i+1
73/77:
fpga_cond_par = (d3['fpga'] == 1).compute() 
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128)).compute()

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3['chamber'][cond_par[i]] = i+1
76/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
76/2: client = Client()
76/3: # client = Client('78.13.47.54:8001') #cluster di mattia
76/4: client
76/5:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
76/6:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
76/7:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
76/8:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
76/9:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b1 = b.filter(head_filter)
76/10: d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
76/11:
# %%time
# def cut_parall(group): 
#     if group.shape[0] < 15 and group.shape[0] > 3 : return group 
#     else : return None 
    
# d1=d.groupby(['orbit'],group_keys=False)\
#                  .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
#                  .dropna()\
#                  .reset_index()\
#                  .drop(['index'],axis=1)
# d1.compute()
76/12: # (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
76/13:
cnt = d.orbit.value_counts().compute()
cnt = cnt[cnt > 3]
cnt = cnt[cnt < 15]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
76/14:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
76/15: d1 = d[d['orbit'].isin(cnt.index)]
76/16:
lunghezzad1 = len(d1)
lunghezzad = len(d)
76/17:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
76/18:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
76/19:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
76/20: d1
76/21:
fpga = d1[d1['fpga'] == 1].orbit.values.compute()
# anche questa volta memorizzo la serie contenente le orbite che devo mantenere nel dataframe. Visto che per ogni evento le hits hanno la 
# stessa orbit allora in questo modo ho selezionato tutte le hit che soddisfano il filtro
76/22: d2 = d1[d1['orbit'].isin(fpga)]
76/23:
chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan memorizzo l'array che mi serve per selezionare gli eventi buoni
76/24:
d3 = d2[d2['orbit'].isin(chan)]
# facendo compute quindi calcolando esplicitamente il d2 e il d3 perch ancora d2  una chiamata non un'esecuzione impieghiamo soltanto un 
# centinaio di millisecondi; il tutto semplicemente memorizzando il dataframe d1 con .persist e aumentando il numero di partizioni a 12. 
# Adesso che per il numero di righe del dataframe  diminuito sarebbe necessario diminuire altrettanto il numero di partizioni
76/25: d3['time'] = 25 * ((d3['orbit'] * 3564) + d3['bx'] + (d3['tdc']/30))
76/26:
fpga_cond_par = (d3['fpga'] == 1)
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128))

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

# for i in range(len(cond_par)) : 
#     d3['chamber'][cond_par[i]] = i+1
76/27: fpga_cond_par
76/28:
fpga_cond_par = (d3['fpga'] == 1)
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128))

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3['chamber'][cond_par[i]] = i+1
76/29:
fpga_cond_par = (d3['fpga'] == 1)
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128))

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3.loc[cond_par[i],'chamber'] = i+1
76/30:
fpga_cond_par = (d3['fpga'] == 1)
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128))

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    d3.loc[cond_par[i],'chamber'].compute() = i+1
76/31:
fpga_cond_par = (d3['fpga'] == 1)
channel_cond_par = ((d3['tdc'] > 64) & (d3['tdc'] <= 128))

cond_par = [~fpga_cond_par & ~channel_cond_par, ~fpga_cond_par & channel_cond_par, fpga_cond_par & ~channel_cond_par, fpga_cond_par & channel_cond_par]

for i in range(len(cond_par)) : 
    (d3.loc[cond_par[i],'chamber']).compute() = i+1
76/32: d3.loc[cond_par[i],'chamber'] = 1
76/33: d3.loc[cond_par[0],'chamber'] = 1
76/34: d3['chamber'][fpga_cond_par] = 1
76/35: d3['chamber'] = 1
76/36: d3
76/37: d3['chamber'][0] = 1
76/38:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
76/39: fin = np.concatenate(fin)
76/40: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
76/41: df
76/42: df['head']
76/43: (df['head']==2).index
76/44: (df['head']==2).index()
76/45: (df['head']==2).index
76/46: (df['head']==2)
76/47: df[(df['head']==2)]
76/48: df = df[(df['head']==2)]
76/49:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
76/50: len(df1)/len(df)
76/51:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1 = df1[df1['fpga'] == 1].orbit.values
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df2 = df1[df1['orbit'].isin(fpga1)]
76/52:
# adesso il filtro analogo al precedente ma con gli scintillatori 128

chan2 = df2[df2['chan'] == 128].orbit.values
df3 = df2[df2['orbit'].isin(chan2)]
76/53:
df3 = df3.reset_index().drop('index', axis=1)
# per resettare gli indici e droppare quelli che otteniamo introducendo una nuova colonna "index". Serve soltanto per il caso sequenziale 
# poich nel dask dataframe non ci importa come sono ordinate le hits tramite gli indici
76/54:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
76/55:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga_cond & channel_cond, fpga_cond & ~channel_cond, fpga_cond & channel_cond]
for i in range(len(cond)) : df3['chamb'][cond[i]] = i+1
76/56: df3
76/57:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))

# per le chamber invece devo considerare il dizionario che ci viene fornito dalle istruzioni
#  Chamber 0  (FPGA = 0) AND (CHANNEL in [0-63])
#  Chamber 1  (FPGA = 0) AND (CHANNEL in [64-127])
#  Chamber 2  (FPGA = 1) AND (CHANNEL in [0-63])
#  Chamber 3  (FPGA = 1) AND (CHANNEL in [64-127])

fpga_cond = df3['fpga'] == 1 
channel_cond = (df3['tdc'] > 64) & (df3['tdc'] <= 128)

cond = [~fpga_cond & ~channel_cond, ~fpga_cond & channel_cond, fpga_cond & ~channel_cond, fpga_cond & channel_cond]
for i in range(len(cond)) : df3['chamb'][cond[i]] = i+1
76/58: client.close()
79/1:
# attenzione ai due filtri di fpga e chan che si devono fare contemporaneamente perch mi serve tenere quegli scintillatori che hanno fpga=1
# come parametro oltre che il canale 128 tipico dello scintillatore
79/2:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
79/3: client = Client()
79/4: # client = Client('78.13.47.54:8001') #cluster di mattia
79/5: client
79/6:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
79/7:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
79/8:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
79/9:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
79/10:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag 

b1 = b.filter(head_filter)
79/11: d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
79/12:
# %%time
# def cut_parall(group): 
#     if group.shape[0] < 15 and group.shape[0] > 3 : return group 
#     else : return None 
    
# d1=d.groupby(['orbit'],group_keys=False)\
#                  .apply(cut_parall, meta =  [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])\
#                  .dropna()\
#                  .reset_index()\
#                  .drop(['index'],axis=1)
# d1.compute()
79/13: # (d.groupby(['orbit'])['orbit'].count() > 3).index.compute()
79/14:
cnt = d.orbit.value_counts().compute()
cnt = cnt[(cnt > 3) & (cnt < 15)]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
79/15:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
79/16: d1 = d[d['orbit'].isin(cnt.index)]
79/17:
lunghezzad1 = len(d1)
lunghezzad = len(d)
79/18:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
79/19:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
79/20:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
79/21: d1
79/22:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
79/23: fin = np.concatenate(fin)
79/24: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
79/25: df = df[(df['head']==2)]
79/26:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
79/27: len(df1)/len(df)
79/28: (df1['fpga'] == 1) & (df1['chan'] == 128)
79/29: ((df1['fpga'] == 1) & (df1['chan'] == 128)).any(True)
79/30: ((df1['fpga'] == 1) & (df1['chan'] == 128)) == true
79/31: ((df1['fpga'] == 1) & (df1['chan'] == 128)) == True
79/32: np.any((df1['fpga'] == 1) & (df1['chan'] == 128))
79/33:
a = np.array((True,False))
np.any(a)
79/34: ((df1['fpga'] == 1) & (df1['chan'] == 128)).values
79/35: (((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True
79/36: (((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True]
79/37:
(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True].head()
# intanto significa che ci sono scintillatori che verificano questa condizione
79/38:
(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True][:5]
# intanto significa che ci sono scintillatori che verificano questa condizione
79/39: (((df1['fpga'] == 1) & (df1['chan'] == 128))
79/40: ((df1['fpga'] == 1) & (df1['chan'] == 128))
79/41: ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True]
79/42: ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
79/43:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
79/44: df1.orbit[indici]
79/45: df1.orbit[indici].values
79/46: orbite_buone = df1.orbit[indici].values
79/47:
# # adesso il filtro analogo al precedente ma con gli scintillatori 128

# chan2 = df2[df2['chan'] == 128].orbit.values
# df3 = df2[df2['orbit'].isin(chan2)]
# gi fatto tutto insieme prima 


# vorrei per confrontare se ottengo lo stesso risultato applicando i due filtri consecutivamente oppure uno dopo l'altro
fpga1 = df1[df1['fpga'] == 1].orbit.values
dfconfronto = df1[df1['orbit'].isin(fpga1)]

chan2 = df2[df2['chan'] == 128].orbit.values
dfconfronto1 = dfconfronto[dfconfronto['orbit'].isin(chan2)]
79/48:
# # adesso il filtro analogo al precedente ma con gli scintillatori 128

# chan2 = df2[df2['chan'] == 128].orbit.values
# df3 = df2[df2['orbit'].isin(chan2)]
# gi fatto tutto insieme prima 


# vorrei per confrontare se ottengo lo stesso risultato applicando i due filtri consecutivamente oppure uno dopo l'altro
fpga1 = df1[df1['fpga'] == 1].orbit.values
dfconfronto = df1[df1['orbit'].isin(fpga1)]

chan2 = dfconfronto[dfconfronto['chan'] == 128].orbit.values
dfconfronto1 = dfconfronto[dfconfronto['orbit'].isin(chan2)]
79/49:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df2 = df1[df1['orbit'].isin(fpga1_chan1)]
79/50: df2 == dfconfronto1
79/51: if (df2 == dfconfronto1): print("i due filtri possono essere eseguiti anche sequenzialmente uno dopo l'altro anzich pensarli in contemporanea")
79/52: if (df2 == dfconfronto1).any(): print("i due filtri possono essere eseguiti anche sequenzialmente uno dopo l'altro anzich pensarli in contemporanea")
79/53: if np.any(df2 == dfconfronto1): print("i due filtri possono essere eseguiti anche sequenzialmente uno dopo l'altro anzich pensarli in contemporanea")
79/54: if np.any(df2 == dfconfronto1): print("I due filtri possono essere eseguiti anche sequenzialmente uno dopo l'altro anzich pensarli in contemporanea")
79/55:
df3 = df3.reset_index().drop('index', axis=1)
# per resettare gli indici e droppare quelli che otteniamo introducendo una nuova colonna "index". Serve soltanto per il caso sequenziale 
# poich nel dask dataframe non ci importa come sono ordinate le hits tramite gli indici
79/56:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
79/57:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
79/58: if np.any(df3 == dfconfronto1): print("I due filtri possono essere eseguiti anche sequenzialmente uno dopo l'altro anzich pensarli in contemporanea")
79/59:
df3 = df3.reset_index().drop('index', axis=1)
# per resettare gli indici e droppare quelli che otteniamo introducendo una nuova colonna "index". Serve soltanto per il caso sequenziale 
# poich nel dask dataframe non ci importa come sono ordinate le hits tramite gli indici
79/60:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
79/61:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df2['bx'] + 25*df2['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
79/62: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
79/63:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
79/64:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()
#s.loc[s.index == df2.orbit.unique()] # non mi serve

df3
79/65:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
# df3['t0'] = s.loc[df3.orbit].t.to_numpy()
# #s.loc[s.index == df2.orbit.unique()] # non mi serve

# df3
s
79/66:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()

df3
79/67:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
79/68: fin = np.concatenate(fin)
79/69: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
79/70: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
79/71: df = df[(df['head']==2)]
79/72:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
79/73: len(df1)/len(df)
79/74: len(df1)/len(df)
79/75:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
79/76: orbite_buone = df1.orbit[indici].values
79/77:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
79/78: df3
79/79:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df2['bx'] + 25*df2['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
79/80:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
79/81:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
79/82: df3
79/83: df3
79/84: df3._is_copy
79/85:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
79/86:
df3._is_copy = None 
# trucco ihih
79/87:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
79/88:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
79/89:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()

df3
79/90:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
79/91:
(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True][:5]
# intanto significa che ci sono scintillatori che verificano questa condizione e prendo ad esempio i primi cinque
79/92:
(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True][:]
# intanto significa che ci sono scintillatori che verificano questa condizione e prendo ad esempio i primi cinque
79/93:
(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True][:5]
# intanto significa che ci sono scintillatori che verificano questa condizione e prendo ad esempio i primi cinque
79/94: indici
79/95: len(indici)
79/96: lunghezzad1
79/97: client.close()
81/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
81/2: client = Client()
81/3: # client = Client('78.13.47.54:8001') #cluster di mattia
81/4: client
81/5:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
81/6:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
81/7:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
81/8:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
81/9:
def head(x):
    if x[5] == 2: return x
    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
81/10:
%%time
b1 = b.filter(head_filter)
d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d.compute()
81/11:
%%time
d1 = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d1 = d1[d1['head'] == 2]
d1.compute()
81/12:
def head(x):
    if x[5] == 2: return x
    
@njit    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
81/13:
%%timeit
b1 = b.filter(head_filter)
d = b1.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d.compute()
81/14:
%%timeit
d1 = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d1 = d1[d1['head'] == 2]
d1.compute()
81/15: client.close()
81/16: client = Client()
81/17: # client = Client('78.13.47.54:8001') #cluster di mattia
81/18: client
81/19:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
81/20:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
81/21:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
81/22:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
81/23:
def head(x):
    if x[5] == 2: return x
    
@njit    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
81/24:
# %%timeit
d1 = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d1 = d1[d1['head'] == 2]
# d1.compute()
81/25: client.close()
82/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
82/2: client = Client()
82/3: # client = Client('78.13.47.54:8001') #cluster di mattia
82/4: client
82/5:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
82/6:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
82/7:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
82/8:
# %%timeit
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d = d[d['head'] == 2]
# d1.compute()
82/9:
cnt = d.orbit.value_counts().compute()
cnt = cnt[(cnt > 3) & (cnt < 15)]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
82/10:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
82/11: d1 = d[d['orbit'].isin(cnt.index)]
82/12:
lunghezzad1 = len(d1)
lunghezzad = len(d)
82/13:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
82/14:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
82/15:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
82/16: d1
82/17:
fpga = d1[d1['fpga'] == 1].orbit.values.compute()
# anche questa volta memorizzo la serie contenente le orbite che devo mantenere nel dataframe. Visto che per ogni evento le hits hanno la 
# stessa orbit allora in questo modo ho selezionato tutte le hit che soddisfano il filtro
82/18: d2 = d1[d1['orbit'].isin(fpga)]
82/19:
chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan memorizzo l'array che mi serve per selezionare gli eventi buoni
82/20:
d3 = d2[d2['orbit'].isin(chan)]
# facendo compute quindi calcolando esplicitamente il d2 e il d3 perch ancora d2  una chiamata non un'esecuzione impieghiamo soltanto un 
# centinaio di millisecondi; il tutto semplicemente memorizzando il dataframe d1 con .persist e aumentando il numero di partizioni a 12. 
# Adesso che per il numero di righe del dataframe  diminuito sarebbe necessario diminuire altrettanto il numero di partizioni
82/21: d3['time'] = 25 * ((d3['orbit'] * 3564) + d3['bx'] + (d3['tdc']/30))
82/22:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
82/23: fin = np.concatenate(fin)
82/24: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
82/25: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
82/26: df = df[(df['head']==2)]
82/27:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
82/28: len(df1)/len(df)
82/29:
(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True]
# intanto significa che ci sono scintillatori che verificano questa condizione e prendo ad esempio i primi cinque
82/30:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
82/31:
len(indici)
#  diverso da len(df3)
82/32: orbite_buone = df1.orbit[indici].values
82/33:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
82/34:
print('Il preprocessing di fpga, chan e head conduce ad un dataframe il {} % pi piccolo di quello iniziale '.format((1-len(df3)/len(df))*100))
# rimane dunque lo 0.3 % del dataframe iniziale con un numero unico di eventi pari a 
print('Il numero unico di eventi  uguale a', len(df3.groupby('orbit').nunique()))
82/35:
df3._is_copy = None 
# trucco ihih perch df3  una copia di df1 e lui ne tiene conto in questo modo segnalando warning
82/36:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
82/37:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
82/38:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()

df3
82/39:
df3.loc[df3['chan'] > 127] = None 
df3.dropna(inplace = True)
df3
82/40:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True)
82/41:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers,meta=tipi).reset_index(drop=True)
82/42:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
82/43:
# assegno la camera
df3['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
df3 = df3.astype(tipi2)

df3.loc[(df3['fpga'] == 0) & (df3['chan'] < 64), 'chamber'] = 0
df2.loc[(df3['fpga'] == 0) & (df3['chan'] > 63), 'chamber'] = 1
df2.loc[(df3['fpga'] == 1) & (df3['chan'] < 64), 'chamber'] = 2
df2.loc[(df3['fpga'] == 1) & (df3['chan'] > 63), 'chamber'] = 3

df3
82/44:
# assegno la camera
df3['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
df3 = df3.astype(tipi2)

df3.loc[(df3['fpga'] == 0) & (df3['chan'] < 64), 'chamber'] = 0
df3.loc[(df3['fpga'] == 0) & (df3['chan'] > 63), 'chamber'] = 1
df3.loc[(df3['fpga'] == 1) & (df3['chan'] < 64), 'chamber'] = 2
df3.loc[(df3['fpga'] == 1) & (df3['chan'] > 63), 'chamber'] = 3

df3
82/45:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
82/46:
#add layer column
df3["layer"]=df3["chan"]%4

#add cell column
df3["cell"]=df3["chan"]%64
82/47:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
82/48: df4
82/49:
#add layer column
#contando dal basso 
df3["layer"]=df3["chan"]%4
df3['layer'].replace({0:3, 3:0}, inplace = True) # 2:2, 1:1
#add cell column
df3["cell"]=df3["chan"]%64
82/50:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
82/51: df4
82/52:
#add layer column
#contando dal basso per partendo da uno non da zero 
df3["layer"]=df3["chan"]%4
df3['layer'].replace({0:3, 3:0}, inplace = True) # 2:2, 1:1
df3['layer'] += 1
#add cell column
df3["cell"]=df3["chan"]%64
82/53:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
82/54: df4
82/55:
df4.loc[df4["chan"] > 63,"chan"] -= 64

f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
df4["column"] = ((df4['chan'] + f(df4["layer"])) / 4).astype(np.uint8)
df4
82/56:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product
from tqdm import tqdm

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
82/57:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product


# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
82/58:
df4.loc[df4["chan"] > 63,"chan"] -= 64

f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
df4["column"] = ((df4['chan'] + f(df4["layer"])) / 4).astype(np.uint8)
df4
82/59: df4.column.max()
82/60:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
df4["center"] = centers_coords[df4.layer-1,df4.column-1] 
df4
82/61:
time_offset = np.array([-1.1, 6.4, 0.5, 2.6])

df4['dt'] = df4['t'] - df4['t0'] + 95 + time_offset[df4['layer'] - 1]
df4
82/62:
# 42/2 = 21
vd = 53.8*1e-3 # v_drift = 53.8 um/ns, voglio lavorare con i millimetri
df4['xr'] = df4['center']*21 + vd*df4['dt']
df4['xl'] = df4['center']*21 - vd*df4['dt']

z_offset = np.array([219.8, 977.3, 1035.6, 1819.8])
dz2 = 13/2

df4['z'] = df4['layer']*dz2 + z_offset[df4['layer'] - 1]
df4
82/63:
@njit 
def numba_score(combin,eventZ) :
    s = np.zeros(combin.shape[0]) 
    for i, c in enumerate(combin) :
        if ((eventZ - eventZ.mean())**2).sum() == 0 : # dice che se no ci sono delle divisioni per zero anche se non riesco a riprodurle a mano... assurdo
            s[i] = 0 #np.nan
        else :
            slope = ((eventZ - eventZ.mean())*(c - c.mean())).sum() / ((eventZ - eventZ.mean())**2).sum()
            intercept = c.mean() - slope*eventZ.mean() # cfr https://en.wikipedia.org/wiki/Simple_linear_regression con x e y scambiati

            s[i] = np.linalg.norm(c - (slope*eventZ+intercept))
    return s

def solve_ambiguity(event):
    combin = np.array(list(product(*event.loc[:, ['xr','xl']].to_numpy())))
    s = numba_score(combin,event.z.to_numpy())
    event['xb'] = combin[s.argmin()]
    return event
82/64:
df5 = df4.copy()
df5 = df5.groupby(['orbit', 'chamber']).apply(solve_ambiguity)
df5
82/65:
#function to calculate coordinates of centres of cells
def centres(ev):
    
    dx=1
    dy=1/2
    gap=4
    
    x = np.zeros(ev.shape[0])
    y = np.zeros(ev.shape[0])
    
    #calculate coordinates of centers
    for i in range(ev.shape[0]): 
        
        layer = ev.loc[i,'layer']
        cell = ev.loc[i,'cell']
        chamber = ev.loc[i,'chamber']

        if layer == 0 : 
            x[i] = cell/4
            y[i] = dy/2 +chamber*gap 
        
        elif layer == 2 :
            x[i] = (cell-2)/4 + dx/2
            y[i] = dy/2 + dy + chamber*gap
        
        elif layer == 1 :
            x[i] = (cell-1)/4
            y[i] = dy/2 + 2*dy + chamber*gap
        
        elif layer == 3 : 
            x[i] = (cell-3)/4 + dx/2
            y[i] = dy/2+3*dy + chamber*gap
        
            
    return [x,y]
82/66:
#function to draw the activated cells
def boxes(ax,x,y):
    
    dx = 1
    dy = 1/2
     
    for i in range(len(x)) :
        rect = patches.Rectangle((x[i]-dx/2,y[i]-dy/2),dx,dy,linewidth=3,edgecolor='r',facecolor='none')
        ax.add_patch(rect)
82/67:
#function to draw the detector
def draw_detector(ax) :
    
    dx = 1
    dy = 1/2
    gap=4

    #adjust
    ax.set_xlim(0,16.5)
    ax.set_ylim(-0.2,15.2)

    #coordinates of layer 0
    x_1 = np.arange(1,16)-dx/2
    y_1 = 0
    
    #coordinates of layer 2
    x_2 = x_1 + dx/2
    y_2 = dy
    
    #coordinates of layer 3
    x_3 = x_1 
    y_3 = 2*dy
    
    #coordinates of layer 4
    x_4 = x_2
    y_4 = 3*dy
    
    for j in range(4):
        
        for i in range(15): 
            rect = patches.Rectangle((x_1[i],y_1),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_2[i],y_2),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_3[i],y_3),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_4[i],y_4),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            ax.axis('off')
        
        #increment y chamber
        y_1+=gap
        y_2+=gap
        y_3+=gap
        y_4+=gap
82/68: data = df5.reset_index(drop=True).compute()
82/69: df5
82/70: data = df5.copy().reset_index(drop=True)
82/71:
events=[value for value in data.groupby(["orbit"]).groups.values()]
event=[data.loc[events[i]]  for i in np.random.randint(low=0,high=len(events),size=6)] #select 6 random events to show
event[0]
82/72:
fig,ax = plt.subplots(nrows=len(event),ncols=1,figsize=(20,60))

#draw selected events
for i,ev in enumerate(event):

    global_event=ev[ev["chan"]<=128].sort_values(by="chamber").reset_index(drop=True)
    local=global_event.groupby("chamber")
    local_events=[local.get_group(x) for x in local.groups]

    #draw events
    dx = 1
    dy = 1/2
    gap=4

    ax[i].set_title("Global Event %d" %i,fontsize=30)
    draw_detector(ax[i])

    for j in range(len(local_events)):

        single=local_events[j].reset_index(drop=True)#.drop(["level_0","index"],axis=1)
        boxes(ax[i],centres(single)[0],centres(single)[1])
        ax[i].errorbar(*centres(single),dy/2,dx/2,fmt='o')

    ax[i].text(x=7.5,y=5*dy+3*gap,s="Chamber 3",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+2*gap,s="Chamber 2",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+gap,s="Chamber 1",fontsize=20)
    ax[i].text(x=7.5,y=5*dy,s="Chamber 0",fontsize=20)

plt.show()
82/73:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product
import matplotlib.pyplot as plt

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
82/74:
fig,ax = plt.subplots(nrows=len(event),ncols=1,figsize=(20,60))

#draw selected events
for i,ev in enumerate(event):

    global_event=ev[ev["chan"]<=128].sort_values(by="chamber").reset_index(drop=True)
    local=global_event.groupby("chamber")
    local_events=[local.get_group(x) for x in local.groups]

    #draw events
    dx = 1
    dy = 1/2
    gap=4

    ax[i].set_title("Global Event %d" %i,fontsize=30)
    draw_detector(ax[i])

    for j in range(len(local_events)):

        single=local_events[j].reset_index(drop=True)#.drop(["level_0","index"],axis=1)
        boxes(ax[i],centres(single)[0],centres(single)[1])
        ax[i].errorbar(*centres(single),dy/2,dx/2,fmt='o')

    ax[i].text(x=7.5,y=5*dy+3*gap,s="Chamber 3",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+2*gap,s="Chamber 2",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+gap,s="Chamber 1",fontsize=20)
    ax[i].text(x=7.5,y=5*dy,s="Chamber 0",fontsize=20)

plt.show()
82/75:
#function to calculate coordinates of centres of cells
def centres(ev):
    
    dx=1
    dy=1/2
    gap=4
    
    x = np.zeros(ev.shape[0])
    y = np.zeros(ev.shape[0])
    
    #calculate coordinates of centers
    for i in range(ev.shape[0]): 
        
        layer = ev.loc[i,'layer']
        cell = ev.loc[i,'cell']
        chamber = ev.loc[i,'chamber']

        if layer == 0 : 
            x[i] = cell/4
            y[i] = dy/2 +chamber*gap 
        
        elif layer == 2 :
            x[i] = (cell-2)/4 + dx/2
            y[i] = dy/2 + dy + chamber*gap
        
        elif layer == 1 :
            x[i] = (cell-1)/4
            y[i] = dy/2 + 2*dy + chamber*gap
        
        elif layer == 3 : 
            x[i] = (cell-3)/4 + dx/2
            y[i] = dy/2+3*dy + chamber*gap
        
            
    return [x,y]
82/76:
#function to draw the activated cells
def boxes(ax,x,y):
    
    dx = 1
    dy = 1/2
     
    for i in range(len(x)) :
        rect = patches.Rectangle((x[i]-dx/2,y[i]-dy/2),dx,dy,linewidth=3,edgecolor='r',facecolor='none')
        ax.add_patch(rect)
82/77:
#function to draw the detector
def draw_detector(ax) :
    
    dx = 1
    dy = 1/2
    gap=4

    #adjust
    ax.set_xlim(0,16.5)
    ax.set_ylim(-0.2,15.2)

    #coordinates of layer 0
    x_1 = np.arange(1,16)-dx/2
    y_1 = 0
    
    #coordinates of layer 2
    x_2 = x_1 + dx/2
    y_2 = dy
    
    #coordinates of layer 3
    x_3 = x_1 
    y_3 = 2*dy
    
    #coordinates of layer 4
    x_4 = x_2
    y_4 = 3*dy
    
    for j in range(4):
        
        for i in range(15): 
            rect = patches.Rectangle((x_1[i],y_1),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_2[i],y_2),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_3[i],y_3),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_4[i],y_4),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            ax.axis('off')
        
        #increment y chamber
        y_1+=gap
        y_2+=gap
        y_3+=gap
        y_4+=gap
82/78:
events=[value for value in data.groupby(["orbit"]).groups.values()]
event=[data.loc[events[i]]  for i in np.random.randint(low=0,high=len(events),size=6)] #select 6 random events to show
event[0]
82/79:
fig,ax = plt.subplots(nrows=len(event),ncols=1,figsize=(20,60))

#draw selected events
for i,ev in enumerate(event):

    global_event=ev[ev["chan"]<=128].sort_values(by="chamber").reset_index(drop=True)
    local=global_event.groupby("chamber")
    local_events=[local.get_group(x) for x in local.groups]

    #draw events
    dx = 1
    dy = 1/2
    gap=4

    ax[i].set_title("Global Event %d" %i,fontsize=30)
    draw_detector(ax[i])

    for j in range(len(local_events)):

        single=local_events[j].reset_index(drop=True)#.drop(["level_0","index"],axis=1)
        boxes(ax[i],centres(single)[0],centres(single)[1])
        ax[i].errorbar(*centres(single),dy/2,dx/2,fmt='o')

    ax[i].text(x=7.5,y=5*dy+3*gap,s="Chamber 3",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+2*gap,s="Chamber 2",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+gap,s="Chamber 1",fontsize=20)
    ax[i].text(x=7.5,y=5*dy,s="Chamber 0",fontsize=20)

plt.show()
82/80: df5
82/81:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
82/82:
fig,ax = plt.subplots(nrows=len(event),ncols=1,figsize=(20,60))

#draw selected events
for i,ev in enumerate(event):

    global_event=ev[ev["chan"]<=128].sort_values(by="chamber").reset_index(drop=True)
    local=global_event.groupby("chamber")
    local_events=[local.get_group(x) for x in local.groups]

    #draw events
    dx = 1
    dy = 1/2
    gap=4

    ax[i].set_title("Global Event %d" %i,fontsize=30)
    draw_detector(ax[i])

    for j in range(len(local_events)):

        single=local_events[j].reset_index(drop=True)#.drop(["level_0","index"],axis=1)
        boxes(ax[i],centres(single)[0],centres(single)[1])
        ax[i].errorbar(*centres(single),dy/2,dx/2,fmt='o')

    ax[i].text(x=7.5,y=5*dy+3*gap,s="Chamber 3",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+2*gap,s="Chamber 2",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+gap,s="Chamber 1",fontsize=20)
    ax[i].text(x=7.5,y=5*dy,s="Chamber 0",fontsize=20)

plt.show()
82/83:
vp1, vp2 = np.unique(d3.loc[d3.chan == 128, 'orbit'], return_counts = True)
d3.loc[d3.orbit.isin(vp1[vp2 > 1])] = None
d3 = d3.dropna().astype(tipi)
d3
82/84:
vp1, vp2 = np.unique(d3.loc[d3.chan == 128, 'orbit'], return_counts = True)
d3[d3.orbit.isin(vp1[vp2 > 1])] = None
#d3 = d3.dropna().astype(tipi)
#d3
82/85:
vp1, vp2 = np.unique(d3.loc[d3.chan == 128, 'orbit'], return_counts = True)
d3[d3.orbit.isin(vp1[vp2 > 1])].compute() = None
#d3 = d3.dropna().astype(tipi)
#d3
82/86:
# per rimuovere le orbite con pi scintillatori non posso utilizzare il metodo loc in quanto con un dask dataframe non  in grado di 
# aggiungere una colonna al dataframe 

# posso fare un groupby per orbit e chan e selezionare quei gruppi che hanno il canale 128 pi di una volta 
d3.groupby(['orbit', 'chan'])
82/87:
# per rimuovere le orbite con pi scintillatori non posso utilizzare il metodo loc in quanto con un dask dataframe non  in grado di 
# aggiungere una colonna al dataframe 

# posso fare un groupby per orbit e chan e selezionare quei gruppi che hanno il canale 128 pi di una volta 
d3.groupby(['orbit', 'chan']).compute()
82/88:
# per rimuovere le orbite con pi scintillatori non posso utilizzare il metodo loc in quanto con un dask dataframe non  in grado di 
# aggiungere una colonna al dataframe 

# posso fare un groupby per orbit e chan e selezionare quei gruppi che hanno il canale 128 pi di una volta 
d3.groupby(['orbit', 'chan'])
82/89: df3
82/90: df3.groupby(['orbit','chan'])
82/91: df3.groupby(['orbit','chan']).head()
82/92: df3.groupby(['orbit','chan']).count
82/93: df3.groupby(['orbit','chan']).count()
82/94:
prova = df3.groupby(['orbit','chan']).count()
prova
82/95:
prova = df3.groupby(['orbit','chan']).count()
prova[prova['chan'] == 128]
82/96:
prova = df3.groupby(['orbit','chan']).count()
prova
82/97:
prova = df3.groupby(['orbit','chan']).count()
prova['chan']
82/98:
prova = df3.groupby(['orbit','chan']).count()
prova
82/99:
prova = df3.groupby(['orbit','chan']).count()
prova.128
82/100:
prova = df3.groupby(['orbit','chan']).count()
prova.chan
82/101:
prova = df3.groupby(['orbit','chan']).count()
prova
82/102:
prova = df3.groupby(['orbit','chan']).count()
prova.chan
82/103:
prova = df3.groupby(['orbit','chan']).count()
prova.groups()
82/104:
prova = df3.groupby(['orbit','chan']).count()
prova.group()
82/105:
prova = df3.groupby(['orbit','chan']).count()
prova.group
82/106:
prova = df3.groupby(['orbit','chan']).count()
prova
82/107:
prova = df3.groupby(['orbit','chan']).count()
prova['chan']
82/108:
prova = df3.groupby(['orbit','chan']).count()
prova[0]
82/109:
prova = df3.groupby(['orbit','chan']).count()
prova[0,:]
82/110:
prova = df3.groupby(['orbit','chan']).count()
prova.loc[0,:]
82/111:
prova = df3.groupby(['orbit','chan']).count()
prova.loc[:,:]
82/112:
prova = df3.groupby(['orbit','chan']).count()
prova.loc[128,:]
82/113:
prova = df3.groupby(['orbit','chan']).count()
prova.iloc[128,:]
82/114:
prova = df3.groupby(['orbit','chan']).count()
prova.loc[128,:]
82/115:
prova = df3.groupby(['orbit','chan']).count()
prova.iloc[df3['chan']==128,:]
82/116:
prova = df3.groupby(['orbit','chan']).count()
prova.loc[df3['chan']==128,:]
82/117:
prova = df3.groupby(['orbit','chan']).count()
prova[df3['chan']==128]
82/118:
prova = df3.groupby(['orbit','chan']).count()
prova[df3['chan']==128,:]
82/119:
prova = df3.groupby(['orbit','chan']).count()
df3['chan']==128
82/120:
prova = df3.groupby(['orbit','chan']).count()
df3[df3['chan']==128].index
82/121:
prova = df3.groupby(['orbit','chan']).count()
df3[df3['chan']==128]
82/122:
prova = df3.groupby(['orbit','chan']).count()
np.any(df3['chan']==128)
82/123: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
82/124: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
82/125: df = df[(df['head']==2)]
82/126:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
82/127: len(df1)/len(df)
82/128:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
82/129: orbite_buone = df1.orbit[indici].values
82/130:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
82/131:
df3._is_copy = None 
# trucco ihih perch df3  una copia di df1 e lui ne tiene conto in questo modo segnalando warning
82/132:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
82/133:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
np.any(df3['chan']==128)
82/134:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
df3[df3['chan']==128]
82/135:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
df3[df3['chan']==128].index
82/136:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
#df3[df3['chan']==128].index
prova
82/137:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
prova[uau]
82/138:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
prova.iloc[uau]
82/139:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
prova.loc[uau,:]
82/140:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
prova
82/141:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
prova[prova['tdc'] < 2]
82/142:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
df3.groupby(['orbit','chan']).groups
82/143:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
df3.groupby(['orbit','chan']).groups[128]
82/144:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
df3.groupby(['orbit','chan']).groups
82/145:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
type(df3.groupby(['orbit','chan']).groups)
82/146:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
(df3.groupby(['orbit','chan']).groups)
82/147:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.values
82/148:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.values()
82/149:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio[dizio.values() > 2]
82/150:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.values() > 2
82/151:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.values()
82/152:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.items()
82/153:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.items()[]
82/154:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
dizio.items()[0]
82/155:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
prova.index
82/156:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
prova[prova['tdc'] > 2]
82/157:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
prova[prova['tdc'] > 1]
82/158:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due 
prova[prova['tdc'] > 1].index
82/159:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

prova[prova['tdc'] > 1].index[]
82/160:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

prova[prova['tdc'] > 1].index[128]
82/161:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

indici_filtro == (,128)
82/162:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

any(ch == 128 for c,ch in indici_filtro)
82/163:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

ch == 128 for c,ch in indici_filtro
82/164:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

indici_filtro[ch == 128 for c,ch in indici_filtro]
82/165:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

dict(indici_filtro)[128]
82/166:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

dict(indici_filtro)
82/167:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index
indici_filtro
82/168:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

dict(indici_filtro)
82/169:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

dict(indici_filtro).values()
82/170:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

dict(indici_filtro).values() == 128
82/171:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

dict(indici_filtro).values()
82/172:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df3.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

np.where(dict(indici_filtro).values()==128)
82/173:
r = np.array((1,2,3))
np.where(r==1)
82/174:
r = np.array((1,2,3))
np.where(r==2)
82/175:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

np.where(dict(indici_filtro).values()==128)
82/176: df.groupby('chan').count()
82/177:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

np.where(dict(indici_filtro).values()==120)
82/178:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

np.where(dict(indici_filtro).values()==110)
82/179: df
82/180:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

np.where(dict(indici_filtro).values()==105)
82/181: prova[prova['tdc'] > 1]
82/182: prova[prova['tdc'] > 1].values()
82/183: prova[prova['tdc'] > 1].values
82/184: prova[prova['tdc'] > 1]
82/185: prova[prova['tdc'] > 1].groups()
82/186: prova[prova['tdc'] > 1].groups
82/187: prova[prova['tdc'] > 1]
82/188: prova[prova['tdc'] > 1].index
82/189: np.where(dict(prova[prova['tdc'] > 1].index).values()==128)
82/190: dict(prova[prova['tdc'] > 1].index).values()
82/191: dict(prova[prova['tdc'] > 1].index).values()==128
82/192: any(dict(prova[prova['tdc'] > 1].index).values()==128)
82/193: np.any(dict(prova[prova['tdc'] > 1].index).values()==128)
82/194:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
# uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index

np.any(dict(indici_filtro).values()==128)
82/195: dict(indici_filtro)
82/196: dict(indici_filtro).values()
82/197:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
# uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index
dizio = dict(indici_filtro)
np.any(dizio.values()==128) 

# SIGNIFICA CHE NON CI SONO PIU' DI UNO SCINTILLATORE PER ORBITA UAU. SE CI DOVESSERO ESSERE DOVREI FARE COSI'
82/198:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
dizio
82/199:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
dizio[0]
82/200:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
dizio
82/201:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
dizio.keys
82/202:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
dizio.keys()
82/203:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
lista_orbits = list(dizio.keys())
lista_orbits
82/204:
match = np.where(dict(indici_filtro).values() == 128) # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
lista_orbits = list(dizio.keys())
lista_orbits[match] #  quello che farebbe al caso nostro se solo ci fossero tali condizioni rispettate
82/205: match
82/206: match[0]
82/207: dizio.values()
82/208: dizio.values() == 124
82/209: dizio.values()[dizio.values() == 124]
82/210: dizio.values()
82/211: list(dizio.values())
82/212: list(dizio.values()) == 128
82/213: list(dizio.values())[list(dizio.values())==128]
82/214: list(dizio.values())
82/215: list(dizio.values())[53]
82/216: list(dizio.values())
82/217: list(dizio.values()).index(128)
82/218: list(dizio.values())[11088]
82/219: dd = list(1,2,2,2,2,2,3)
82/220: dd = list([1,2,2,2,2,2,3])
82/221:
dd = list([1,2,2,2,2,2,3])
dd.index(2)
82/222: 128 in list(dizio.values())
82/223:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
# uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index
dizio = dict(indici_filtro)
# np.any(dizio.values()==128) attenzione che cos non li trover mai perch dizio.values non  un array ma una tuple quindi il broadcasting
# mi restituisce informazioni sbagliate 
128 in lsit(dizio.values())
82/224:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
# uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

indici_filtro = prova[prova['tdc'] > 1].index
dizio = dict(indici_filtro)
# np.any(dizio.values()==128) attenzione che cos non li trover mai perch dizio.values non  un array ma una tuple quindi il broadcasting
# mi restituisce informazioni sbagliate 
128 in list(dizio.values())
82/225:
dd = list([1,2,2,2,2,2,3])
np.where(dd==2)
82/226:
dd = list([1,2,2,2,2,2,3])
np.where(dd=2)
82/227:
dd = [1,2,2,2,2,2,3]
dd
82/228:
dd = [1,2,2,2,2,2,3]
dd==2
82/229:
dd = [1,2,2,2,2,2,3]
dd[dd==2]
82/230:
dd = [1,2,2,2,2,2,3]
dd[np.where(2 in dd)]
82/231:
dd = np.array([1,2,2,2,2,2,3])
dd == 2
82/232: dizio.values())
82/233: dizio.values()
82/234: np.array(dizio.values())
82/235: np.array(dizio.values()) == 128
82/236: np.array(list(dizio.values())) == 128
82/237:
match = np.array(list(dizio.values())) == 128 # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
lista_orbits = list(dizio.keys())
lista_orbits[match] #  quello che farebbe al caso nostro se solo ci fossero tali condizioni rispettate
82/238: match
82/239:
match = np.array(list(dizio.values())) == 128 # cos trovo le posizioni nel dizionario in cui ci sono match di valori 128 
array_orbits = np.array(list(dizio.keys()))
array_orbits[match] #  quello che farebbe al caso nostro se solo ci fossero tali condizioni rispettate
82/240:
match = np.array(list(dizio.values())) != 128 # cos trovo le posizioni nel dizionario in cui NON ci sono match di valori 128 
array_orbits = np.array(list(dizio.keys()))
orbits_tenere = array_orbits[match] 
# ecco trovate le orbite da tenere. Anzich trovare le orbite da eliminare prendiamo quelle da tenere perch  pi semplice selezionare il 
# dataframe che abbia queste orbite qui
82/241:
# per rimuovere le orbite con pi scintillatori non posso utilizzare il metodo loc in quanto con un dask dataframe non  in grado di 
# aggiungere una colonna al dataframe 

#L'INIZIO DEL DELIRIO
82/242: client.close()
83/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
83/2: client = Client()
83/3: # client = Client('78.13.47.54:8001') #cluster di mattia
83/4: client
83/5:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
83/6:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
83/7:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
83/8:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
83/9:
def head(x):
    if x[5] == 2: return x
    
@njit    
def head_filter(x):
    if x[5] == 2: return True 

def head_remove(x):
    return x[5] != 2

# b.map(head).compute()
# questa bag ha lo stesso numero di elementi della prima ma con None al posto di quegli array che non hanno head == 2

# b.remove(head_remove).compute()
# mi sa che remove non  una buona scelta per fare un filtro visto che a differenza di map non so come venga applicata la funzione head_remove
# alla bag
83/10:
# %%timeit
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d = d[d['head'] == 2]
# d1.compute()
83/11:
cnt = d.orbit.value_counts().compute()
cnt = cnt[(cnt > 3) & (cnt < 15)]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
83/12:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
83/13: d1 = d[d['orbit'].isin(cnt.index)]
83/14:
lunghezzad1 = len(d1)
lunghezzad = len(d)
83/15:
# faccio cos per evitare di dover calcolare ogni volta le due lunghezze dei dask dataframes. L'operazion fatta tramite dask restituisce
# lo stesso risultato di quella invece fatta sequenzialmente tramite pandas
lunghezzad1/lunghezzad
83/16:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
83/17:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
83/18: d1
83/19:
fpga = d1[d1['fpga'] == 1].orbit.values.compute()
# anche questa volta memorizzo la serie contenente le orbite che devo mantenere nel dataframe. Visto che per ogni evento le hits hanno la 
# stessa orbit allora in questo modo ho selezionato tutte le hit che soddisfano il filtro
83/20: d2 = d1[d1['orbit'].isin(fpga)]
83/21:
chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan memorizzo l'array che mi serve per selezionare gli eventi buoni
83/22:
d3 = d2[d2['orbit'].isin(chan)]
# facendo compute quindi calcolando esplicitamente il d2 e il d3 perch ancora d2  una chiamata non un'esecuzione impieghiamo soltanto un 
# centinaio di millisecondi; il tutto semplicemente memorizzando il dataframe d1 con .persist e aumentando il numero di partizioni a 12. 
# Adesso che per il numero di righe del dataframe  diminuito sarebbe necessario diminuire altrettanto il numero di partizioni
83/23: d3['time'] = 25 * ((d3['orbit'] * 3564) + d3['bx'] + (d3['tdc']/30))
83/24: d3
83/25:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

d3.groupby(['orbit','chan']).count()
83/26:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
83/27:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
dizionario = dict(indici)
dizionario.values()
83/28:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
dizionario = dict(indici)
dizionario
83/29:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
dizionario = dict(indici)
indici
83/30:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
dizionario = dict(indici)
# gi da dizionario iniziano ad essere eseguiti i task delle promesse fatte in conteggi ed indici. 
match = np.array(list(dizionario.values())) != 128 # cos trovo le posizioni nel dizionario in cui NON ci sono match di valori 128 
array_orbits = np.array(list(dizionario.keys()))
orbits_tenere = array_orbits[match] 

orbits_tenere
83/31: df['orbit'][orbits_tenere]
83/32:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
83/33: fin = np.concatenate(fin)
83/34: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
83/35: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
83/36: df = df[(df['head']==2)]
83/37:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
83/38: df['orbit'][orbits_tenere]
83/39:
# prova per elilminare in un modo alternativo i gruppi che hanno pi di uno scintillatore 128
prova = df.groupby(['orbit','chan']).count()
# uau = df3[df3['chan']==128].index
# prova[prova['tdc'] < 2] # qua per voglio selezionare soltanto quelli che hanno il canale 128 e capire a quale orbita corrispondono
# dizio = (df3.groupby(['orbit','chan']).groups) 
# di questi mi interessano quelli che hanno come valore un numero pi di due innanzitutto ovvero che hanno un canale che compare sicuramente
# pi di una volta. Tra questi ci saranno anche quelli con le chiavi 128 che devo eliminare. Per fare ci guardo le orbite corrispondenti
# alle chiavi channel = 128 dopo aver effettuato il filtro di valori maggiori di due. Faccio la stessa cosa ma in basso tramite direttamente 
# una series di pandas e trovando un array multiIndex

# indici_filtro = prova[prova['tdc'] > 1].index
# dizio = dict(indici_filtro)
# # np.any(dizio.values()==128) attenzione che cos non li trover mai perch dizio.values non  un array ma una tuple quindi il broadcasting
# # mi restituisce informazioni sbagliate 
# 128 in list(dizio.values()) #significa che c' il 128 quindi bisogna trovare le chiavi corrispondenti
83/40: prova
83/41:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
dizionario = dict(indici)
# gi da dizionario iniziano ad essere eseguiti i task delle promesse fatte in conteggi ed indici. 
match = np.array(list(dizionario.values())) != 128 # cos trovo le posizioni nel dizionario in cui NON ci sono match di valori 128 
array_orbits = np.array(list(dizionario.keys()))
orbits_tenere = array_orbits[match] 

d3['orbit'][orbits_tenere]
83/42: df['orbit'][orbits_tenere]
83/43: orbits_tenere
83/44:
# MI SONO RICONDOTTO QUINDI SEMPRE AL METODO ISIN PER FARE UN FILTRO. VEDIAMO DI APPLICARE QUESTI METODI AL DASK DATAFRAME

conteggi = d3.groupby(['orbit','chan']).count()
indici = conteggi[conteggi['tdc'] > 1].index
dizionario = dict(indici)
# gi da dizionario iniziano ad essere eseguiti i task delle promesse fatte in conteggi ed indici. 
match = np.array(list(dizionario.values())) != 128 # cos trovo le posizioni nel dizionario in cui NON ci sono match di valori 128 
array_orbits = np.array(list(dizionario.keys()))
orbits_tenere = array_orbits[match] 

d4 = d3[d3['orbit'].isin(orbits_tenere)]
83/45: d4
83/46: len(d4)
83/47: len(df1)/len(df)
83/48:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
83/49:
len(indici)
#  diverso da len(df3)
83/50: orbite_buone = df1.orbit[indici].values
83/51:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
83/52:
df3._is_copy = None 
# trucco ihih perch df3  una copia di df1 e lui ne tiene conto in questo modo segnalando warning
83/53:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
83/54:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
83/55: len(df3)
83/56: df2
83/57: df3.groupby(['orbit','chan']).count
83/58: df3.groupby(['orbit','chan']).count()
83/59: df3.groupby(['orbit','chan']).groups()
83/60: df3.groupby(['orbit','chan']).groups
83/61: df3.groupby(['orbit','chan']).count()
83/62: df3.groupby(['orbit','chan']).count()[df3['orbit'] == 128]
83/63: df3.groupby(['orbit','chan']).count()[df3['chan'] == 128]
83/64: df3.groupby(['orbit','chan']).count()[(df3.groupby(['orbit','chan']).count()) == 128]
83/65: df3.groupby(['orbit','chan']).count()[(df3.groupby(['orbit','chan']).count())['chan'] == 128]
83/66: df3.groupby(['orbit','chan']).count()[(df3.groupby(['orbit','chan']).count()).reset_index['chan'] == 128]
83/67: df3.groupby(['orbit','chan']).count()[(df3.groupby(['orbit','chan']).count()).reset_index()['chan'] == 128]
83/68:
v1, v2 = np.unique(d3['orbit'][d3.chan == 128], return_counts = True)
d3[d3.orbit.isin(v1[v2 > 1])] = None
d3 = d3.dropna().astype(tipi)
d3
83/69:
v1, v2 = np.unique(d3['orbit'][d3.chan == 128], return_counts = True).compute()
d3[d3.orbit.isin(v1[v2 > 1])] = None
d3 = d3.dropna().astype(tipi)
d3
83/70:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
d3[d3.orbit.isin(v1[v2 > 1])] = None
d3 = d3.dropna().astype(tipi)
d3
83/71: (d3['orbit'][d3.chan == 128]).compute()
83/72:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
print(v1,v2)
# d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
83/73:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# print(v1,v2)
d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
83/74:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# print(v1,v2)
(d3[d3.orbit.isin(v1[v2 > 1])]).compute() = None
# d3 = d3.dropna().astype(tipi)
# d3
83/75:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
d5 = d3[d3.orbit.isin(v1[v2 < 2])]
d5
83/76:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
d5 = d3[d3.orbit.isin(v1[v2 < 2])]
len(d5)
83/77: d4.groupby(['orbit','chan']).count()
83/78: d4.groupby(['orbit','chan']).count().compute()
83/79:
def multiple_scint(group):
    if group[group.chan==128].shape[0]>1: return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u2')]

d4l=d3.groupby("orbit").apply(multiple_scint,meta=tipi).reset_index(drop=True)
83/80: len(d4l)
83/81:
def multiple_scint(group):
    if group[group.chan==128].shape[0]>1: return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('t','u2')]

d4l=d3.groupby("orbit").apply(multiple_scint,meta=metadata).reset_index(drop=True)
83/82: len(d4l)
83/83: d3
83/84:
def multiple_scint(group):
    if group[group.chan==128].shape[0]>1: return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2')]

d4l=d3.groupby("orbit").apply(multiple_scint,meta=metadata).reset_index(drop=True)
83/85: len(d4l)
83/86: len(d4l)
83/87: len(d4)
83/88:
%%timeit
d4l.compute()
83/89:
%%timeit
d5.compute()
83/90:
%%timeit
d4.compute()
83/91:
s = d5.loc[d5['chan'] == 128, ['orbit', 't']].set_index('orbit')
d5['t0'] = s.loc[df3.orbit].t.to_numpy()
83/92:
s = d5.loc[d5['chan'] == 128, ['orbit', 't']].set_index('orbit')
# d5['t0'] = s.loc[df3.orbit].t.to_numpy()
83/93:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[df3.orbit].t.to_numpy()
83/94:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[df3.orbit].t.to_numpy()

s
83/95:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[df3.orbit].t.to_numpy()

s.compute()
83/96:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].t.to_numpy()
83/97:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].t.to_numpy()
d5.compute()
83/98: d3['time'] = 25 * (d3['bx'] + (d3['tdc']/30))
83/99:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
d5 = d3[d3.orbit.isin(v1[v2 < 2])]

# SCELTA FINALE PER FILTRO DI PIU' SCINTILLATORI PER ORBITA
# 130 ms  10.9 ms per loop (mean  std. dev. of 7 runs, 10 loops each)
# len(d5) = 8100
83/100:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].t.to_numpy()
d5.compute()
83/101:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
83/102: d5.repartition(npartitions=4)
83/103:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
83/104: d5
83/105:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore 
d5 = d5.repartition(npartitions=4)
83/106:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
83/107:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
83/108: fin = np.concatenate(fin)
83/109: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
83/110: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
83/111: df = df[(df['head']==2)]
83/112:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
83/113: len(df1)/len(df)
83/114:
# (((df1['fpga'] == 1) & (df1['chan'] == 128)).values)[(((df1['fpga'] == 1) & (df1['chan'] == 128)).values)==True]
# intanto significa che ci sono scintillatori che verificano questa condizione e prendo ad esempio i primi cinque
83/115:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
83/116:
len(indici)
#  diverso da len(df3)
83/117: orbite_buone = df1.orbit[indici].values
83/118:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
83/119:
df3._is_copy = None 
# trucco ihih perch df3  una copia di df1 e lui ne tiene conto in questo modo segnalando warning
83/120:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
83/121:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
83/122:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()

df3
83/123:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
83/124:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].t
# d5.compute()
83/125:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
s
83/126:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
s.loc[s.orbit].t.to_numpy()
83/127:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
s.loc[d5.orbit].t.to_numpy()
83/128:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].t.to_numpy()
# d5.compute()
# s.loc[d5.orbit].t.to_numpy()
s.compute()
83/129: d5.compute()
83/130: s[d5.orbit]
83/131: d5.orbit
83/132: d5.orbit.compute()
83/133:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].t.to_numpy()
s.compute()
83/134:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
s.loc[d5.orbit].time.to_numpy()
# s.compute()
83/135:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']]
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
s.compute()
83/136:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']]
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
s.loc[d5.orbit].time.to_numpy()
# s.compute()
83/137:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
d5.set_index('orbit')
83/138:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit')['orbit']].time.to_numpy()
d5.set_index('orbit').orbit
83/139:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit')['orbit']].time.to_numpy()
d5.set_index('orbit').index
83/140:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time.to_numpy()
d5.set_index('orbit').index
83/141:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
d5['t0'] = s.loc[d5.set_index('orbit').index].time.to_numpy()
# d5.set_index('orbit').index
83/142:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
d5['t0'] = s.loc[d5.set_index('orbit').index].time
# d5.set_index('orbit').index
83/143:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
s.loc[d5.set_index('orbit').index]
83/144:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
s.loc[d5.set_index('orbit').index].time
83/145: df3
83/146: df3.set_index('orbit').reset_index
83/147: df3.set_index('orbit').reset_index()
83/148:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
d5.set_index('orbit')['t0'] = s.loc[d5.set_index('orbit').index].time
83/149: d5.compute()
83/150:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
d5 = d5.set_index('orbit')
d5['t0'] = s.loc[d5.set_index('orbit').index].time
83/151:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
d5 = d5.set_index('orbit')
d5['t0'] = s.loc[d5.index].time
83/152:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
d5.set_index('orbit')['t0'] = s.loc[d5.set_index('orbit').index].time
83/153:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
d5 = d3[d3.orbit.isin(v1[v2 < 2])]

# SCELTA FINALE PER FILTRO DI PIU' SCINTILLATORI PER ORBITA
# 130 ms  10.9 ms per loop (mean  std. dev. of 7 runs, 10 loops each)
# len(d5) = 8100
83/154: d5.compute()
83/155:
s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
d5 = d5.set_index('orbit')
d5['t0'] = s.loc[d5.index].time
83/156: d5.compute()
83/157: d5 = d5.reset_index()
83/158: d5.compute()
83/159: d5.loc[d5['chan'] > 127]
83/160:
# rimuovo gli scintillatori, che non possono essere piazzati in nessuna colonna in particolare
# NON SOLO QUELLI! Ci sono canali spuri ancora al di sopra
#df2.loc[df2['chan'] == 128] = None  # non basta
d5.loc[d5['chan'] > 127] = None 
d5.dropna(inplace = True)
d5
83/161: d5[d5['chan'] > 127]
83/162: d5[d5['chan'] > 127] = None
83/163: d5[d5['chan'] < 128]
83/164:
# rimuovo gli scintillatori, che non possono essere piazzati in nessuna colonna in particolare
# NON SOLO QUELLI! Ci sono canali spuri ancora al di sopra
#df2.loc[df2['chan'] == 128] = None  # non basta
d5 = d5[d5['chan'] < 128]
83/165: d5.compute()
83/166:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)

# df3.loc[(df3['fpga'] == 0) & (df3['chan'] < 64), 'chamber'] = 0
# df3.loc[(df3['fpga'] == 0) & (df3['chan'] > 63), 'chamber'] = 1
# df3.loc[(df3['fpga'] == 1) & (df3['chan'] < 64), 'chamber'] = 2
# df3.loc[(df3['fpga'] == 1) & (df3['chan'] > 63), 'chamber'] = 3

# df3
83/167: d6
83/168: df3.loc[(df3['fpga'] == 0) & (df3['chan'] < 64), 'chamber'] = 0
83/169: d6.loc[(d6['fpga'] == 0) & (d6['chan'] < 64), 'chamber'] = 0
83/170: d6[(d6['fpga'] == 0) & (d6['chan'] < 64), 'chamber'] = 0
83/171: d6['chamber'][(d6['fpga'] == 0) & (d6['chan'] < 64)] = 0
83/172: (d6['fpga'] == 0) & (d6['chan'] < 64)
83/173: (d6['fpga'] == 0) & (d6['chan'] < 64).compute()
83/174: (d6['fpga'] == 0) & (d6['chan'] < 64)
83/175: ((d6['fpga'] == 0) & (d6['chan'] < 64)).compute()
83/176:
for i in range(len(condizioni_chambers)):
    d6['chamber'][condizioni_chambers[i]] = i
83/177:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
condizioni_chambers = [((d6['fpga'] == 0) & (d6['chan'] < 64)).compute(), ((d6['fpga'] == 0) & (d6['chan'] > 63)).compute(),
                       ((d6['fpga'] == 1) & (d6['chan'] < 64)).compute(), ((d6['fpga'] == 1) & (d6['chan'] > 63)).compute()]
83/178:
for i in range(len(condizioni_chambers)):
    d6['chamber'][condizioni_chambers[i]] = i
83/179: condizioni_chambers[0]
83/180: d6['chamber'][condizioni_chambers[0]]
83/181:
d6['chamber']
# d6['chamber'][condizioni_chambers[0]]
83/182:
d6['chamber'].compute()
# d6['chamber'][condizioni_chambers[0]]
83/183: condizioni_chambers[0]
83/184: condizioni_chambers[0].values()
83/185: condizioni_chambers[0].values
83/186: d6['chamber'][condizioni_chambers[0].values]
83/187: len(condizioni_chambers)
83/188: len(condizioni_chambers[0])
83/189:
# for i in range(len(condizioni_chambers)):
#     d6['chamber'][condizioni_chambers[i]] = i
d6['chamber'][condizioni_chambers[0]]
83/190: condizioni_chambers[0]
83/191: condizioni_chambers[0].values
83/192: d6['chamber'][condizioni_chambers[0].values]
83/193:
for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
83/194:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks = [((d6['fpga'] == 0) & (d6['chan'] < 64)).compute(), ((d6['fpga'] == 0) & (d6['chan'] > 63)).compute(),
                       ((d6['fpga'] == 1) & (d6['chan'] < 64)).compute(), ((d6['fpga'] == 1) & (d6['chan'] > 63)).compute()]
83/195:
for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
83/196:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks=[((df2["fpga"]==0)&(df2["chan"]//64==0)),
       ((df2["fpga"]==0)&(df2["chan"]//64==1)),
       ((df2["fpga"]==1)&(df2["chan"]//64==0)),
       ((df2["fpga"]==1)&(df2["chan"]//64==1))]
83/197:
for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
83/198:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks=[((d6["fpga"]==0)&(d6["chan"]//64==0)),
       ((d6["fpga"]==0)&(d6["chan"]//64==1)),
       ((d6["fpga"]==1)&(d6["chan"]//64==0)),
       ((d6["fpga"]==1)&(d6["chan"]//64==1))]
83/199:
for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
83/200: d6.compute()
83/201:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks=[((d6["fpga"]==0)&(d6["chan"]<64)),
       ((d6["fpga"]==0)&(d6["chan"]>63)),
       ((d6["fpga"]==1)&(d6["chan"]<64)),
       ((d6["fpga"]==1)&(d6["chan"]>63))]

for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
83/202: d5
83/203: d5.compute()
83/204:
%%time
d5.compute()
83/205: client.close()
86/1:
import os
import dask
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
86/2: client = Client()
86/3: # client = Client('78.13.47.54:8001') #cluster di mattia
86/4: client
86/5:
path = os.path.join('data_0000*.dat')
all_rec = iglob(path, recursive=True)
data_names = [i for i in all_rec]

# def load_from_file(file): 
#     m = np.fromfile(file, dtype = '<u8')
    

# b = db.from_sequence([])
# for i in data_names:
#     b[i] = (db.from_sequence([i]).map(load_from_file))
# non funziona 

# def load_from_file_all(files):
#     for file in files:
#         m = np.concatenate(m, np.fromfile(file, dtype= '<u8'))
#     m = [*(np.fromfile(file, dtype = '<u8') for file in files)]
#     return m 
# data_names1 = ['data_000001.dat','data_000000.dat']
86/6:
def convert(mi):
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat

def load_from_file(file): 
    m = np.fromfile(file, dtype = '<u8')
    mat = convert(m)
    return mat 

# b = db.from_sequence(data_names).map(load_from_file).flatten()
86/7:
def load_from_file1(mi):
    m = np.fromfile(file, dtype = '<u8')
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    return mat
86/8:
data_names1 = ['data_000000.dat', 'data_000001.dat']
b = db.from_sequence(data_names1).map(load_from_file).flatten()
86/9:
# %%timeit
d = b.to_dataframe(meta = [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d = d[d['head'] == 2]
# d1.compute()
86/10:
cnt = d.orbit.value_counts().compute()
cnt = cnt[(cnt > 3) & (cnt < 15)]
# per rendere pi veloce il conto decido di memorizzare in ram gli indici che servono per filtrare il dataframe
86/11:
# %%time
# d1 = d[d['orbit'].isin(cnt.index)].compute()
# Il broadcasting senza apply  venti volte pi veloce (60 secondi dell'apply contro i 3s invece del broadcasting)
86/12: d1 = d[d['orbit'].isin(cnt.index)]
86/13:
d1 = d1.persist()
# decido di memorizzare temporaneamente in ram il dataframe con ancora non tutti i filtri effettuati
86/14:
# potrebbe risultare utile, almeno in questo caso particolare essendo le operazioni da fare indipendenti dal numero di file letti ma funzioni 
# del numero di eventi considerati (tramite le orbits) ripartizionare il dask df. In principio le partizioni sono tante quante il numero di 
# file letti. Potremmo considerare tante partizioni, quante come dice la documentazione di dask 

d1 = d1.repartition(npartitions=12) # tanti quanti sono i threads
86/15: d1
86/16:
# Filtro di fpga =1 

fpga = d1[d1['fpga'] == 1].orbit.values.compute()
# anche questa volta memorizzo la serie contenente le orbite che devo mantenere nel dataframe. Visto che per ogni evento le hits hanno la 
# stessa orbit allora in questo modo ho selezionato tutte le hit che soddisfano il filtro
86/17: d2 = d1[d1['orbit'].isin(fpga)]
86/18:
# Filtro di chan = 128

chan = d2[d2['chan'] == 128].orbit.values.compute()
# allo stesso modo per chan memorizzo l'array che mi serve per selezionare gli eventi buoni
86/19:
d3 = d2[d2['orbit'].isin(chan)]
# facendo compute quindi calcolando esplicitamente il d2 e il d3 perch ancora d2  una chiamata non un'esecuzione impieghiamo soltanto un 
# centinaio di millisecondi; il tutto semplicemente memorizzando il dataframe d1 con .persist e aumentando il numero di partizioni a 12. 
# Adesso che per il numero di righe del dataframe  diminuito sarebbe necessario diminuire altrettanto il numero di partizioni
86/20: d3['time'] = 25 * (d3['bx'] + (d3['tdc']/30))
86/21:
v1, v2 = np.unique((d3['orbit'][d3.chan == 128]).compute(), return_counts = True)
# d3[d3.orbit.isin(v1[v2 > 1])] = None
# d3 = d3.dropna().astype(tipi)
# d3
d5 = d3[d3.orbit.isin(v1[v2 < 2])]

# SCELTA FINALE PER FILTRO DI PIU' SCINTILLATORI PER ORBITA
# 130 ms  10.9 ms per loop (mean  std. dev. of 7 runs, 10 loops each)
# len(d5) = 8100
86/22:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    #do some stuff
    stop = time.time()
    duration = stop-start
    print(duration)
# d5 = d5.repartition(npartitions=4)
86/23:
import os
import dask
import time
from distributed import Client
import dask.dataframe as dd
import pandas as pd
import io
import requests
import base64
import binascii
import numpy as np
import dask.bag as db
from numba import njit
import re
from glob import iglob
from scipy.interpolate import BarycentricInterpolator
from itertools import product
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# url = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_batch/'
# data_names = requests.get(url, verify = False).text
# data_names = data_names.split(sep = '\n')
86/24:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    #do some stuff
    stop = time.time()
    duration = stop-start
    print(duration)
# d5 = d5.repartition(npartitions=4)
86/25: d5.sum(axis=1)
86/26: d5.sum(axis=1).compute()
86/27: d5.sum().compute()
86/28: d5.mean().compute()
86/29: d5.apply(lambda x: x**2)
86/30: d5.apply(lambda x: x**2, axis=1)
86/31: d5.apply(lambda x: x**2, axis=1, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
86/32:
# d5.apply(lambda x: x**2, axis=1, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1')])
d5
86/33:
d5.apply(lambda x: x**2, axis=1, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
86/34:
d5.groupby('orbit').apply(lambda x: x**2, axis=1, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
86/35:
d5.groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
86/36:
d5.groupby('orbit').apply(lambda x: len(x)**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
86/37:
d5.groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
86/38:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print(duration)
# d5 = d5.repartition(npartitions=4)
86/39:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print(i,duration)
# d5 = d5.repartition(npartitions=4)
86/40:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print(i,duration)
# d5 = d5.repartition(npartitions=4)
86/41: d5
86/42:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print(i,duration,drep)
# d5 = d5.repartition(npartitions=4)
86/43:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print(i,duration)
    display(drep)
# d5 = d5.repartition(npartitions=4)
86/44:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration)
# d5 = d5.repartition(npartitions=4)
86/45:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.repartition(npartitions=i).groupby('orbit').apply(lambda x: x**2, meta=[('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), 
                                       ('time', 'u4')]).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
# d5 = d5.repartition(npartitions=4)
86/46:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1'),('t0','u2')]


# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True)
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/47:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1'),('t0','u2')]


# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    drep = d5.groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/48: d5
86/49:
drep = d5.copy()
drep
86/50:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1'),('t0','u2')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for i,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,i)

    #add layer column
    drep["layer"]=drep["chan"]%4

    #add cell column
    drep["cell"]=drep["chan"]%64
    drep.groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/51:
# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,24,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for i,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,i)

    #add layer column
    drep["layer"]=drep["chan"]%4

    #add cell column
    drep["cell"]=drep["chan"]%64
    drep.groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/52:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,30,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for i,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,i)

    #add layer column
    drep["layer"]=drep["chan"]%4

    #add cell column
    drep["cell"]=drep["chan"]%64
    drep.groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/53:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,30,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(nrepartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/54:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(4,30,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/55:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(0,30,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/56:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(1,30,4):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/57:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(2,30,2):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration = stop-start
    print("nrepartitions=",i,"duration=",duration,"s")
86/58:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()
n = [i for i in range(2,30,2)]
duration = []

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(2,30,2):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration.append(stop-start)
    
    print("nrepartitions=",i,"duration=",duration[i],"s")
86/59:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()
n = [i for i in range(2,30,2)]
duration = []

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i,k in enumerate(range(2,30,2)):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration.append(stop-start)
    
    print("nrepartitions=",i,"duration=",duration[k],"s")
86/60:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()
n = [i for i in range(2,30,2)]
duration = []

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i,k in enumerate(1+range(2,30,2)):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    duration.append(stop-start)
    
    print("nrepartitions=",i,"duration=",duration[k],"s")
86/61:
# PROVA PARTIZIONI 

# decido in questo momento di ripartizionare in quanto il dataframe iniziale  stato ridotto a tal punto da rendere possibile una suddivisione
# in partizioni con n minore. confronto comunque tramite una semplice operazione nel dataframe come la somma di tutti i valori di una colonna 
# che coinvolge interamente tutte le partizioni in modo che comunichino tra di loro 

# definisco un filtro da usare con apply che effettivamente  un processo che sottointende uno shuffle di tutti i dati nel dataframe e una 
# comunicazione tra le partizioni
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata=[('tdc', 'u1'),('bx', 'u2'),('orbit', 'u4'),('chan', 'u2'),('fpga', 'u1'),('head', 'u1'),
         ('time','u2'),('chamber','u1'),('layer','u1'),('cell','u1')]

drep = d5.copy()
n = [i for i in range(2,30,2)]
duration = []

# Scelgo un numero di partizioni multiplo del numero di workers che ho 
for i in range(2,30,2):
    start = time.time()
    #assign chamber values
    drep["chamber"]=None

    masks=[((drep["fpga"]==0)&(drep["chan"]//64==0)),
           ((drep["fpga"]==0)&(drep["chan"]//64==1)),
           ((drep["fpga"]==1)&(drep["chan"]//64==0)),
           ((drep["fpga"]==1)&(drep["chan"]//64==1))]

    for j,mask in enumerate(masks):
        drep["chamber"]=drep["chamber"].mask(mask,j)

        #add layer column
        drep["layer"]=drep["chan"]%4

        #add cell column
        drep["cell"]=drep["chan"]%64
    drep.repartition(npartitions=i).groupby("orbit").apply(multiple_layers,meta=metadata).reset_index(drop=True).compute()
    stop = time.time()
    dur = stop - start
    duration.append(stop-start)
    
    print("nrepartitions=",i,"duration=",dur,"s")
86/62: hist(n,dur)
86/63: hist(n,dur)
86/64: plt.hist(n,dur)
86/65: plt.hist(dur)
86/66:
# plt.hist(dur)
n
86/67: plt.hist(dur,n)
86/68: plt.plot(n,dur)
86/69: plt.hist(n,duration)
86/70: plt.plot(n,duration)
86/71: plt.plot(n,duration);
86/72:
plt.plot(n,duration);
plt.xlabel("npartitions")
86/73:
plt.plot(n,duration);
plt.xlabel("Npartitions");
plt.ylabel("Duration in seconds")
86/74:
plt.plot(n,duration);
plt.xlabel("Npartitions");
plt.ylabel("Duration in seconds");
86/75:
plt.plot(n,duration, col="r");
plt.xlabel("Npartitions");
plt.ylabel("Duration in seconds");
86/76:
plt.plot(n,duration,"r");
plt.xlabel("Npartitions");
plt.ylabel("Duration in seconds");
86/77:
plt.plot(n,duration,"db");
plt.xlabel("Npartitions");
plt.ylabel("Duration in seconds");
86/78:
# necessariamente in questo caso bisogna inizialmente settare gli indici del dask dataframe uguali alle orbite in modo che l'assegnazione
# della nuova colonna t0 tramite la series s che ha come indici proprio le orbite vada a buon fine. Dask  molto pi schizzinoso di pandas 
# e quando gli indici di due series non corrispondono allora restituisce un errore mentre pandas ne fa un'inferenza vedendo a quale colonna 
# somigliano gli indici di s e cio le orbit, assegnando dunque alla colonna t0 righe in base a quali siano le loro orbite e non per forza gli
# indici

s = d5.loc[d5['chan'] == 128, ['orbit', 'time']].set_index('orbit')
# d5['t0'] = s.loc[d5.orbit].time.to_numpy()
# d5.compute()
# s.loc[d5.orbit].time.to_numpy()
# s.compute()
# d5['t0'] = s.loc[d5.set_index('orbit').index].time
d5 = d5.set_index('orbit')
d5['t0'] = s.loc[d5.index].time
86/79:
# poich ci interessa tenere le orbite come una colonna del dataframe distinta resetto quanto fatto prima mantenendo per la nuova colonna t0 
d5 = d5.reset_index()
86/80:
# rimuovo gli scintillatori, che non possono essere piazzati in nessuna colonna in particolare
# NON SOLO QUELLI! Ci sono canali spuri ancora al di sopra
#df2.loc[df2['chan'] == 128] = None  # non basta
d5 = d5[d5['chan'] < 128]
86/81:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks=[((d6["fpga"]==0)&(d6["chan"]<64)),
       ((d6["fpga"]==0)&(d6["chan"]>63)),
       ((d6["fpga"]==1)&(d6["chan"]<64)),
       ((d6["fpga"]==1)&(d6["chan"]>63))]

for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
86/82:
m = [*(np.fromfile(file, dtype = '<u8') for file in data_names1)]
fin = []
for mi in m:   
    mat = np.zeros((mi.shape[0], 6), dtype = "u4")
    mat[:,0] = mi & 31 # tdc, 0-4 (5)
    mat[:,1] = (mi & 131040) >> 5 # bx, 5-16 (12)
    mat[:,2] = (mi & 562949953290240) >> 17 # orbit, 17-48 (32)
    mat[:,3] = (mi & 287667426198290432) >> 49 # chan, 49-57 (9)
    mat[:,4] = (mi & 2017612633061982208) >> 58 # fpga (3)
    mat[:,5] = mi >> 61
    fin.append(mat)
86/83: fin = np.concatenate(fin)
86/84: df = pd.DataFrame(fin, columns = ['tdc', 'bx', 'orbit', 'chan', 'fpga', 'head'])
86/85: tipi = {'tdc':'u1', 'bx':'u2', 'orbit':'u4', 'chan':'u2', 'fpga':'u1', 'head':'u1'}
86/86: df = df[(df['head']==2)]
86/87:
%%time 

def cut(group): 
    if group.shape[0] < 15 and group.shape[0] > 3 : return group 
    else : return None 
    
df1=df.groupby(['orbit'],group_keys=False)\
                 .apply(cut)\
                 .dropna()\
                 .reset_index()\
                 .drop(['index'],axis=1)
86/88:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks=[((d6["fpga"]==0)&(d6["chan"]<64)),
       ((d6["fpga"]==0)&(d6["chan"]>63)),
       ((d6["fpga"]==1)&(d6["chan"]<64)),
       ((d6["fpga"]==1)&(d6["chan"]>63))]

for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
86/89: d6
86/90: d6.compute()
86/91: d6
86/92:
#add layer column
#contando dal basso per partendo da uno non da zero 
d6["layer"]=d6["chan"]%4
d6['layer'].replace({0:3, 3:0}, inplace = True) # 2:2, 1:1
d6['layer'] += 1
#add cell column
df3["cell"]=df3["chan"]%64
86/93:
#add layer column
#contando dal basso per partendo da uno non da zero 
d6["layer"]=d6["chan"]%4
d6['layer'].replace({0:3, 3:0}) # 2:2, 1:1
d6['layer'] += 1
#add cell column
df3["cell"]=df3["chan"]%64
86/94:
#add layer column
#contando dal basso per partendo da uno non da zero 
d6["layer"]=d6["chan"]%4
d6['layer'].replace({0:3, 3:0}) # 2:2, 1:1
d6['layer'] += 1
#add cell column
d6["cell"]=d6["chan"]%64
86/95: d6.compute()
86/96: len(df1)/len(df)
86/97:
indici = ((df1['fpga'] == 1) & (df1['chan'] == 128))[((df1['fpga'] == 1) & (df1['chan'] == 128))==True].index
# questi sono gli indici delle orbite di ciascun gruppo in cui un evento  considerato accettabile
86/98:
len(indici)
#  diverso da len(df3)
86/99: orbite_buone = df1.orbit[indici].values
86/100:
# adesso devo considerare tutte le operazione che far sui gruppi di orbite in quanto sono gli eventi che stiamo considerando. Dunque i 
# successivi filtri servono per filtrare i gruppi di hits non le singole righe del dataframe 

fpga1_chan1 = orbite_buone
# queste sono le orbits corrispondenti a fpga = 1 quindi se faccio .index e poi .isin dovrei aver filtrato il 
# dataframe. Visto che le orbite per ogni gruppo sono uguale praticamente sto selezionando quei gruppi che hanno fpga == 1

df3 = df1[df1['orbit'].isin(fpga1_chan1)]
86/101:
df3._is_copy = None 
# trucco ihih perch df3  una copia di df1 e lui ne tiene conto in questo modo segnalando warning
86/102:
# adesso aggiungo le colonne del tempo e delle camere:

# 25  (ORBIT  3564 + BX + TDC/30) per il tempo 
df3['t'] = 25*df3['bx'] + 25*df3['tdc']/30
# df3['time'] = 25 * ((df3['orbit'] * 3564) + df3['bx'] + (df3['tdc']/30))
86/103:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna().astype(tipi)
df3
86/104: df3
86/105:
# rimuovo le orbite con pi scintillatori
v1, v2 = np.unique(df3.loc[df3.chan == 128, 'orbit'], return_counts = True)
df3.loc[df3.orbit.isin(v1[v2 > 1])] = None
df3 = df3.dropna()
df3
86/106:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()

df3
86/107:
df3._is_copy = None 
# trucco ihih perch df3  una copia di df1 e lui ne tiene conto in questo modo segnalando warning
86/108:
s = df3.loc[df3['chan'] == 128, ['orbit', 't']].set_index('orbit')
df3['t0'] = s.loc[df3.orbit].t.to_numpy()

df3
86/109:
# rimuovo gli scintillatori, che non possono essere piazzati in nessuna colonna in particolare
# NON SOLO QUELLI! Ci sono canali spuri ancora al di sopra
#df2.loc[df2['chan'] == 128] = None  # non basta
df3.loc[df3['chan'] > 127] = None 
df3.dropna(inplace = True)
df3
86/110:
# assegno la camera
df3['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
df3 = df3.astype(tipi2)

df3.loc[(df3['fpga'] == 0) & (df3['chan'] < 64), 'chamber'] = 0
df3.loc[(df3['fpga'] == 0) & (df3['chan'] > 63), 'chamber'] = 1
df3.loc[(df3['fpga'] == 1) & (df3['chan'] < 64), 'chamber'] = 2
df3.loc[(df3['fpga'] == 1) & (df3['chan'] > 63), 'chamber'] = 3

df3
86/111:
#add layer column
#contando dal basso per partendo da uno non da zero 
df3["layer"]=df3["chan"]%4
df3['layer'].replace({0:3, 3:0}, inplace = True) # 2:2, 1:1
df3['layer'] += 1
#add cell column
df3["cell"]=df3["chan"]%64
86/112: df3
86/113:
#add layer column
#contando dal basso per partendo da uno non da zero 
df3["layer"]=df3["chan"]%4
df3['layer'].replace({0:3, 3:0}, inplace = True) # 2:2, 1:1
df3['layer'] += 1
#add cell column
df3["cell"]=df3["chan"]%64
df3
86/114:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

df4=df3.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
86/115: df4
86/116:
df4.loc[df4["chan"] > 63,"chan"] -= 64

f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
df4["column"] = ((df4['chan'] + f(df4["layer"])) / 4).astype(np.uint8)
df4
86/117:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
df4["center"] = centers_coords[df4.layer-1,df4.column-1] 
df4
86/118:
time_offset = np.array([-1.1, 6.4, 0.5, 2.6])

df4['dt'] = df4['t'] - df4['t0'] + 95 + time_offset[df4['layer'] - 1]
df4
86/119:
# 42/2 = 21
vd = 53.8*1e-3 # v_drift = 53.8 um/ns, voglio lavorare con i millimetri
df4['xr'] = df4['center']*21 + vd*df4['dt']
df4['xl'] = df4['center']*21 - vd*df4['dt']

z_offset = np.array([219.8, 977.3, 1035.6, 1819.8])
dz2 = 13/2

df4['z'] = df4['layer']*dz2 + z_offset[df4['layer'] - 1]
df4
86/120:
@njit 
def numba_score(combin,eventZ) :
    s = np.zeros(combin.shape[0]) 
    for i, c in enumerate(combin) :
        if ((eventZ - eventZ.mean())**2).sum() == 0 : # dice che se no ci sono delle divisioni per zero anche se non riesco a riprodurle a mano... assurdo
            s[i] = 0 #np.nan
        else :
            slope = ((eventZ - eventZ.mean())*(c - c.mean())).sum() / ((eventZ - eventZ.mean())**2).sum()
            intercept = c.mean() - slope*eventZ.mean() # cfr https://en.wikipedia.org/wiki/Simple_linear_regression con x e y scambiati

            s[i] = np.linalg.norm(c - (slope*eventZ+intercept))
    return s

def solve_ambiguity(event):
    combin = np.array(list(product(*event.loc[:, ['xr','xl']].to_numpy())))
    s = numba_score(combin,event.z.to_numpy())
    event['xb'] = combin[s.argmin()]
    return event
86/121:
df5 = df4.copy()
df5 = df5.groupby(['orbit', 'chamber']).apply(solve_ambiguity)
df5
86/122: data = df5.copy().reset_index(drop=True)
86/123:
#function to draw the activated cells
def boxes(ax,x,y):
    
    dx = 1
    dy = 1/2
     
    for i in range(len(x)) :
        rect = patches.Rectangle((x[i]-dx/2,y[i]-dy/2),dx,dy,linewidth=3,edgecolor='r',facecolor='none')
        ax.add_patch(rect)
86/124:
#function to draw the detector
def draw_detector(ax) :
    
    dx = 1
    dy = 1/2
    gap=4

    #adjust
    ax.set_xlim(0,16.5)
    ax.set_ylim(-0.2,15.2)

    #coordinates of layer 0
    x_1 = np.arange(1,16)-dx/2
    y_1 = 0
    
    #coordinates of layer 2
    x_2 = x_1 + dx/2
    y_2 = dy
    
    #coordinates of layer 3
    x_3 = x_1 
    y_3 = 2*dy
    
    #coordinates of layer 4
    x_4 = x_2
    y_4 = 3*dy
    
    for j in range(4):
        
        for i in range(15): 
            rect = patches.Rectangle((x_1[i],y_1),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_2[i],y_2),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_3[i],y_3),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            rect = patches.Rectangle((x_4[i],y_4),dx,dy,linewidth=1,edgecolor='black',facecolor='none')
            ax.add_patch(rect)

            ax.axis('off')
        
        #increment y chamber
        y_1+=gap
        y_2+=gap
        y_3+=gap
        y_4+=gap
86/125:
events=[value for value in data.groupby(["orbit"]).groups.values()]
event=[data.loc[events[i]]  for i in np.random.randint(low=0,high=len(events),size=6)] #select 6 random events to show
event[0]
86/126:
fig,ax = plt.subplots(nrows=len(event),ncols=1,figsize=(20,60))

#draw selected events
for i,ev in enumerate(event):

    global_event=ev[ev["chan"]<=128].sort_values(by="chamber").reset_index(drop=True)
    local=global_event.groupby("chamber")
    local_events=[local.get_group(x) for x in local.groups]

    #draw events
    dx = 1
    dy = 1/2
    gap=4

    ax[i].set_title("Global Event %d" %i,fontsize=30)
    draw_detector(ax[i])

    for j in range(len(local_events)):

        single=local_events[j].reset_index(drop=True)#.drop(["level_0","index"],axis=1)
        boxes(ax[i],centres(single)[0],centres(single)[1])
        ax[i].errorbar(*centres(single),dy/2,dx/2,fmt='o')

    ax[i].text(x=7.5,y=5*dy+3*gap,s="Chamber 3",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+2*gap,s="Chamber 2",fontsize=20)
    ax[i].text(x=7.5,y=5*dy+gap,s="Chamber 1",fontsize=20)
    ax[i].text(x=7.5,y=5*dy,s="Chamber 0",fontsize=20)

plt.show()
86/127: d6.compute()
86/128:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

d7 = d6.groupby("orbit").apply(multiple_layers).reset_index(drop=True)
86/129:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group

d7 = d6.groupby("orbit").apply(multiple_layers, meta=metadata).reset_index(drop=True)
86/130: d7
86/131: d7.compute()
86/132: metadata
86/133:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
metadata['t0'] = 'u4'
d7 = d6.groupby("orbit").apply(multiple_layers, meta=metadata).reset_index(drop=True)
86/134: d7
86/135:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/136: d7.compute()
86/137: d6
86/138:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('tdc', 'u1'), ('bx', 'u2'), ('orbit', 'u4'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/139: d7
86/140: d7.compute()
86/141: d6
86/142:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/143: d7.compute()
86/144:
d7.loc[d7["chan"] > 63,"chan"] -= 64

f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/145:
d7[d7["chan"][(d7["chan"] > 63).compute()] -= 64

f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/146:
d7["chan"][(d7["chan"] > 63).compute()] -= 64


f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/147:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7['chan'] > 63 
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/148:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63).copmute()
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/149:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63).compute()
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/150:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63).values8
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/151:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63).values
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/152:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63).values()
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/153:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63)
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/154:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
(d7['chan'] > 63).compute()
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/155:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
np.array((d7['chan'] > 63).compute())
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/156:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7['chan'][np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/157:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7['chan'].compute()
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/158:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7[np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/159:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7[(d7['chan'] > 63)]
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/160:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7[(d7['chan'] > 63)]['chan']
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/161:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7[(d7['chan'] > 63)]['chan'].compute()
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/162:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
d7[(d7['chan'] > 63)]['chan'].compute() -= 64
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/163:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = d7[(d7['chan'] > 63)]['chan'].compute() -= 64
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/164:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = d7[(d7['chan'] > 63)]['chan'].compute() - 64
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/165:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = d7[(d7['chan'] > 63)]['chan'].compute() - 64
# channel diventa la nuova colonna 'chen' del dataframe d7

d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/166:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = d7[(d7['chan'] > 63)]['chan'].compute() - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
channel
# d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/167:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = (d7[(d7['chan'] > 63)]['chan'].compute() - 64).values()
# channel diventa la nuova colonna 'chen' del dataframe d7
channel
# d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/168:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = np.array(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
channel
# d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/169:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = np.array(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/170:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = list(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/171:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.series(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/172:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.serie(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/173:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.Serie(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/174:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.Series(d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/175:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.Series('chan',meta='u1',d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/176:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64)
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/177:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1')
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/178: d7
86/179: rep(2)
86/180: np.repeat(None,13)
86/181:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/182:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'].compute() - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
channel
# d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/183:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'].compute() - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
type(channel)
# d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/184:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
channel
# d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/185:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
86/186:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
# channel
d7['chan'] = channel
# [np.array((d7['chan'] > 63).compute())]
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
d7.compute()
86/187:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
d7.compute()
86/188:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
f(d7["layer"])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/189:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
len(f(d7["layer"]))
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/190:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
type(f(d7["layer"]))
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/191:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
f(d7["layer"]) + d7['chan']
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/192:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
d7['layer'].apply(f)
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/193:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                           ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                           ('layer', 'u1'), ('cell', 'u1')])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/194:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                           ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                           ('layer', 'u1'), ('cell', 'u1')])
temp + d7['chan']
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/195:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                           ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                           ('layer', 'u1'), ('cell', 'u1')])
(temp + d7['chan'])/4
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/196:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                           ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                           ('layer', 'u1'), ('cell', 'u1')])
((temp + d7['chan'])/4).compute()
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/197:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                           ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                           ('layer', 'u1'), ('cell', 'u1')])
temp.compute()
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/198:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])
temp.compute()
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/199:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])
f(d7['layer'])
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/200:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64 # questa  ancora un promessa
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])
f(d7['layer']).delayed()
# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/201: type(channel)
86/202: dd.Series(f(d7['layer']))
86/203: dd.from_array(f(d7['layer']))
86/204: dd.from_array(f(d7['layer'])).compute()
86/205: (dd.from_array(f(d7['layer'])) + d7['chan']).compute()
86/206: d7['chan'].compute()
86/207:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64 # questa  ancora un promessa
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/208: d7['chan'].compute()
86/209: d7.compute()
86/210: channel.compute()
86/211:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/212: d7.compute()
86/213:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64 # questa  ancora un promessa
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)
# d7.compute()
86/214: d7.compute()
86/215: channel.compute()
86/216: channel.index()
86/217: channel.index
86/218: channel.index.compute()
86/219: d7['chan'][channel.index.compute()]
86/220: d7.loc[channel.index,'chan']
86/221: d7.loc[channel.index,'chan'] = channel
86/222: d7.loc['chan'][channel.index]
86/223: d7['chan'][channel.index]
86/224: d7['chan'][channel.index] = channel
86/225: d7['chan'][channel.index].compute()
86/226:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/227: d7.compute()
86/228: d7['chan'][channel.index]
86/229: d7['chan'][channel.index].compute()
86/230: d7['chan'][channel.index] = channel
86/231: d7['chan'][channel.index].compute()
86/232: d7['chan'][channel.index] = channel
86/233: d7['chan'][channel.index.compute()] = channel
86/234: d7['chan'][channel.index.compute()]
86/235: d7['chan'][channel.index]
86/236: d7['chan'][channel.index]
86/237: d7['chan'][channel.index].compute()
86/238: d7['chan'][channel.index] = channel.compute()
86/239: d7['chan'][channel.index]
86/240: channel
86/241: channel.compute()
86/242: channel.values()
86/243: channel.values
86/244: channel
86/245: channel.compute()
86/246: channel.compute().values
86/247: dd.from_array(channel.compute().values)
86/248: d7['chan'][channel.index] = dd.from_array(channel.compute().values)
86/249:
def traslazione_chan(x):
    if x > 63: x -= 64
d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])
86/250: d7.compute()
86/251:
def traslazione_chan(x):
    if x > 63: x -= 64
d7['chan'] = d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])
86/252: d7.compute()
86/253:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7['chan'] > 63
86/254:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)
86/255:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7.mask((d7['chan'] > 63), other = d7['chan'] - 64, axis=1)
86/256:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)
86/257:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7.mask((d7['chan'] > 63), other = d7['chan'] - 64, meta =[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
                                                           ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), 
                                                           ('cell', 'u1')])
86/258:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)
86/259:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

d7['chan'] > 63
86/260:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

d7[d7['chan'] > 63]
86/261:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/262: d7.compute()
86/263: d7[d7['chan'] > 63]['chan'].compute()
86/264: d7[d7['chan'] > 63]['chan'] - 64.compute()
86/265: (d7[d7['chan'] > 63]['chan'] - 64).compute()
86/266:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?

d7.assign(d7['chan'][d7['chan'] > 63] = channel)
86/267:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?

d7.assign(d7['chan'][d7['chan'] > 63] == channel)
86/268:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?

d7.assign(d7['chan'][d7['chan'] > 63] = channel)
86/269:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?

d7.assign(d7[d7['chan'] > 63]['chan'] = channel)
86/270:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?

d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/271:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?
d7.replace()
# d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/272:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?
d7.replace(d7[d7['chan'] > 63]['chan'], channel)
# d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/273:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?
d7.replace(d7[d7['chan'] > 63]['chan'], channel).compute()
# d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/274: d7.compute()
86/275:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?
d7.replace(to_replace = d7[d7['chan'] > 63]['chan'], value = channel)
# d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/276:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?
d7.replace(to_replace = d7[d7['chan'] > 63]['chan'], value = channel).compute()
# d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/277: channel
86/278: channel.compute()
86/279:
(d7[d7['chan'] > 63]['chan'] - 64).compute()
# posso usare replace anzich fare index assigning con una series?
d7.replace(to_replace = d7[d7['chan'] > 63]['chan'], value = d7[d7['chan'] > 63]['chan'] - 64).compute()
# d7.assign((d7[d7['chan'] > 63]['chan']) = channel)
86/280:
# d7["chan"][(d7["chan"] > 63).compute()] -= 64
# in pratica devo scalare i canali maggiori di 64 di 64 
# channel = dd.Series('chan',d7[(d7['chan'] > 63)]['chan'].compute() - 64, meta='u1', divisions = np.repeat(None,13))
channel = d7[(d7['chan'] > 63)]['chan'] - 64 
# questa  ancora un promessa ma non funziona perch alla fine sostituisco l'intera colonna di channel con questi valori che per non sono tutti
# channel diventa la nuova colonna 'chen' del dataframe d7
d7['chan'] = channel
# f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()
86/281: d7.compute()
86/282: channel
86/283: channel.compute()
86/284: len(channel.compute())
86/285: channel.compute()
86/286:
#drop events with multiple hits on the same layer
def multiple_layers(group):
    
    #obtain all ordered (chamber,layer) pairs
    pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
    if pairs.duplicated().any(): return None
    else: return group
    
d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/287: d7.compute()
86/288:
def traslazione_chan(x):
    if x > 63: x -= 64
d7.apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
                                   ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/289:
def traslazione_chan(x):
    if x > 63: x -= 64
d7.apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
                                   ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')], axis=1)
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/290: d7.compute()
86/291: d7.loc[d7.chan > 63, 'chan'] -= 64
86/292: d7['chan'][d7.chan > 63] -= 64
86/293: d7['chan'][d7.chan > 63] = d7['chan'][d7.chan > 63] - 64
86/294:
def traslazione_chan(x):
    if x > 63: x -= 64
d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
                                   ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')], axis=1)
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

# d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/295:
def traslazione_chan(x):
    if x > 63: x -= 64
d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
                                   ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')], axis=1).compute()
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

# d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/296:
def traslazione_chan(x):
    if x > 63: x -= 64
d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
                                   ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()
# d7 = d7.groupby('chan').apply(traslazione_chan, meta = 
# [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
# ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7.mask((d7['chan'] > 63), other = d7['chan'] - 64)

# d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/297:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()

d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64)

# d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/298:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()

d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64).compute()

# d7[d7['chan'] > 63]['chan'] = d7[d7['chan'] > 63]['chan'] - 64
86/299: d7.compute()
86/300:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()

d78['chan'] = d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64)
86/301:
# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()

d7['chan'] = d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64)
86/302: d7.compute()
86/303:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()
86/304: f(d7["layer"])
86/305: dd.from_array(f(d7["layer"]))
86/306:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()



# maschera per layer 2 
d7['layer'] == 2
86/307:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()


d7["column"] = ((d7['chan'] + d7["layer"]) / 4)

# maschera per layer 2 
d7['column'] = d7.mask(d7['layer'] == 2, other = (3 + d7['chan'])/4 )
d7['column'] = d7.mask(d7['layer'] == 3, other = (2 + d7['chan'])/4 )
86/308: d7
86/309:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()


d7["column"] = ((d7['chan'] + d7["layer"]) / 4)

# maschera per layer 2 
d7['column'] = d7['column'].mask(d7['layer'] == 2, other = (3 + d7['chan'])/4 )
d7['column'] = d7['column'].mask(d7['layer'] == 3, other = (2 + d7['chan'])/4 )
86/310: d7
86/311: d7.compute()
86/312: d7['column'] = d7['column'].astype('u1')
86/313: d7.compute()
86/314: d6.compute()
86/315:
# non funziona niente qua sopra, facciamo alla vecchia maniera
def stesso_layer(gruppo) :
    if gruppo['layer'].nunique() == len(gruppo) :
        return gruppo
    else :
        return None 

d7 = d6.groupby(['orbit', 'chamber']).apply(stesso_layer, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).dropna().reset_index(drop = True)
d7.compute()

# #drop events with multiple hits on the same layer
# def multiple_layers(group):
    
#     #obtain all ordered (chamber,layer) pairs
#     pairs=pd.Series([(a,b) for a,b in zip(group["chamber"],group["layer"])]) #might be slow
    
#     if pairs.duplicated().any(): return None
#     else: return group
    
# d7 = d6.groupby("orbit").apply(multiple_layers, meta= [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
#                                                        ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
#                                                        ('layer', 'u1'), ('cell', 'u1')]).reset_index(drop=True)
86/316:
# non funziona niente qua sopra, facciamo alla vecchia maniera
def stesso_layer(gruppo) :
    if gruppo['layer'].nunique() == len(gruppo) :
        return gruppo
    else :
        return None 

df4 = df3.groupby(['orbit', 'chamber']).apply(stesso_layer).dropna().reset_index(drop = True)
86/317: df4
86/318:
# TRASLAZIONE DELLA COLONNA DEI CHAN DI 64 PER QUELLI CHE SONO > 63

# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()
d7['chan'] = d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64)
d7.compute()
86/319:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()


d7["column"] = ((d7['chan'] + d7["layer"]) / 4)

# maschera per layer 2 
d7['column'] = d7['column'].mask(d7['layer'] == 2, other = (3 + d7['chan'])/4 )
d7['column'] = d7['column'].mask(d7['layer'] == 3, other = (2 + d7['chan'])/4 )

d7['column'] = d7['column'].astype('u1')
86/320: d7.compute()
86/321:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
d8["center"] = centers_coords[d7.layer-1,d7.column-1] 
d8
86/322:
# non funziona niente qua sopra, facciamo alla vecchia maniera
def stesso_layer(gruppo) :
    if gruppo['layer'].nunique() == len(gruppo) :
        return gruppo
    else :
        return None 

d7 = d6.groupby(['orbit', 'chamber']).apply(stesso_layer, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).dropna().reset_index(drop = True)
d7.compute()
86/323: dcopia = d7[['layer','cell']].copy()
86/324: dcopia.compute()
86/325: dcopia[dcopia['layer'] == 2]
86/326: dcopia[dcopia['layer'] == 2].compute()
86/327: dcopia = d7[['layer','chan']].copy()
86/328: dcopia[dcopia['layer'] == 2].compute()
86/329: (dcopia[dcopia['layer'] == 2].compute()['chan'] + 3) % 4
86/330: np.count_nonzero(((dcopia[dcopia['layer'] == 2].compute()['chan'] + 3) % 4)==0) == 1376
86/331:
# TRASLAZIONE DELLA COLONNA DEI CHAN DI 64 PER QUELLI CHE SONO > 63

# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()
d7['chan'] = d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64)
d7.compute()
86/332:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()


d7["column"] = ((d7['chan'] + d7["layer"]) / 4)

# maschera per layer 2 
d7['column'] = d7['column'].mask(d7['layer'] == 2, other = (3 + d7['chan'])/4 )
d7['column'] = d7['column'].mask(d7['layer'] == 3, other = (2 + d7['chan'])/4 )

# d7['column'] = d7['column'].astype('u1')
86/333: d7.compute()
86/334:
# assegno la camera
d5['chamber'] = 23 # un numero a caso che per non  None (che non "entra" in un intero)
tipi2 = tipi
tipi2['chamber'] = 'u1'
d6 = d5.astype(tipi2)
# poich le assegnazioni con gli oggetti di dask non si possono fare se come indici non sono forniti array e non dask promises costruisco una 
# lista di array booleani che rappresentano le condizioni per attribuire alle opportune righe le giuste chambers
masks=[((d6["fpga"]==0)&(d6["chan"]<64)),
       ((d6["fpga"]==0)&(d6["chan"]>63)),
       ((d6["fpga"]==1)&(d6["chan"]<64)),
       ((d6["fpga"]==1)&(d6["chan"]>63))]

for i,mask in enumerate(masks):
    d6["chamber"]=d6["chamber"].mask(mask,i)
86/335: d6.compute()
86/336:
#add layer column
#contando dal basso per partendo da uno non da zero 
d6["layer"]=d6["chan"]%4
d6['layer'] = d6['layer'].replace({0:3, 3:0}) # 2:2, 1:1
d6['layer'] += 1
# #add cell column
# d6["cell"]=d6["chan"]%64

#SAPPIAMO CHE QUESTA COLONNA CELL E' INUTILE
86/337: d6.compute()
86/338:
# non funziona niente qua sopra, facciamo alla vecchia maniera
def stesso_layer(gruppo) :
    if gruppo['layer'].nunique() == len(gruppo) :
        return gruppo
    else :
        return None 

d7 = d6.groupby(['orbit', 'chamber']).apply(stesso_layer, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1'), ('cell', 'u1')]).dropna().reset_index(drop = True)
d7.compute()
86/339:
# non funziona niente qua sopra, facciamo alla vecchia maniera
def stesso_layer(gruppo) :
    if gruppo['layer'].nunique() == len(gruppo) :
        return gruppo
    else :
        return None 

d7 = d6.groupby(['orbit', 'chamber']).apply(stesso_layer, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                       ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), 
                                                       ('layer', 'u1')]).dropna().reset_index(drop = True)
d7.compute()
86/340:
# TRASLAZIONE DELLA COLONNA DEI CHAN DI 64 PER QUELLI CHE SONO > 63

# def traslazione_chan(x):
#     if x > 63: x -= 64
# d7['chan'].apply(traslazione_chan, meta = [('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), 
#                                    ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')]).compute()
d7['chan'] = d7['chan'].mask((d7['chan'] > 63), other = d7['chan'] - 64)
d7.compute()
86/341:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()


d7["column"] = ((d7['chan'] + d7["layer"]) / 4)

# maschera per layer 2 
d7['column'] = d7['column'].mask(d7['layer'] == 2, other = (3 + d7['chan'])/4 )
d7['column'] = d7['column'].mask(d7['layer'] == 3, other = (2 + d7['chan'])/4 )

# d7['column'] = d7['column'].astype('u1')
86/342: d7.compute()
86/343:
f = BarycentricInterpolator([1,2,3,4],[1,3,2,4])
# temp = d7['layer'].apply(f, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), ('fpga', 'u1'), ('head', 'u1'), ('time', 'u2'), ('t0', 'u2'), ('chamber', 'u1'), ('layer', 'u1'), ('cell', 'u1')])

# d7["column"] = ((d7['chan'] + f(d7["layer"])) / 4)

# d7.compute()


d7["column"] = ((d7['chan'] + d7["layer"]) / 4)

# maschera per layer 2 
d7['column'] = d7['column'].mask(d7['layer'] == 2, other = (3 + d7['chan'])/4 )
d7['column'] = d7['column'].mask(d7['layer'] == 3, other = (2 + d7['chan'])/4 )

d7['column'] = d7['column'].astype('u1')
86/344: d7.compute()
86/345: max(d7['layer'])
86/346: d7['column'].max()
86/347: d7['column'].max().compute()
86/348:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
d8["center"] = centers_coords[d7.layer-1,d7.column-1] 
d8
86/349: d8 = d7.copy()
86/350:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
d8["center"] = centers_coords[d7.layer-1,d7.column-1] 
d8
86/351:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
d8["center"] = dd.from_array(centers_coords[d7.layer-1,d7.column-1])
d8.compute()
86/352:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
# d8["center"] = centers_coords[d7.layer-1,d7.column-1]
d8['center'] = d7['column']*2
d8['center'] = d7['center'].mask(d7['layer'] == 1, other = d7['center'] - 1)
d8['center'] = d7['center'].mask(d7['layer'] == 3, other = d7['center'] - 1)
d8.compute()
86/353:
d8 = d7.copy()
d8['center'] = 2
86/354:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
# d8["center"] = centers_coords[d7.layer-1,d7.column-1]
d8['center'] = d7['column']*2
d8['center'] = d7['center'].mask(d7['layer'] == 1, other = d7['center'] - 1)
d8['center'] = d7['center'].mask(d7['layer'] == 3, other = d7['center'] - 1)
d8.compute()
86/355:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
# d8["center"] = centers_coords[d7.layer-1,d7.column-1]
d8['center'] = d8['column']*2
d8['center'] = d8['center'].mask(d8['layer'] == 1, other = d8['center'] - 1)
d8['center'] = d8['center'].mask(d8['layer'] == 3, other = d8['center'] - 1)
d8.compute()
86/356: d8 = d7.copy()
86/357:
centers_coords = np.vstack((np.arange(2,34,2),np.arange(1,33,2))) # ok. invertito  giusto come deve essere, ho fatto un check con le prime righe/colonne
centers_coords = np.vstack((centers_coords,centers_coords))
# d8["center"] = centers_coords[d7.layer-1,d7.column-1]
d8['center'] = d8['column']*2
d8['center'] = d8['center'].mask(d8['layer'] == 1, other = d8['center'] - 1)
d8['center'] = d8['center'].mask(d8['layer'] == 3, other = d8['center'] - 1)
d8.compute()
86/358:
time_offset = np.array([-1.1, 6.4, 0.5, -2.6])

d8['dt'] = d8['time'] - d8['t0'] + 95 #+ dd.from_array(time_offset[d8['layer'] - 1])

for i,to in enumerate(time_offset):
    d8['dt'] = d8['dt'].mask(d8['layer'] == i+1, other = d8['dt'] + to) 
    
# d8['dt'] = d8['dt'].mask(d8['layer'] == 1, other = d8['dt'] - 1.1) 
# d8['dt'] = d8['dt'].mask(d8['layer'] == 2, other = d8['dt'] + 6.4) 
# d8['dt'] = d8['dt'].mask(d8['layer'] == 3, other = d8['dt'] - 1.1) 
# d8['dt'] = d8['dt'].mask(d8['layer'] == 4, other = d8['dt'] - 1.1) 

# 42/2 = 21
# vd = 53.8*1e-3 # v_drift = 53.8 um/ns, voglio lavorare con i millimetri
# d8['xr'] = d8['center']*21 + vd*d8['dt']
# d8['xl'] = d8['center']*21 - vd*d8['dt']

# z_offset = np.array([219.8, 977.3, 1035.6, 1819.8])
# dz2 = 13/2

# d8['z'] = d8['layer']*d8 + dd.from_array(z_offset[d8['layer'] - 1])
86/359: d8.compute()
86/360:
time_offset = np.array([-1.1, 6.4, 0.5, -2.6])

d8['dt'] = d8['time'] - d8['t0'] + 95 #+ dd.from_array(time_offset[d8['layer'] - 1])

for i,to in enumerate(time_offset):
    d8['dt'] = d8['dt'].mask(d8['chamber'] == i, other = d8['dt'] + to) 
    
# d8['dt'] = d8['dt'].mask(d8['chamber'] == 1, other = d8['dt'] - 1.1) 
# d8['dt'] = d8['dt'].mask(d8['chamber'] == 2, other = d8['dt'] + 6.4) 
# d8['dt'] = d8['dt'].mask(d8['chamber'] == 3, other = d8['dt'] - 1.1) 
# d8['dt'] = d8['dt'].mask(d8['chamber'] == 4, other = d8['dt'] - 1.1) 

# 42/2 = 21
vd = 53.8*1e-3 # v_drift = 53.8 um/ns, voglio lavorare con i millimetri
d8['xr'] = d8['center']*21 + vd*d8['dt']
d8['xl'] = d8['center']*21 - vd*d8['dt']

z_offset = np.array([219.8, 977.3, 1035.6, 1819.8])
dz2 = 13/2

d8['z'] = d8['layer']*dz2 

for i,zo in enumerate(z_offset):
    d8['z'] = d8['z'].mask(d8['chamber'] == i, other = d8['z'] + zo) 
    
# d8['z'] = d8['layer']*dz2 + dd.from_array(z_offset[d8['chamber'])
86/361: d8.compute()
86/362:
@njit 
def numba_score(combin,eventZ) :
    s = np.zeros(combin.shape[0]) 
    for i, c in enumerate(combin) :
        if ((eventZ - eventZ.mean())**2).sum() == 0 : # dice che se no ci sono delle divisioni per zero anche se non riesco a riprodurle a mano... assurdo
            s[i] = 0 #np.nan
        else :
            slope = ((eventZ - eventZ.mean())*(c - c.mean())).sum() / ((eventZ - eventZ.mean())**2).sum()
            intercept = c.mean() - slope*eventZ.mean() # cfr https://en.wikipedia.org/wiki/Simple_linear_regression con x e y scambiati

            s[i] = np.linalg.norm(c - (slope*eventZ+intercept))
    return s

def solve_ambiguity(event):
    combin = np.array(list(product(*event.loc[:, ['xr','xl']].to_numpy())))
    s = numba_score(combin,event.z.to_numpy())
    event['xb'] = combin[s.argmin()]
    return event
86/363: d8.compute()
86/364:
d9 = d8.copy()
d9 = d9.groupby(['orbit', 'chamber']).apply(solve_ambiguity, meta=[('orbit', 'u4'), ('tdc', 'u1'), ('bx', 'u2'), ('chan', 'u2'), 
                                                                   ('fpga', 'u1'), ('head', 'u1'), ('time', 'f8'), ('t0', 'f8'), 
                                                                   ('chamber', 'u1'), ('layer', 'u1'), ('column', 'u1'),
                                                                   ('center', 'u1'), ('dt', 'f8'), ('xr', 'f8'), ('xl', 'f8'),
                                                                   ('z', 'f8'), ('xb', 'f8')])

d9.compute()
86/365:
df8 = d9.compute()
n = 0 # la orbita che grafichiamo con il codice qua sotto

vxr = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').xr.to_numpy().reshape((-1, 4))
vxl = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').xl.to_numpy().reshape((-1, 4))
vxb = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').xb.to_numpy().reshape((-1, 4))
vz = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').z.to_numpy().reshape((-1, 4))
for i in range(3) :
    fig, ax = plt.subplots()
    ax.scatter(vxl[i], vz[i], c = 'blue')
    ax.scatter(vxr[i], vz[i], c = 'red')
    ax.scatter(vxb[i], vz[i], c = 'black')
# blu xl, rosso xr, nero xb, mi secco a fare una legenda solo per questo grrrrr
86/366: df8 = d9.compute()
86/367:
n = 0 # la orbita che grafichiamo con il codice qua sotto

vxr = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').xr.to_numpy()#.reshape((-1, 4))
# vxl = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').xl.to_numpy().reshape((-1, 4))
# vxb = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').xb.to_numpy().reshape((-1, 4))
# vz = df8.loc[df8.orbit == df8.orbit.unique()[n]].sort_values('chamber').z.to_numpy().reshape((-1, 4))
# for i in range(3) :
#     fig, ax = plt.subplots()
#     ax.scatter(vxl[i], vz[i], c = 'blue')
#     ax.scatter(vxr[i], vz[i], c = 'red')
#     ax.scatter(vxb[i], vz[i], c = 'black')
# # blu xl, rosso xr, nero xb, mi secco a fare una legenda solo per questo grrrrr
86/368: vxr
   1: %history -g -f filename
